{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Westminster Ground Truth Analysis (Google Colab)\n",
        "\n",
        "This notebook demonstrates the complete workflow for creating orthomosaics from DJI drone imagery and evaluating their accuracy.\n",
        "\n",
        "**Note**: This notebook is designed for Google Colab. Upload your data to Google Drive and mount it to use this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Install Dependencies and Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install numpy>=1.24.0 opencv-python>=4.8.0 scipy>=1.11.0 scikit-image>=0.21.0\n",
        "!pip install rasterio>=1.3.0 pillow>=10.0.0 matplotlib>=3.7.0 pandas>=2.0.0\n",
        "!pip install pyproj>=3.6.0 shapely>=2.0.0 requests>=2.31.0 tqdm>=4.66.0 exifread>=3.0.0 utm>=0.7.0\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Package Files\n",
        "\n",
        "Upload the `westminster_ground_truth_analysis` package directory to `/content/` or clone from repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you have the package in Google Drive, add it to path\n",
        "# Or clone from repository if available\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Update this path to point to your package location\n",
        "package_path = Path(\"/content/westminster_ground_truth_analysis\")\n",
        "if package_path.exists():\n",
        "    sys.path.insert(0, str(package_path))\n",
        "    print(f\"Added {package_path} to Python path\")\n",
        "else:\n",
        "    print(\"Package not found. Please upload the westminster_ground_truth_analysis directory to /content/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import package (adjust path as needed)\n",
        "try:\n",
        "    from westminster_ground_truth_analysis import (\n",
        "        GCPParser,\n",
        "        DJIMetadataParser,\n",
        "        OrthomosaicPipeline,\n",
        "        download_basemap,\n",
        "        compare_orthomosaic_to_basemap,\n",
        "        visualize_matches,\n",
        "        visualize_reprojection_errors,\n",
        "        visualize_camera_poses,\n",
        "        create_match_quality_report\n",
        "    )\n",
        "    print(\"Package imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    print(\"Please ensure the package is in the Python path\")\n",
        "\n",
        "# Set up paths - UPDATE THESE TO POINT TO YOUR DATA\n",
        "# Option 1: Data in Google Drive\n",
        "data_dir = Path(\"/content/drive/MyDrive/New Westminster Oct _25\")\n",
        "# Option 2: Upload data directly to Colab\n",
        "# data_dir = Path(\"/content/data\")\n",
        "\n",
        "output_dir = Path(\"/content/outputs\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Data directory: {data_dir}\")\n",
        "print(f\"Output directory: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Ground Control Points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse GCP file\n",
        "gcp_file = data_dir / \"25-3288-CONTROL-NAD83-UTM10N-EGM2008.csv\"\n",
        "gcp_parser = GCPParser(str(gcp_file))\n",
        "\n",
        "gcps = gcp_parser.get_gcps()\n",
        "print(f\"Loaded {len(gcps)} ground control points\")\n",
        "\n",
        "# Display first few GCPs\n",
        "for gcp in gcps[:5]:\n",
        "    print(f\"  {gcp.name}: X={gcp.x:.2f}, Y={gcp.y:.2f}, Z={gcp.z:.2f}\")\n",
        "\n",
        "# Get bounds\n",
        "min_x, min_y, max_x, max_y = gcp_parser.get_bounds()\n",
        "print(f\"\\nGCP Bounds: X=[{min_x:.2f}, {max_x:.2f}], Y=[{min_y:.2f}, {max_y:.2f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Process First Dataset (DJI_202510060955_017_25-3288)\n",
        "\n",
        "**Note**: This may take a while depending on the number of images. Consider processing a subset first for testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup for first dataset\n",
        "dataset1_dir = data_dir / \"DJI_202510060955_017_25-3288\"\n",
        "\n",
        "# Try to parse DJI metadata\n",
        "dji_metadata1 = DJIMetadataParser(str(dataset1_dir))\n",
        "\n",
        "# Create pipeline without GCPs first\n",
        "pipeline1_no_gcp = OrthomosaicPipeline(\n",
        "    image_dir=str(dataset1_dir),\n",
        "    output_dir=str(output_dir / \"dataset1_no_gcp\"),\n",
        "    feature_detector=\"sift\",\n",
        "    max_features=5000,\n",
        "    match_ratio=0.7,\n",
        "    use_gcps=False,\n",
        "    dji_metadata=dji_metadata1\n",
        ")\n",
        "\n",
        "print(\"Processing dataset 1 (without GCPs)...\")\n",
        "print(\"This may take 30-60 minutes depending on the number of images...\")\n",
        "output1_no_gcp = pipeline1_no_gcp.run_full_pipeline(output_name=\"dataset1_no_gcp\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pipeline with GCPs\n",
        "pipeline1_with_gcp = OrthomosaicPipeline(\n",
        "    image_dir=str(dataset1_dir),\n",
        "    output_dir=str(output_dir / \"dataset1_with_gcp\"),\n",
        "    feature_detector=\"sift\",\n",
        "    max_features=5000,\n",
        "    match_ratio=0.7,\n",
        "    use_gcps=True,\n",
        "    gcp_parser=gcp_parser,\n",
        "    dji_metadata=dji_metadata1\n",
        ")\n",
        "\n",
        "print(\"Processing dataset 1 (with GCPs)...\")\n",
        "output1_with_gcp = pipeline1_with_gcp.run_full_pipeline(output_name=\"dataset1_with_gcp\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visualize Results and Continue with Remaining Steps\n",
        "\n",
        "The remaining steps (visualization, dataset 2 processing, basemap comparison) follow the same pattern as the local notebook. See `test_westminster_analysis.ipynb` for the complete workflow.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
