{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Westminster Ground Truth Analysis with MetaShape (Spexi Data)\n",
    "\n",
    "This notebook processes drone imagery from Spexi data to create orthomosaics using **Agisoft MetaShape**:\n",
    "\n",
    "1. **KMZ Processing**: Load KMZ file and extract H3 cells\n",
    "2. **Image Download**: Download images from 6 manifest files (with caching to avoid re-downloading)\n",
    "3. **GCP Loading**: Load GCPs from CSV (UTM) and convert to WGS84 for MetaShape\n",
    "4. **Orthomosaic Creation**: Generate orthomosaics with and without GCPs using MetaShape\n",
    "5. **Intermediate File Saving**: All MetaShape intermediate files are saved to avoid recomputation\n",
    "\n",
    "## Data Sources:\n",
    "- **KMZ File**: NewWest_AOI.kmz (contains H3 cell information)\n",
    "- **Manifest Files**: 6 manifest files in Cells/ortho/ directory\n",
    "- **GCPs**: 25-3288-CONTROL-NAD83-UTM10N-EGM2008.csv (UTM Zone 10N)\n",
    "\n",
    "We create two orthomosaics:\n",
    "- Orthomosaic **without** GCPs (using only image matching)\n",
    "- Orthomosaic **with** GCPs (using image matching + ground control points)\n",
    "\n",
    "**Note**: GeoTIFF orthomosaics are exported with LZW lossless compression to reduce file size without losing visual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using qualicum_beach_gcp_analysis package\n",
      "‚úì Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import logging\n",
    "import csv\n",
    "import utm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Add qualicum package to path (if available)\n",
    "package_dir = Path.cwd()\n",
    "sys.path.insert(0, str(package_dir))\n",
    "\n",
    "# Try to import from qualicum_beach_gcp_analysis package\n",
    "try:\n",
    "    from qualicum_beach_gcp_analysis import (\n",
    "        load_gcps_from_kmz,\n",
    "        calculate_gcp_bbox,\n",
    "        bbox_to_h3_cells,\n",
    "        download_all_images_from_input_dir,\n",
    "        export_to_metashape_csv,\n",
    "        export_to_metashape_xml,\n",
    "        process_orthomosaic,\n",
    "        PhotoMatchQuality,\n",
    "        DepthMapQuality,\n",
    "    )\n",
    "    USE_QUALICUM_PACKAGE = True\n",
    "    print(\"‚úì Using qualicum_beach_gcp_analysis package\")\n",
    "except ImportError:\n",
    "    USE_QUALICUM_PACKAGE = False\n",
    "    print(\"‚ö†Ô∏è  qualicum_beach_gcp_analysis package not available\")\n",
    "    print(\"   Some functionality may be limited\")\n",
    "\n",
    "print(\"‚úì Imports successful!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load KMZ File and Extract H3 Cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading KMZ file: /Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25/NewWest_AOI.kmz\n",
      "Loading GCPs from: /Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25/NewWest_AOI.kmz\n",
      "Found 1 KML file(s) in KMZ\n",
      "Found 1 placemarks in KMZ file (namespace: http://www.opengis.net/kml/2.2)\n",
      "Successfully parsed 1 GCPs from KMZ file\n",
      "\n",
      "‚úì Loaded 1 placemarks from KMZ\n",
      "\n",
      "‚úì Extracted 1 H3 cells from KMZ\n",
      "\n",
      "‚úì Calculated 36 H3 cells from bounding box (resolution 12)\n",
      "\n",
      "First few H3 cells from KMZ:\n",
      "  1. GCP_0000\n"
     ]
    }
   ],
   "source": [
    "# Path to the KMZ file\n",
    "kmz_path = Path(\"/Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25/NewWest_AOI.kmz\")\n",
    "\n",
    "if not kmz_path.exists():\n",
    "    raise FileNotFoundError(f\"KMZ file not found: {kmz_path}\")\n",
    "\n",
    "print(f\"Loading KMZ file: {kmz_path}\")\n",
    "\n",
    "if USE_QUALICUM_PACKAGE:\n",
    "    # Load GCPs from KMZ (these contain H3 cell IDs)\n",
    "    gcps_from_kmz = load_gcps_from_kmz(str(kmz_path))\n",
    "    print(f\"\\n‚úì Loaded {len(gcps_from_kmz)} placemarks from KMZ\")\n",
    "    \n",
    "    # Extract H3 cell IDs from GCP IDs (they are the H3 cell identifiers)\n",
    "    h3_cells = [gcp.get('id', '') for gcp in gcps_from_kmz if gcp.get('id')]\n",
    "    print(f\"\\n‚úì Extracted {len(h3_cells)} H3 cells from KMZ\")\n",
    "    \n",
    "    # Also calculate H3 cells from bounding box for verification\n",
    "    bbox = calculate_gcp_bbox(gcps_from_kmz, padding=0.01)\n",
    "    h3_cells_from_bbox = bbox_to_h3_cells(bbox, resolution=12)\n",
    "    print(f\"\\n‚úì Calculated {len(h3_cells_from_bbox)} H3 cells from bounding box (resolution 12)\")\n",
    "    \n",
    "    # Display first few H3 cells\n",
    "    if h3_cells:\n",
    "        print(\"\\nFirst few H3 cells from KMZ:\")\n",
    "        for i, cell_id in enumerate(h3_cells[:10]):\n",
    "            print(f\"  {i+1}. {cell_id}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cannot load KMZ without qualicum_beach_gcp_analysis package\")\n",
    "    h3_cells = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Images from Manifest Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 13:55:08,258 - qualicum_beach_gcp_analysis.s3_downloader - INFO - Found 6 manifest files\n",
      "2025-12-03 13:55:08,395 - qualicum_beach_gcp_analysis.s3_downloader - INFO - Processing manifest: input-file_8928de89117ffff.txt\n",
      "2025-12-03 13:55:08,396 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Bucket: spexi-data-domain-assets-production-ca-central-1\n",
      "2025-12-03 13:55:08,396 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   S3 prefix: standardized-images/8928de89117ffff/134449/\n",
      "2025-12-03 13:55:08,397 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Total images: 155\n",
      "2025-12-03 13:55:08,431 - botocore.tokens - INFO - Loading cached SSO token for spexi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest directory: /Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25/Cells/ortho\n",
      "Photos will be downloaded to: /Users/mauriciohessflores/Documents/Code/MyCode/research-westminster_ground_truth_analysis/input/images\n",
      "\n",
      "Downloading images from S3...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 13:55:13,873 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 10/155 images...\n",
      "2025-12-03 13:55:17,792 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 20/155 images...\n",
      "2025-12-03 13:55:21,788 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 30/155 images...\n",
      "2025-12-03 13:55:26,469 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 40/155 images...\n",
      "2025-12-03 13:55:30,522 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 50/155 images...\n",
      "2025-12-03 13:55:35,415 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 60/155 images...\n",
      "2025-12-03 13:55:39,575 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 70/155 images...\n",
      "2025-12-03 13:55:44,126 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 80/155 images...\n",
      "2025-12-03 13:55:48,288 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 90/155 images...\n",
      "2025-12-03 13:55:52,431 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 100/155 images...\n",
      "2025-12-03 13:55:57,167 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 110/155 images...\n",
      "2025-12-03 13:56:01,757 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 120/155 images...\n",
      "2025-12-03 13:56:05,966 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 130/155 images...\n",
      "2025-12-03 13:56:10,560 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 140/155 images...\n",
      "2025-12-03 13:56:14,587 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 150/155 images...\n",
      "2025-12-03 13:56:17,194 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Completed: 155 downloaded, 0 skipped, 0 failed\n",
      "2025-12-03 13:56:17,290 - qualicum_beach_gcp_analysis.s3_downloader - INFO - Processing manifest: input-file_8928de89187ffff.txt\n",
      "2025-12-03 13:56:17,291 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Bucket: spexi-data-domain-assets-production-ca-central-1\n",
      "2025-12-03 13:56:17,292 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   S3 prefix: standardized-images/8928de89187ffff/134265/\n",
      "2025-12-03 13:56:17,293 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Total images: 178\n",
      "2025-12-03 13:56:22,441 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 10/178 images...\n",
      "2025-12-03 13:56:26,611 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 20/178 images...\n",
      "2025-12-03 13:56:30,915 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 30/178 images...\n",
      "2025-12-03 13:56:35,162 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 40/178 images...\n",
      "2025-12-03 13:56:39,476 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 50/178 images...\n",
      "2025-12-03 13:56:44,420 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 60/178 images...\n",
      "2025-12-03 13:56:48,680 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 70/178 images...\n",
      "2025-12-03 13:56:53,112 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 80/178 images...\n",
      "2025-12-03 13:56:57,313 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 90/178 images...\n",
      "2025-12-03 13:57:01,876 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 100/178 images...\n",
      "2025-12-03 13:57:06,677 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 110/178 images...\n",
      "2025-12-03 13:57:11,394 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 120/178 images...\n",
      "2025-12-03 13:57:16,100 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 130/178 images...\n",
      "2025-12-03 13:57:20,175 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 140/178 images...\n",
      "2025-12-03 13:57:24,926 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 150/178 images...\n",
      "2025-12-03 13:57:29,736 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 160/178 images...\n",
      "2025-12-03 13:57:34,063 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 170/178 images...\n",
      "2025-12-03 13:57:37,462 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Completed: 178 downloaded, 0 skipped, 0 failed\n",
      "2025-12-03 13:57:37,509 - qualicum_beach_gcp_analysis.s3_downloader - INFO - Processing manifest: input-file_8928de891a3ffff.txt\n",
      "2025-12-03 13:57:37,510 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Bucket: spexi-data-domain-assets-production-ca-central-1\n",
      "2025-12-03 13:57:37,511 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   S3 prefix: standardized-images/8928de891a3ffff/135506/\n",
      "2025-12-03 13:57:37,512 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Total images: 155\n",
      "2025-12-03 13:57:42,365 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 10/155 images...\n",
      "2025-12-03 13:57:46,753 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 20/155 images...\n",
      "2025-12-03 13:57:51,264 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 30/155 images...\n",
      "2025-12-03 13:57:56,042 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 40/155 images...\n",
      "2025-12-03 13:58:00,925 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 50/155 images...\n",
      "2025-12-03 13:58:06,103 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 60/155 images...\n",
      "2025-12-03 13:58:10,276 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 70/155 images...\n",
      "2025-12-03 13:58:15,135 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 80/155 images...\n",
      "2025-12-03 13:58:19,815 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 90/155 images...\n",
      "2025-12-03 13:58:24,322 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 100/155 images...\n",
      "2025-12-03 13:58:29,552 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 110/155 images...\n",
      "2025-12-03 13:58:34,115 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 120/155 images...\n",
      "2025-12-03 13:58:38,659 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 130/155 images...\n",
      "2025-12-03 13:58:42,838 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 140/155 images...\n",
      "2025-12-03 13:58:47,261 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 150/155 images...\n",
      "2025-12-03 13:58:50,070 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Completed: 155 downloaded, 0 skipped, 0 failed\n",
      "2025-12-03 13:58:50,162 - qualicum_beach_gcp_analysis.s3_downloader - INFO - Processing manifest: input-file_8928de891abffff.txt\n",
      "2025-12-03 13:58:50,163 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Bucket: spexi-data-domain-assets-production-ca-central-1\n",
      "2025-12-03 13:58:50,164 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   S3 prefix: standardized-images/8928de891abffff/175391/\n",
      "2025-12-03 13:58:50,165 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Total images: 155\n",
      "2025-12-03 13:58:55,989 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 10/155 images...\n",
      "2025-12-03 13:59:01,370 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 20/155 images...\n",
      "2025-12-03 13:59:06,783 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 30/155 images...\n",
      "2025-12-03 13:59:12,183 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 40/155 images...\n",
      "2025-12-03 13:59:17,360 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 50/155 images...\n",
      "2025-12-03 13:59:22,633 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 60/155 images...\n",
      "2025-12-03 13:59:28,533 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 70/155 images...\n",
      "2025-12-03 13:59:34,295 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 80/155 images...\n",
      "2025-12-03 13:59:40,047 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 90/155 images...\n",
      "2025-12-03 13:59:45,965 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 100/155 images...\n",
      "2025-12-03 13:59:51,647 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 110/155 images...\n",
      "2025-12-03 13:59:57,813 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 120/155 images...\n",
      "2025-12-03 14:00:03,573 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 130/155 images...\n",
      "2025-12-03 14:00:09,257 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 140/155 images...\n",
      "2025-12-03 14:00:14,875 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 150/155 images...\n",
      "2025-12-03 14:00:16,488 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Completed: 153 downloaded, 2 skipped, 0 failed\n",
      "2025-12-03 14:00:16,535 - qualicum_beach_gcp_analysis.s3_downloader - INFO - Processing manifest: input-file_8928de891b3ffff.txt\n",
      "2025-12-03 14:00:16,536 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Bucket: spexi-data-domain-assets-production-ca-central-1\n",
      "2025-12-03 14:00:16,537 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   S3 prefix: standardized-images/8928de891b3ffff/175379/\n",
      "2025-12-03 14:00:16,537 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Total images: 155\n",
      "2025-12-03 14:00:22,240 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 10/155 images...\n",
      "2025-12-03 14:00:27,923 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 20/155 images...\n",
      "2025-12-03 14:00:32,935 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 30/155 images...\n",
      "2025-12-03 14:00:37,780 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 40/155 images...\n",
      "2025-12-03 14:00:42,821 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 50/155 images...\n",
      "2025-12-03 14:00:48,025 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 60/155 images...\n",
      "2025-12-03 14:00:53,387 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 70/155 images...\n",
      "2025-12-03 14:00:58,189 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 80/155 images...\n",
      "2025-12-03 14:01:03,425 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 90/155 images...\n",
      "2025-12-03 14:01:08,450 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 100/155 images...\n",
      "2025-12-03 14:01:13,764 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 110/155 images...\n",
      "2025-12-03 14:01:20,168 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 120/155 images...\n",
      "2025-12-03 14:01:25,844 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 130/155 images...\n",
      "2025-12-03 14:01:31,471 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 140/155 images...\n",
      "2025-12-03 14:01:37,261 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 150/155 images...\n",
      "2025-12-03 14:01:41,019 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Completed: 155 downloaded, 0 skipped, 0 failed\n",
      "2025-12-03 14:01:41,060 - qualicum_beach_gcp_analysis.s3_downloader - INFO - Processing manifest: input-file_8928de891bbffff.txt\n",
      "2025-12-03 14:01:41,061 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Bucket: spexi-data-domain-assets-production-ca-central-1\n",
      "2025-12-03 14:01:41,061 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   S3 prefix: standardized-images/8928de891bbffff/175387/\n",
      "2025-12-03 14:01:41,062 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Total images: 154\n",
      "2025-12-03 14:01:46,405 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 10/154 images...\n",
      "2025-12-03 14:01:51,620 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 20/154 images...\n",
      "2025-12-03 14:01:56,479 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 30/154 images...\n",
      "2025-12-03 14:02:01,553 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 40/154 images...\n",
      "2025-12-03 14:02:06,349 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 50/154 images...\n",
      "2025-12-03 14:02:11,922 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 60/154 images...\n",
      "2025-12-03 14:02:17,712 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 70/154 images...\n",
      "2025-12-03 14:02:23,168 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 80/154 images...\n",
      "2025-12-03 14:02:28,079 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 90/154 images...\n",
      "2025-12-03 14:02:33,098 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 100/154 images...\n",
      "2025-12-03 14:02:38,726 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 110/154 images...\n",
      "2025-12-03 14:02:52,344 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 120/154 images...\n",
      "2025-12-03 14:03:19,894 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 130/154 images...\n",
      "2025-12-03 14:03:46,426 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 140/154 images...\n",
      "2025-12-03 14:04:13,101 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Downloaded 150/154 images...\n",
      "2025-12-03 14:04:22,714 - qualicum_beach_gcp_analysis.s3_downloader - INFO -   Completed: 153 downloaded, 1 skipped, 0 failed\n",
      "2025-12-03 14:04:22,718 - qualicum_beach_gcp_analysis.s3_downloader - INFO - \n",
      "============================================================\n",
      "2025-12-03 14:04:22,719 - qualicum_beach_gcp_analysis.s3_downloader - INFO - Download Summary\n",
      "2025-12-03 14:04:22,720 - qualicum_beach_gcp_analysis.s3_downloader - INFO - ============================================================\n",
      "2025-12-03 14:04:22,721 - qualicum_beach_gcp_analysis.s3_downloader - INFO - Total manifest files: 6\n",
      "2025-12-03 14:04:22,721 - qualicum_beach_gcp_analysis.s3_downloader - INFO - Total images: 952\n",
      "2025-12-03 14:04:22,722 - qualicum_beach_gcp_analysis.s3_downloader - INFO - Downloaded: 949\n",
      "2025-12-03 14:04:22,722 - qualicum_beach_gcp_analysis.s3_downloader - INFO - Skipped (already exist): 3\n",
      "2025-12-03 14:04:22,723 - qualicum_beach_gcp_analysis.s3_downloader - INFO - Failed: 0\n",
      "2025-12-03 14:04:22,724 - qualicum_beach_gcp_analysis.s3_downloader - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚úì Image download complete\n",
      "\n",
      "Summary:\n",
      "  Total images: 952\n",
      "  Downloaded: 949\n",
      "  Skipped (already exist): 3\n"
     ]
    }
   ],
   "source": [
    "# Setup paths\n",
    "data_dir = Path(\"/Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25\")\n",
    "manifest_dir = data_dir / \"Cells\" / \"ortho\"\n",
    "photos_dir = Path(\"input/images\")\n",
    "\n",
    "if not manifest_dir.exists():\n",
    "    raise FileNotFoundError(f\"Manifest directory not found: {manifest_dir}\")\n",
    "\n",
    "print(f\"Manifest directory: {manifest_dir}\")\n",
    "print(f\"Photos will be downloaded to: {photos_dir.absolute()}\")\n",
    "\n",
    "if USE_QUALICUM_PACKAGE:\n",
    "    # Download all images from manifest files\n",
    "    # skip_existing=True ensures files are not re-downloaded if they already exist\n",
    "    print(\"\\nDownloading images from S3...\")\n",
    "    print(\"=\" * 60)\n",
    "    download_stats = download_all_images_from_input_dir(\n",
    "        input_dir=manifest_dir,\n",
    "        photos_dir=photos_dir,\n",
    "        skip_existing=True  # Don't re-download if images already exist\n",
    "    )\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úì Image download complete\")\n",
    "    \n",
    "    # Count total images downloaded\n",
    "    total_images = sum(s.get('total', 0) for s in download_stats.values())\n",
    "    total_downloaded = sum(s.get('downloaded', 0) for s in download_stats.values())\n",
    "    total_skipped = sum(s.get('skipped', 0) for s in download_stats.values())\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Total images: {total_images}\")\n",
    "    print(f\"  Downloaded: {total_downloaded}\")\n",
    "    print(f\"  Skipped (already exist): {total_skipped}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cannot download images without qualicum_beach_gcp_analysis package\")\n",
    "    print(\"   Please install the package or manually download images to:\")\n",
    "    print(f\"   {photos_dir.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Convert GCPs to WGS84\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GCPs from: /Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25/25-3288-CONTROL-NAD83-UTM10N-EGM2008.csv\n",
      "\n",
      "‚úì Loaded 22 GCPs from CSV (UTM Zone 10N)\n"
     ]
    },
    {
     "ename": "OutOfRangeError",
     "evalue": "easting out of range (must be between 100,000 m and 999,999 m)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m\n\u001b[1;32m     35\u001b[0m gcps_wgs84 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gcp \u001b[38;5;129;01min\u001b[39;00m gcps_utm:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Convert UTM to lat/lon (UTM Zone 10N)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# X is Northing, Y is Easting\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     lat, lon \u001b[38;5;241m=\u001b[39m \u001b[43mutm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_latlon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgcp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgcp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     gcp_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: gcp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: gcp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.01\u001b[39m  \u001b[38;5;66;03m# Very high accuracy (1cm) for high weight in bundle adjustment\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     }\n\u001b[1;32m     50\u001b[0m     gcps_wgs84\u001b[38;5;241m.\u001b[39mappend(gcp_dict)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/utm/conversion.py:125\u001b[0m, in \u001b[0;36mto_latlon\u001b[0;34m(easting, northing, zone_number, zone_letter, northern, strict)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m in_bounds(easting, \u001b[38;5;241m100000\u001b[39m, \u001b[38;5;241m1000000\u001b[39m, upper_strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 125\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OutOfRangeError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124measting out of range (must be between 100,000 m and 999,999 m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m in_bounds(northing, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10000000\u001b[39m):\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OutOfRangeError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorthing out of range (must be between 0 m and 10,000,000 m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: easting out of range (must be between 100,000 m and 999,999 m)"
     ]
    }
   ],
   "source": [
    "# Ensure Path is imported\n",
    "try:\n",
    "    from pathlib import Path\n",
    "except ImportError:\n",
    "    pass  # Already imported\n",
    "\n",
    "# Define data_dir if not already defined (from Step 2)\n",
    "try:\n",
    "    _ = data_dir\n",
    "except NameError:\n",
    "    data_dir = Path(\"/Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25\")\n",
    "    print(f\"‚ÑπÔ∏è  data_dir not defined, using default: {data_dir}\")\n",
    "\n",
    "# Path to GCP CSV file (UTM coordinates)\n",
    "gcp_file = data_dir / \"25-3288-CONTROL-NAD83-UTM10N-EGM2008.csv\"\n",
    "\n",
    "if not gcp_file.exists():\n",
    "    raise FileNotFoundError(f\"GCP file not found: {gcp_file}\")\n",
    "\n",
    "print(f\"Loading GCPs from: {gcp_file}\")\n",
    "\n",
    "# Parse GCP file (UTM coordinates)\n",
    "gcps_utm = []\n",
    "with open(gcp_file, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            name = row.get('Name', row.get('name', ''))\n",
    "            x = float(row.get('X', row.get('x', 0)))\n",
    "            y = float(row.get('Y', row.get('y', 0)))\n",
    "            z = float(row.get('Z', row.get('z', row.get('Elevation', row.get('elevation', 0)))))\n",
    "            \n",
    "            gcps_utm.append({\n",
    "                'name': name,\n",
    "                'x': x,  # Northing\n",
    "                'y': y,  # Easting\n",
    "                'z': z\n",
    "            })\n",
    "        except (ValueError, KeyError) as e:\n",
    "            print(f\"‚ö†Ô∏è  Skipping invalid row: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\n‚úì Loaded {len(gcps_utm)} GCPs from CSV (UTM Zone 10N)\")\n",
    "\n",
    "# Convert UTM to WGS84 lat/lon for MetaShape\n",
    "# NOTE: UTM coordinates need to be in valid ranges:\n",
    "# - Easting: 100,000 to 999,999 m\n",
    "# - Northing: 0 to 10,000,000 m\n",
    "# We need to determine which column is easting and which is northing\n",
    "gcps_wgs84 = []\n",
    "\n",
    "# First, check the values to determine correct ordering\n",
    "print(\"\\nChecking coordinate ranges...\")\n",
    "first_gcp = gcps_utm[0] if gcps_utm else None\n",
    "if first_gcp:\n",
    "    print(f\"  First GCP values: X={first_gcp['x']:.2f}, Y={first_gcp['y']:.2f}\")\n",
    "    print(f\"  X range: {min(g['x'] for g in gcps_utm):.2f} to {max(g['x'] for g in gcps_utm):.2f}\")\n",
    "    print(f\"  Y range: {min(g['y'] for g in gcps_utm):.2f} to {max(g['y'] for g in gcps_utm):.2f}\")\n",
    "\n",
    "# Determine which is easting and which is northing\n",
    "# Easting should be 100,000-999,999, Northing should be 0-10,000,000\n",
    "x_min, x_max = min(g['x'] for g in gcps_utm), max(g['x'] for g in gcps_utm)\n",
    "y_min, y_max = min(g['y'] for g in gcps_utm), max(g['y'] for g in gcps_utm)\n",
    "\n",
    "# Check if X is in easting range (100k-999k) or northing range (0-10M)\n",
    "x_is_easting = 100000 <= x_min <= 999999 and 100000 <= x_max <= 999999\n",
    "x_is_northing = 0 <= x_min <= 10000000 and 0 <= x_max <= 10000000\n",
    "y_is_easting = 100000 <= y_min <= 999999 and 100000 <= y_max <= 999999\n",
    "y_is_northing = 0 <= y_min <= 10000000 and 0 <= y_max <= 10000000\n",
    "\n",
    "print(f\"\\nCoordinate analysis:\")\n",
    "print(f\"  X is easting: {x_is_easting}, X is northing: {x_is_northing}\")\n",
    "print(f\"  Y is easting: {y_is_easting}, Y is northing: {y_is_northing}\")\n",
    "\n",
    "# Determine correct mapping\n",
    "if x_is_easting and y_is_northing:\n",
    "    # X is easting, Y is northing (standard UTM)\n",
    "    easting_col = 'x'\n",
    "    northing_col = 'y'\n",
    "    print(f\"\\n‚úì Using X as easting, Y as northing (standard UTM)\")\n",
    "elif y_is_easting and x_is_northing:\n",
    "    # Y is easting, X is northing (swapped)\n",
    "    easting_col = 'y'\n",
    "    northing_col = 'x'\n",
    "    print(f\"\\n‚úì Using Y as easting, X as northing (swapped)\")\n",
    "else:\n",
    "    # Try both and see which works\n",
    "    print(f\"\\n‚ö†Ô∏è  Cannot determine automatically, trying both orderings...\")\n",
    "    # Try X as easting first\n",
    "    try:\n",
    "        test_easting = gcps_utm[0]['x']\n",
    "        test_northing = gcps_utm[0]['y']\n",
    "        if 100000 <= test_easting <= 999999 and 0 <= test_northing <= 10000000:\n",
    "            easting_col = 'x'\n",
    "            northing_col = 'y'\n",
    "            print(f\"  Trying X as easting, Y as northing...\")\n",
    "        else:\n",
    "            raise ValueError(\"X/Y don't match expected ranges\")\n",
    "    except:\n",
    "        # Try Y as easting\n",
    "        try:\n",
    "            test_easting = gcps_utm[0]['y']\n",
    "            test_northing = gcps_utm[0]['x']\n",
    "            if 100000 <= test_easting <= 999999 and 0 <= test_northing <= 10000000:\n",
    "                easting_col = 'y'\n",
    "                northing_col = 'x'\n",
    "                print(f\"  Trying Y as easting, X as northing...\")\n",
    "            else:\n",
    "                raise ValueError(\"Neither ordering matches expected ranges\")\n",
    "        except:\n",
    "            raise ValueError(f\"Cannot determine easting/northing. X range: {x_min:.2f}-{x_max:.2f}, Y range: {y_min:.2f}-{y_max:.2f}\")\n",
    "\n",
    "# Convert UTM to WGS84 lat/lon\n",
    "for gcp in gcps_utm:\n",
    "    try:\n",
    "        easting = gcp[easting_col]\n",
    "        northing = gcp[northing_col]\n",
    "        \n",
    "        # Validate ranges before conversion\n",
    "        if not (100000 <= easting <= 999999):\n",
    "            raise ValueError(f\"Easting {easting:.2f} out of range (100,000-999,999)\")\n",
    "        if not (0 <= northing <= 10000000):\n",
    "            raise ValueError(f\"Northing {northing:.2f} out of range (0-10,000,000)\")\n",
    "        \n",
    "        # Convert UTM to lat/lon (UTM Zone 10N)\n",
    "        lat, lon = utm.to_latlon(easting, northing, 10, 'N')\n",
    "        \n",
    "        gcp_dict = {\n",
    "            'id': gcp['name'],\n",
    "            'label': gcp['name'],\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'z': gcp['z'],\n",
    "            'accuracy': 0.01  # Very high accuracy (1cm) for high weight in bundle adjustment\n",
    "        }\n",
    "        gcps_wgs84.append(gcp_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error converting GCP {gcp['name']}: {e}\")\n",
    "        print(f\"   X={gcp['x']:.2f}, Y={gcp['y']:.2f}, Easting={gcp.get(easting_col, 'N/A'):.2f}, Northing={gcp.get(northing_col, 'N/A'):.2f}\")\n",
    "        raise\n",
    "\n",
    "print(f\"\\n‚úì Converted {len(gcps_wgs84)} GCPs to WGS84 lat/lon\")\n",
    "print(\"\\nFirst few GCPs (WGS84):\")\n",
    "for gcp in gcps_wgs84[:5]:\n",
    "    print(f\"  {gcp['id']}: ({gcp['lat']:.6f}, {gcp['lon']:.6f}, z={gcp['z']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Export GCPs for MetaShape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "gcp_output_dir = output_dir / \"gcps\"\n",
    "gcp_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if USE_QUALICUM_PACKAGE:\n",
    "    # Export GCPs to MetaShape XML format (preferred by MetaShape)\n",
    "    gcp_xml_path = gcp_output_dir / \"gcps_metashape.xml\"\n",
    "    export_to_metashape_xml(gcps_wgs84, str(gcp_xml_path))\n",
    "    print(f\"‚úì GCPs exported to XML: {gcp_xml_path}\")\n",
    "    \n",
    "    # Also export CSV for reference\n",
    "    gcp_csv_path = gcp_output_dir / \"gcps_metashape.csv\"\n",
    "    export_to_metashape_csv(gcps_wgs84, str(gcp_csv_path))\n",
    "    print(f\"‚úì GCPs also exported to CSV: {gcp_csv_path}\")\n",
    "    \n",
    "    # Use CSV file for processing (more reliable than XML)\n",
    "    gcp_file_for_processing = gcp_csv_path\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cannot export GCPs without qualicum_beach_gcp_analysis package\")\n",
    "    print(\"   GCPs are already in WGS84 format and can be used directly\")\n",
    "    gcp_file_for_processing = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process Orthomosaic WITHOUT GCPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if METASHAPE_AVAILABLE is defined\n",
    "try:\n",
    "    _ = METASHAPE_AVAILABLE\n",
    "except NameError:\n",
    "    METASHAPE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  METASHAPE_AVAILABLE not defined. Assuming MetaShape is not available.\")\n",
    "\n",
    "if not METASHAPE_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è  MetaShape not available. Skipping processing.\")\n",
    "else:\n",
    "    # Setup paths for processing\n",
    "    intermediate_dir = output_dir / \"intermediate\"\n",
    "    ortho_output_dir = output_dir / \"orthomosaics\"\n",
    "    \n",
    "    # Process orthomosaic WITHOUT GCPs\n",
    "    # Note: clean_intermediate_files=False will reuse existing processing steps\n",
    "    # Set to True to start fresh and delete previous work\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Processing orthomosaic WITHOUT GCPs...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    project_path_no_gcps = intermediate_dir / \"orthomosaic_no_gcps.psx\"\n",
    "    \n",
    "    if USE_QUALICUM_PACKAGE:\n",
    "        stats_no_gcps = process_orthomosaic(\n",
    "            photos_dir=photos_dir,\n",
    "            output_path=ortho_output_dir,\n",
    "            project_path=project_path_no_gcps,\n",
    "            product_id=\"orthomosaic_no_gcps\",\n",
    "            clean_intermediate_files=False,  # Reuse existing processing if available\n",
    "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
    "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
    "            tiepoint_limit=10000,\n",
    "            use_gcps=False\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Cannot process orthomosaic without qualicum_beach_gcp_analysis package\")\n",
    "        stats_no_gcps = None\n",
    "    \n",
    "    if stats_no_gcps:\n",
    "        print(\"\\n‚úì Orthomosaic processing (without GCPs) complete!\")\n",
    "        print(f\"  Number of photos: {stats_no_gcps['num_photos']}\")\n",
    "        print(f\"\\nüìÅ Output Files:\")\n",
    "        ortho_path_no_gcps = Path(stats_no_gcps['ortho_path'])\n",
    "        if ortho_path_no_gcps.exists():\n",
    "            file_size_mb = ortho_path_no_gcps.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  ‚úì Orthomosaic GeoTIFF: {ortho_path_no_gcps.absolute()}\")\n",
    "            print(f\"    Size: {file_size_mb:.2f} MB (LZW compressed, lossless)\")\n",
    "        if 'log_file_path' in stats_no_gcps:\n",
    "            print(f\"  üìù Log file: {stats_no_gcps['log_file_path']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Process Orthomosaic WITH GCPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if METASHAPE_AVAILABLE is defined\n",
    "try:\n",
    "    _ = METASHAPE_AVAILABLE\n",
    "except NameError:\n",
    "    METASHAPE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  METASHAPE_AVAILABLE not defined. Assuming MetaShape is not available.\")\n",
    "\n",
    "if not METASHAPE_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è  MetaShape not available. Skipping processing.\")\n",
    "else:\n",
    "    # Setup paths for processing\n",
    "    intermediate_dir = output_dir / \"intermediate\"\n",
    "    ortho_output_dir = output_dir / \"orthomosaics\"\n",
    "    \n",
    "    # Process orthomosaic WITH GCPs\n",
    "    # Note: clean_intermediate_files=False will reuse existing processing steps\n",
    "    # Set to True to start fresh and delete previous work\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Processing orthomosaic WITH GCPs...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    project_path_with_gcps = intermediate_dir / \"orthomosaic_with_gcps.psx\"\n",
    "    \n",
    "    if USE_QUALICUM_PACKAGE and gcp_file_for_processing:\n",
    "        stats_with_gcps = process_orthomosaic(\n",
    "            photos_dir=photos_dir,\n",
    "            output_path=ortho_output_dir,\n",
    "            project_path=project_path_with_gcps,\n",
    "            gcp_file=gcp_file_for_processing,\n",
    "            product_id=\"orthomosaic_with_gcps\",\n",
    "            clean_intermediate_files=False,  # Reuse existing processing if available\n",
    "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
    "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
    "            tiepoint_limit=10000,\n",
    "            use_gcps=True,\n",
    "            gcp_accuracy=0.01  # Very low accuracy (1cm) for very high weight in bundle adjustment\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Cannot process orthomosaic with GCPs without qualicum_beach_gcp_analysis package\")\n",
    "        stats_with_gcps = None\n",
    "    \n",
    "    if stats_with_gcps:\n",
    "        print(\"\\n‚úì Orthomosaic processing (with GCPs) complete!\")\n",
    "        print(f\"  Number of photos: {stats_with_gcps['num_photos']}\")\n",
    "        print(f\"  Number of markers: {stats_with_gcps.get('num_markers', 0)}\")\n",
    "        print(f\"\\nüìÅ Output Files:\")\n",
    "        ortho_path_with_gcps = Path(stats_with_gcps['ortho_path'])\n",
    "        if ortho_path_with_gcps.exists():\n",
    "            file_size_mb = ortho_path_with_gcps.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  ‚úì Orthomosaic GeoTIFF: {ortho_path_with_gcps.absolute()}\")\n",
    "            print(f\"    Size: {file_size_mb:.2f} MB (LZW compressed, lossless)\")\n",
    "        if 'log_file_path' in stats_with_gcps:\n",
    "            print(f\"  üìù Log file: {stats_with_gcps['log_file_path']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has:\n",
    "1. ‚úì Loaded KMZ file and extracted H3 cells\n",
    "2. ‚úì Downloaded images from 6 manifest files (with caching to avoid re-downloading)\n",
    "3. ‚úì Loaded and converted GCPs from UTM to WGS84\n",
    "4. ‚úì Created orthomosaics with and without GCPs using MetaShape\n",
    "5. ‚úì Saved all intermediate files to avoid recomputation\n",
    "\n",
    "**Note**: \n",
    "- All MetaShape processing steps check for existing intermediate results and skip recomputation if they already exist. This means you can safely re-run cells without losing progress.\n",
    "- GeoTIFF orthomosaics are exported with **LZW lossless compression** to reduce file size by 30-50% without any visual quality loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to install from requirements.txt first\n",
    "requirements_file = Path(\"requirements.txt\")\n",
    "if requirements_file.exists():\n",
    "    print(\"Installing packages from requirements.txt...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(requirements_file)])\n",
    "    print(\"‚úì Packages installed from requirements.txt\")\n",
    "else:\n",
    "    # Fallback: install packages individually\n",
    "    print(\"requirements.txt not found. Installing packages individually...\")\n",
    "    packages = [\n",
    "        \"numpy>=1.24.0\",\n",
    "        \"rasterio>=1.3.0\",\n",
    "        \"pillow>=10.0.0\",\n",
    "        \"matplotlib>=3.7.0\",\n",
    "        \"pandas>=2.0.0\",\n",
    "        \"pyproj>=3.6.0\",\n",
    "        \"requests>=2.31.0\",\n",
    "        \"utm>=0.7.0\",\n",
    "        \"h3>=3.7.0\",\n",
    "        \"boto3>=1.28.0\",\n",
    "    ]\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    print(\"‚úì All packages installed\")\n",
    "\n",
    "# Check for MetaShape\n",
    "try:\n",
    "    import Metashape\n",
    "    print(\"‚úì MetaShape Python API is available\")\n",
    "    METASHAPE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  MetaShape Python API not found. Please install Agisoft MetaShape and its Python API.\")\n",
    "    METASHAPE_AVAILABLE = False\n",
    "\n",
    "print(\"\\nSetup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
