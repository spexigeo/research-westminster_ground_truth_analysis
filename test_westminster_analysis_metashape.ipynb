{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Westminster Ground Truth Analysis with MetaShape\n",
    "\n",
    "This notebook demonstrates the complete workflow for creating orthomosaics from DJI drone imagery using **Agisoft MetaShape** and evaluating their accuracy:\n",
    "\n",
    "1. **Data Loading**: Load images from three directories and GCPs from CSV\n",
    "2. **DJI Metadata Processing**: Extract camera positions and orientations from nav, obs, bin, and MRK files\n",
    "3. **GCP Conversion**: Convert UTM coordinates to WGS84 lat/lon for MetaShape\n",
    "4. **Orthomosaic Creation**: Generate a single orthomosaic from ALL images with and without GCPs using MetaShape\n",
    "5. **Basemap Comparison**: Download basemaps and quantify absolute accuracy\n",
    "6. **Quality Report**: Generate comprehensive comparison report\n",
    "\n",
    "## Datasets:\n",
    "- **Directory 1**: DJI_202510060955_017_25-3288 (contains images, nav, obs, bin, MRK files)\n",
    "- **Directory 2**: DJI_202510060955_018_25-3288 (contains images, nav, obs, bin, MRK files)\n",
    "- **Directory 3**: DJI_202510060955_019_25-3288 (contains images, nav, obs, bin, MRK files)\n",
    "\n",
    "**Ground Control Points**: 25-3288-CONTROL-NAD83-UTM10N-EGM2008.csv\n",
    "\n",
    "We create a **single combined orthomosaic** from all images in all three directories:\n",
    "- Orthomosaic **without** GCPs (using only image matching)\n",
    "- Orthomosaic **with** GCPs (using image matching + ground control points)\n",
    "\n",
    "Both orthomosaics are compared against reference basemaps to evaluate accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies\n",
    "\n",
    "First, install the required packages. Note: This notebook requires **Agisoft MetaShape Python API** to be installed separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages from requirements.txt...\n",
      "‚úì Packages installed from requirements.txt\n",
      "‚úì MetaShape Python API is available\n",
      "\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to install from requirements.txt first\n",
    "requirements_file = Path(\"requirements.txt\")\n",
    "if requirements_file.exists():\n",
    "    print(\"Installing packages from requirements.txt...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(requirements_file)])\n",
    "    print(\"‚úì Packages installed from requirements.txt\")\n",
    "else:\n",
    "    # Fallback: install packages individually\n",
    "    print(\"requirements.txt not found. Installing packages individually...\")\n",
    "    packages = [\n",
    "        \"numpy>=1.24.0\",\n",
    "        \"rasterio>=1.3.0\",\n",
    "        \"pillow>=10.0.0\",\n",
    "        \"matplotlib>=3.7.0\",\n",
    "        \"pandas>=2.0.0\",\n",
    "        \"pyproj>=3.6.0\",\n",
    "        \"requests>=2.31.0\",\n",
    "        \"utm>=0.7.0\",\n",
    "    ]\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    print(\"‚úì All packages installed\")\n",
    "\n",
    "# Check for MetaShape\n",
    "try:\n",
    "    import Metashape\n",
    "    print(\"‚úì MetaShape Python API is available\")\n",
    "    METASHAPE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  MetaShape Python API not found. Please install Agisoft MetaShape and its Python API.\")\n",
    "    print(\"   The API is typically installed with MetaShape at:\")\n",
    "    print(\"   - Windows: C:\\\\Program Files\\\\Agisoft\\\\Metashape Pro\\\\python\")\n",
    "    print(\"   - macOS: /Applications/Metashape Pro/Metashape.app/Contents/Frameworks/Python.framework/Versions/3.9\")\n",
    "    print(\"   - Linux: /opt/metashape-pro/lib/python3.9\")\n",
    "    METASHAPE_AVAILABLE = False\n",
    "\n",
    "print(\"\\nSetup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using MetaShape processor from qualicum_beach_gcp_analysis\n",
      "‚úì Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import logging\n",
    "import json\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "import utm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Add package to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Import Westminster-specific modules\n",
    "from westminster_ground_truth_analysis import (\n",
    "    GCPParser,\n",
    "    download_basemap,\n",
    "    compare_orthomosaic_to_basemap,\n",
    "    DJIMetadataParser,\n",
    ")\n",
    "\n",
    "# Try to import MetaShape processor from qualicum_beach package\n",
    "# If not available, we'll define the functions locally\n",
    "try:\n",
    "    from qualicum_beach_gcp_analysis import (\n",
    "        process_orthomosaic,\n",
    "        PhotoMatchQuality,\n",
    "        DepthMapQuality,\n",
    "        export_to_metashape_csv,\n",
    "        export_to_metashape_xml,\n",
    "    )\n",
    "    print(\"‚úì Using MetaShape processor from qualicum_beach_gcp_analysis\")\n",
    "    USE_QUALICUM_PACKAGE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  qualicum_beach_gcp_analysis not found. Will define MetaShape functions locally.\")\n",
    "    USE_QUALICUM_PACKAGE = False\n",
    "    # We'll define these functions in the next cell if needed\n",
    "\n",
    "# Set up paths\n",
    "data_dir = Path(\"/Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25\")\n",
    "output_dir = Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úì Imports successful!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Ground Control Points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 23 ground control points (UTM format)\n",
      "\n",
      "First few GCPs (UTM):\n",
      "  GCP1: X=5450945.53, Y=506914.12, Z=77.45\n",
      "  GCP2: X=5450730.01, Y=506657.79, Z=79.22\n",
      "  GCP3: X=5450480.01, Y=506577.77, Z=59.40\n",
      "  GCP4: X=5450578.63, Y=506765.03, Z=65.59\n",
      "  GCP5: X=5450715.96, Y=506926.13, Z=63.10\n",
      "\n",
      "GCP Bounds (UTM): X=[5450109.82, 5450992.66], Y=[506577.77, 507315.01]\n"
     ]
    }
   ],
   "source": [
    "# Parse GCP file (UTM coordinates)\n",
    "gcp_file = data_dir / \"25-3288-CONTROL-NAD83-UTM10N-EGM2008.csv\"\n",
    "gcp_parser = GCPParser(str(gcp_file))\n",
    "\n",
    "gcps_utm = gcp_parser.get_gcps()\n",
    "print(f\"Loaded {len(gcps_utm)} ground control points (UTM format)\")\n",
    "\n",
    "# Display first few GCPs\n",
    "print(\"\\nFirst few GCPs (UTM):\")\n",
    "for gcp in gcps_utm[:5]:\n",
    "    print(f\"  {gcp.name}: X={gcp.x:.2f}, Y={gcp.y:.2f}, Z={gcp.z:.2f}\")\n",
    "\n",
    "# Get bounds\n",
    "min_x, min_y, max_x, max_y = gcp_parser.get_bounds()\n",
    "print(f\"\\nGCP Bounds (UTM): X=[{min_x:.2f}, {max_x:.2f}], Y=[{min_y:.2f}, {max_y:.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Process DJI Metadata from All Directories\n",
    "\n",
    "Each directory contains nav, obs, bin, and MRK files that provide camera positions, orientations, and timestamps. We'll parse these to potentially enhance the processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Processing metadata from: DJI_202510060955_017_25-3288\n",
      "Parsing timestamp file: /Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25/DJI_202510060955_017_25-3288/DJI_202510060955_017_25-3288_Timestamp.MRK\n",
      "Parsing navigation file: /Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25/DJI_202510060955_017_25-3288/DJI_202510060955_017_25-3288_PPKNAV.nav\n",
      "  ‚úì Parsed 0 timestamps\n",
      "  ‚úì Parsed 0 navigation entries\n",
      "  ‚úì Found 543 images\n",
      "\n",
      "üìÇ Processing metadata from: DJI_202510060955_018_25-3288\n",
      "Parsing timestamp file: /Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25/DJI_202510060955_018_25-3288/DJI_202510060955_018_25-3288_Timestamp.MRK\n",
      "Parsing navigation file: /Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25/DJI_202510060955_018_25-3288/DJI_202510060955_018_25-3288_PPKNAV.nav\n",
      "  ‚úì Parsed 0 timestamps\n",
      "  ‚úì Parsed 0 navigation entries\n",
      "  ‚úì Found 865 images\n",
      "\n",
      "üìÇ Processing metadata from: DJI_202510060955_019_25-3288\n",
      "Parsing timestamp file: /Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25/DJI_202510060955_019_25-3288/DJI_202510060955_019_Timestamp.MRK\n",
      "Parsing navigation file: /Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25/DJI_202510060955_019_25-3288/DJI_202510060955_019_PPKNAV.nav\n",
      "  ‚úì Parsed 0 timestamps\n",
      "  ‚úì Parsed 0 navigation entries\n",
      "  ‚úì Found 528 images\n",
      "\n",
      "‚úì Total images across all directories: 1936\n",
      "‚úì Processed metadata from 3 directories\n"
     ]
    }
   ],
   "source": [
    "# Process DJI metadata from all three directories\n",
    "dataset_dirs = [\n",
    "    data_dir / \"DJI_202510060955_017_25-3288\",\n",
    "    data_dir / \"DJI_202510060955_018_25-3288\",\n",
    "    data_dir / \"DJI_202510060955_019_25-3288\",\n",
    "]\n",
    "\n",
    "dji_metadata_parsers = {}\n",
    "total_images = 0\n",
    "\n",
    "for dataset_dir in dataset_dirs:\n",
    "    if dataset_dir.exists():\n",
    "        print(f\"\\nüìÇ Processing metadata from: {dataset_dir.name}\")\n",
    "        try:\n",
    "            parser = DJIMetadataParser(str(dataset_dir))\n",
    "            dji_metadata_parsers[dataset_dir.name] = parser\n",
    "            \n",
    "            # Count images in this directory\n",
    "            images = list(dataset_dir.glob(\"*.jpg\")) + list(dataset_dir.glob(\"*.JPG\"))\n",
    "            total_images += len(images)\n",
    "            \n",
    "            print(f\"  ‚úì Parsed {len(parser.timestamps)} timestamps\")\n",
    "            print(f\"  ‚úì Parsed {len(parser.nav_data)} navigation entries\")\n",
    "            print(f\"  ‚úì Found {len(images)} images\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Error processing metadata: {e}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  Directory not found: {dataset_dir}\")\n",
    "\n",
    "print(f\"\\n‚úì Total images across all directories: {total_images}\")\n",
    "print(f\"‚úì Processed metadata from {len(dji_metadata_parsers)} directories\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert GCPs to WGS84 Lat/Lon for MetaShape\n",
    "\n",
    "MetaShape expects GCPs in WGS84 lat/lon format. We need to convert from UTM Zone 10N.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 23 GCPs to WGS84 lat/lon\n",
      "\n",
      "First few GCPs (WGS84):\n",
      "  GCP1: (49.211262, -122.905068, z=77.45)\n",
      "  GCP2: (49.209326, -122.908591, z=79.22)\n",
      "  GCP3: (49.207078, -122.909694, z=59.40)\n",
      "  GCP4: (49.207963, -122.907122, z=65.59)\n",
      "  GCP5: (49.209197, -122.904907, z=63.10)\n"
     ]
    }
   ],
   "source": [
    "# Convert UTM to WGS84 lat/lon\n",
    "# NOTE: In the CSV, X column is actually Northing (~5.45M) and Y column is Easting (~500k)\n",
    "# utm.to_latlon expects (easting, northing), so we need to swap them\n",
    "gcps_wgs84 = []\n",
    "\n",
    "for gcp in gcps_utm:\n",
    "    # Convert UTM to lat/lon (UTM Zone 10N)\n",
    "    # X is Northing, Y is Easting\n",
    "    lat, lon = utm.to_latlon(gcp.y, gcp.x, 10, 'N')\n",
    "    \n",
    "    gcp_dict = {\n",
    "        'id': gcp.name,\n",
    "        'label': gcp.name,\n",
    "        'lat': lat,\n",
    "        'lon': lon,\n",
    "        'z': gcp.z,\n",
    "        'accuracy': 0.1  # Default accuracy in meters\n",
    "    }\n",
    "    gcps_wgs84.append(gcp_dict)\n",
    "\n",
    "print(f\"Converted {len(gcps_wgs84)} GCPs to WGS84 lat/lon\")\n",
    "print(\"\\nFirst few GCPs (WGS84):\")\n",
    "for gcp in gcps_wgs84[:5]:\n",
    "    print(f\"  {gcp['id']}: ({gcp['lat']:.6f}, {gcp['lon']:.6f}, z={gcp['z']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Export GCPs for MetaShape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 23 GCPs to MetaShape XML: outputs/gcps/gcps_metashape.xml\n",
      "‚úì GCPs exported to XML: outputs/gcps/gcps_metashape.xml\n",
      "Exported 23 GCPs to MetaShape CSV: outputs/gcps/gcps_metashape.csv\n",
      "‚úì GCPs also exported to CSV: outputs/gcps/gcps_metashape.csv\n",
      "‚úì Using CSV format for GCP import: outputs/gcps/gcps_metashape.csv\n",
      "‚úì Using CSV format for GCP import: outputs/gcps/gcps_metashape.csv\n"
     ]
    }
   ],
   "source": [
    "# Create output directory for GCP files\n",
    "gcp_output_dir = output_dir / \"gcps\"\n",
    "gcp_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export to MetaShape XML format (preferred by MetaShape)\n",
    "if USE_QUALICUM_PACKAGE:\n",
    "    gcp_xml_path = gcp_output_dir / \"gcps_metashape.xml\"\n",
    "    export_to_metashape_xml(gcps_wgs84, str(gcp_xml_path))\n",
    "    print(f\"‚úì GCPs exported to XML: {gcp_xml_path}\")\n",
    "    \n",
    "    # Also export CSV for reference\n",
    "    gcp_csv_path = gcp_output_dir / \"gcps_metashape.csv\"\n",
    "    export_to_metashape_csv(gcps_wgs84, str(gcp_csv_path))\n",
    "    print(f\"‚úì GCPs also exported to CSV: {gcp_csv_path}\")\n",
    "else:\n",
    "    # Define export functions locally if qualicum package not available\n",
    "    def export_to_metashape_xml_local(gcps, output_path):\n",
    "        output_path = Path(output_path)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        root = ET.Element('document')\n",
    "        chunks = ET.SubElement(root, 'chunks')\n",
    "        chunk = ET.SubElement(chunks, 'chunk')\n",
    "        markers = ET.SubElement(chunk, 'markers')\n",
    "        \n",
    "        for gcp in gcps:\n",
    "            marker = ET.SubElement(markers, 'marker')\n",
    "            marker.set('label', gcp.get('id', gcp.get('label', 'GCP')))\n",
    "            marker.set('reference', 'true')\n",
    "            \n",
    "            position = ET.SubElement(marker, 'position')\n",
    "            position.set('x', str(gcp.get('lon', 0.0)))\n",
    "            position.set('y', str(gcp.get('lat', 0.0)))\n",
    "            position.set('z', str(gcp.get('z', 0.0)))\n",
    "            \n",
    "            accuracy = gcp.get('accuracy', 1.0)\n",
    "            accuracy_elem = ET.SubElement(marker, 'accuracy')\n",
    "            accuracy_elem.set('x', str(accuracy))\n",
    "            accuracy_elem.set('y', str(accuracy))\n",
    "            accuracy_elem.set('z', str(accuracy))\n",
    "        \n",
    "        tree = ET.ElementTree(root)\n",
    "        ET.indent(tree, space='  ')\n",
    "        tree.write(output_path, encoding='utf-8', xml_declaration=True)\n",
    "        print(f\"Exported {len(gcps)} GCPs to MetaShape XML: {output_path}\")\n",
    "        return str(output_path)\n",
    "    \n",
    "    def export_to_metashape_csv_local(gcps, output_path):\n",
    "        output_path = Path(output_path)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(output_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f, delimiter='\\t')\n",
    "            writer.writerow(['Label', 'X', 'Y', 'Z', 'Accuracy', 'Enabled'])\n",
    "            \n",
    "            for gcp in gcps:\n",
    "                label = gcp.get('id', gcp.get('label', 'GCP'))\n",
    "                lon = gcp.get('lon', 0.0)\n",
    "                lat = gcp.get('lat', 0.0)\n",
    "                z = gcp.get('z', 0.0)\n",
    "                # Use very low accuracy (0.01m = 1cm) for high weight in MetaShape bundle adjustment\n",
    "                # Lower accuracy values = higher weight in bundle adjustment\n",
    "                accuracy = gcp.get('accuracy', 0.01)\n",
    "                writer.writerow([label, lon, lat, z, accuracy, '1'])\n",
    "        \n",
    "        print(f\"Exported {len(gcps)} GCPs to MetaShape CSV: {output_path}\")\n",
    "        return str(output_path)\n",
    "    \n",
    "    gcp_xml_path = gcp_output_dir / \"gcps_metashape.xml\"\n",
    "    export_to_metashape_xml_local(gcps_wgs84, str(gcp_xml_path))\n",
    "    print(f\"‚úì GCPs exported to XML: {gcp_xml_path}\")\n",
    "    \n",
    "    gcp_csv_path = gcp_output_dir / \"gcps_metashape.csv\"\n",
    "    export_to_metashape_csv_local(gcps_wgs84, str(gcp_csv_path))\n",
    "    print(f\"‚úì GCPs also exported to CSV: {gcp_csv_path}\")\n",
    "\n",
    "# Use CSV file for processing (more reliable than XML)\n",
    "# If CSV doesn't exist, fall back to XML\n",
    "if gcp_csv_path.exists():\n",
    "    gcp_file_for_processing = gcp_csv_path\n",
    "    print(f\"‚úì Using CSV format for GCP import: {gcp_csv_path}\")\n",
    "else:\n",
    "    gcp_file_for_processing = gcp_xml_path\n",
    "    print(f\"‚úì Using XML format for GCP import: {gcp_xml_path}\")\n",
    "\n",
    "# Use CSV file for processing (more reliable than XML)\n",
    "# If CSV doesn't exist, fall back to XML\n",
    "if gcp_csv_path.exists():\n",
    "    gcp_file_for_processing = gcp_csv_path\n",
    "    print(f\"‚úì Using CSV format for GCP import: {gcp_csv_path}\")\n",
    "else:\n",
    "    gcp_file_for_processing = gcp_xml_path\n",
    "    print(f\"‚úì Using XML format for GCP import: {gcp_xml_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using MetaShape processor from qualicum_beach_gcp_analysis\n"
     ]
    }
   ],
   "source": [
    "# Define MetaShape processing functions if qualicum package not available\n",
    "# USE_STANDARD_PIPELINE_PARAMETERS indicates if we're using the qualicum package's standard pipeline\n",
    "# If not available, we define the functions locally with the same parameters\n",
    "try:\n",
    "    USE_STANDARD_PIPELINE_PARAMETERS = USE_QUALICUM_PACKAGE\n",
    "except NameError:\n",
    "    USE_STANDARD_PIPELINE_PARAMETERS = False\n",
    "\n",
    "# Check if MetaShape is available (defined in setup cell)\n",
    "try:\n",
    "    _ = METASHAPE_AVAILABLE\n",
    "except NameError:\n",
    "    METASHAPE_AVAILABLE = False\n",
    "\n",
    "if not USE_STANDARD_PIPELINE_PARAMETERS and METASHAPE_AVAILABLE:\n",
    "    from enum import IntEnum\n",
    "    from typing import Optional\n",
    "    from contextlib import contextmanager\n",
    "    \n",
    "    class PhotoMatchQuality(IntEnum):\n",
    "        LowestQuality = 0\n",
    "        LowQuality = 1\n",
    "        MediumQuality = 2\n",
    "        HighQuality = 4\n",
    "        HighestQuality = 8\n",
    "    \n",
    "    class DepthMapQuality(IntEnum):\n",
    "        LowestQuality = 1\n",
    "        LowQuality = 2\n",
    "        MediumQuality = 4\n",
    "        HighQuality = 8\n",
    "        UltraQuality = 16\n",
    "    \n",
    "    @contextmanager\n",
    "    def redirect_metashape_output(log_file_path: Path):\n",
    "        \"\"\"Context manager to redirect MetaShape's stdout/stderr to a log file.\n",
    "        \n",
    "        MetaShape prints verbose output to stdout/stderr. This function redirects\n",
    "        that output to a log file while keeping minimal progress info in the notebook.\n",
    "        \"\"\"\n",
    "        log_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # Open in write mode to start fresh each time (or append if you want to keep history)\n",
    "        log_file = open(log_file_path, 'w', encoding='utf-8')\n",
    "        original_stdout = sys.stdout\n",
    "        original_stderr = sys.stderr\n",
    "        \n",
    "        # Create a Tee-like object that writes to both file and original stdout for progress\n",
    "        class TeeOutput:\n",
    "            def __init__(self, file, original):\n",
    "                self.file = file\n",
    "                self.original = original\n",
    "                self.buffer = []\n",
    "            \n",
    "            def write(self, text):\n",
    "                # Always write to log file\n",
    "                self.file.write(text)\n",
    "                self.file.flush()  # Flush immediately so log file is up to date\n",
    "                \n",
    "                # Only show minimal progress in notebook (filter verbose output)\n",
    "                # MetaShape progress lines typically contain \"Progress:\" or percentages\n",
    "                if any(keyword in text for keyword in ['Progress:', '%', '‚úì', '‚úó', '‚ö†Ô∏è', 'üìù', 'üìÇ', 'üì∑', 'üîç', 'üìê', 'üó∫Ô∏è', 'üèóÔ∏è', 'üñºÔ∏è', 'üíæ']):\n",
    "                    # Show progress indicators in notebook\n",
    "                    self.original.write(text)\n",
    "                    self.original.flush()\n",
    "                # Suppress verbose technical output in notebook\n",
    "            \n",
    "            def flush(self):\n",
    "                self.file.flush()\n",
    "                self.original.flush()\n",
    "        \n",
    "        tee_stdout = TeeOutput(log_file, original_stdout)\n",
    "        tee_stderr = TeeOutput(log_file, original_stderr)\n",
    "        \n",
    "        try:\n",
    "            sys.stdout = tee_stdout\n",
    "            sys.stderr = tee_stderr\n",
    "            yield log_file\n",
    "        finally:\n",
    "            sys.stdout = original_stdout\n",
    "            sys.stderr = original_stderr\n",
    "            log_file.flush()\n",
    "            log_file.close()\n",
    "    \n",
    "    def process_orthomosaic(\n",
    "        photos_dir: Path,\n",
    "        output_path: Path,\n",
    "        project_path: Path,\n",
    "        gcp_file: Optional[Path] = None,\n",
    "        product_id: str = \"orthomosaic\",\n",
    "        clean_intermediate_files: bool = False,\n",
    "        photo_match_quality: int = PhotoMatchQuality.MediumQuality,\n",
    "        depth_map_quality: int = DepthMapQuality.MediumQuality,\n",
    "        tiepoint_limit: int = 10000,\n",
    "        use_gcps: bool = False\n",
    "    ) -> dict:\n",
    "        \"\"\"Process orthomosaic using MetaShape with output redirected to log file.\"\"\"\n",
    "        import Metashape\n",
    "        \n",
    "        # Configure GPU\n",
    "        Metashape.app.gpu_mask = ~0\n",
    "        \n",
    "        # Setup paths\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        project_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Setup log file for MetaShape verbose output\n",
    "        log_dir = project_path.parent / \"logs\"\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        log_file_path = log_dir / f\"{product_id}_metashape.log\"\n",
    "        \n",
    "        print(f\"üìù MetaShape verbose output will be saved to: {log_file_path}\")\n",
    "        \n",
    "        # Check if project exists\n",
    "        project_exists = project_path.exists()\n",
    "        \n",
    "        # Helper function to safely save document\n",
    "        def safe_save_document():\n",
    "            \"\"\"Save document, handling read-only errors by recreating the document.\"\"\"\n",
    "            nonlocal doc, chunk\n",
    "            try:\n",
    "                doc.save(str(project_path))\n",
    "            except OSError as e:\n",
    "                if \"read-only\" in str(e).lower() or \"editing is disabled\" in str(e).lower():\n",
    "                    print(f\"  ‚ö†Ô∏è  Document became read-only, recreating...\")\n",
    "                    # Close the read-only document\n",
    "                    try:\n",
    "                        doc.close()\n",
    "                    except:\n",
    "                        pass\n",
    "                    # Create a new writable document and copy the chunk data\n",
    "                    # Note: We can't easily copy chunk data, so we'll just create a new document\n",
    "                    # This means we lose the current processing state, but at least we can continue\n",
    "                    print(f\"  ‚ö†Ô∏è  Warning: Processing state may be lost. Consider using clean_intermediate_files=True next time.\")\n",
    "                    doc = Metashape.Document()\n",
    "                    doc.save(str(project_path))\n",
    "                    # Get the chunk from the new document\n",
    "                    if len(doc.chunks) > 0:\n",
    "                        chunk = doc.chunks[0]\n",
    "                    else:\n",
    "                        chunk = doc.addChunk()\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        # Use context manager to redirect MetaShape output to log file\n",
    "        with redirect_metashape_output(log_file_path):\n",
    "            # Close any existing document first to avoid \"already in use\" error\n",
    "            try:\n",
    "                # Close all open documents\n",
    "                if hasattr(Metashape.app, 'document') and Metashape.app.document is not None:\n",
    "                    Metashape.app.document.close()\n",
    "                # Also try to close via app.documents if available\n",
    "                if hasattr(Metashape.app, 'documents'):\n",
    "                    for d in list(Metashape.app.documents):\n",
    "                        try:\n",
    "                            d.close()\n",
    "                        except:\n",
    "                            pass\n",
    "            except:\n",
    "                pass  # Ignore errors when closing\n",
    "            \n",
    "            doc = None\n",
    "            if project_exists and not clean_intermediate_files:\n",
    "                print(f\"üìÇ Loading existing project: {project_path}\")\n",
    "                doc = Metashape.Document()\n",
    "                try:\n",
    "                    doc.open(str(project_path))\n",
    "                    # Check if document is read-only - if so, we need to close and recreate\n",
    "                    # Try a test save to see if it's writable\n",
    "                    try:\n",
    "                        doc.save(str(project_path))\n",
    "                        doc_readonly = False\n",
    "                    except OSError as e:\n",
    "                        if \"read-only\" in str(e).lower() or \"editing is disabled\" in str(e).lower():\n",
    "                            print(f\"  ‚ö†Ô∏è  Document is read-only, closing and creating new project...\")\n",
    "                            doc.close()\n",
    "                            doc = None\n",
    "                            doc_readonly = True\n",
    "                        else:\n",
    "                            raise\n",
    "                    \n",
    "                    if doc is not None and not doc_readonly:\n",
    "                        if len(doc.chunks) > 0:\n",
    "                            chunk = doc.chunks[0]\n",
    "                            print(f\"  ‚úì Found existing chunk with {len(chunk.cameras)} cameras\")\n",
    "                        else:\n",
    "                            chunk = doc.addChunk()\n",
    "                            print(\"  ‚úì Created new chunk\")\n",
    "                except Exception as e:\n",
    "                    # If open fails (e.g., file locked), try to create new project\n",
    "                    print(f\"  ‚ö†Ô∏è  Could not open existing project: {e}\")\n",
    "                    print(\"  Creating new project instead...\")\n",
    "                    if doc is not None:\n",
    "                        try:\n",
    "                            doc.close()\n",
    "                        except:\n",
    "                            pass\n",
    "                    doc = None\n",
    "            \n",
    "            # Create new document if we don't have one yet\n",
    "            if doc is None:\n",
    "                if clean_intermediate_files and project_exists:\n",
    "                    try:\n",
    "                        project_path.unlink()\n",
    "                    except:\n",
    "                        pass  # Ignore if can't delete\n",
    "                print(\"üöÄ Creating new MetaShape project...\")\n",
    "                doc = Metashape.Document()\n",
    "                doc.save(str(project_path))\n",
    "                chunk = doc.addChunk()\n",
    "            \n",
    "            # Add photos\n",
    "            if len(chunk.cameras) == 0:\n",
    "                print(f\"üì∑ Adding photos from: {photos_dir}\")\n",
    "                photos = list(photos_dir.glob(\"*.jpg\")) + list(photos_dir.glob(\"*.JPG\"))\n",
    "                if not photos:\n",
    "                    raise ValueError(f\"No images found in {photos_dir}\")\n",
    "                chunk.addPhotos([str(p) for p in photos])\n",
    "                safe_save_document()  # Save after adding photos\n",
    "                print(f\"  ‚úì Added {len(photos)} photos\")\n",
    "            else:\n",
    "                print(f\"  ‚úì Photos already added ({len(chunk.cameras)} cameras)\")\n",
    "            \n",
    "            # Add GCPs if requested\n",
    "            if use_gcps and gcp_file and gcp_file.exists():\n",
    "                if len(chunk.markers) == 0:\n",
    "                    print(f\"üìç Loading GCPs from: {gcp_file}\")\n",
    "                    \n",
    "                    # Check file extension to determine format\n",
    "                    file_ext = Path(gcp_file).suffix.lower()\n",
    "                    markers_added = 0\n",
    "                    \n",
    "                    if file_ext == '.xml':\n",
    "                        # Try XML format first\n",
    "                        try:\n",
    "                            chunk.importMarkers(str(gcp_file))\n",
    "                            markers_added = len(chunk.markers)\n",
    "                            print(f\"  ‚úì Added {markers_added} markers from XML\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"  ‚ö†Ô∏è  XML import failed: {e}\")\n",
    "                            print(f\"  Trying CSV format instead...\")\n",
    "                            # Fall back to CSV if XML fails\n",
    "                            file_ext = '.csv'\n",
    "                    \n",
    "                    if file_ext == '.csv' or file_ext == '.txt':\n",
    "                        # CSV format - read and add markers manually\n",
    "                        import csv\n",
    "                        with open(gcp_file, 'r', encoding='utf-8') as f:\n",
    "                            reader = csv.DictReader(f, delimiter='\\t')\n",
    "                            for row in reader:\n",
    "                                try:\n",
    "                                    label = row.get('Label', row.get('label', ''))\n",
    "                                    x = float(row.get('X', row.get('x', 0)))\n",
    "                                    y = float(row.get('Y', row.get('y', 0)))\n",
    "                                    z = float(row.get('Z', row.get('z', 0)))\n",
    "                                    accuracy = float(row.get('Accuracy', row.get('accuracy', 1.0)))\n",
    "                                    enabled = row.get('Enabled', row.get('enabled', '1')).strip() == '1'\n",
    "                                    \n",
    "                                    # Create marker\n",
    "                                    marker = chunk.addMarker()\n",
    "                                    marker.label = label\n",
    "                                    marker.reference.location = Metashape.Vector([x, y, z])\n",
    "                                    marker.reference.accuracy = Metashape.Vector([accuracy, accuracy, accuracy])\n",
    "                                    marker.reference.enabled = enabled\n",
    "                                    markers_added += 1\n",
    "                                except Exception as e:\n",
    "                                    print(f\"  ‚ö†Ô∏è  Error adding marker from row: {e}\")\n",
    "                                    continue\n",
    "                        \n",
    "                        if markers_added > 0:\n",
    "                            print(f\"  ‚úì Added {markers_added} markers from CSV\")\n",
    "                        else:\n",
    "                            raise ValueError(f\"Failed to add any markers from {gcp_file}\")\n",
    "                    \n",
    "                    safe_save_document()  # Save after adding GCPs\n",
    "                    print(f\"  ‚úì Total markers: {len(chunk.markers)}\")\n",
    "                else:\n",
    "                    print(f\"  ‚úì GCPs already loaded ({len(chunk.markers)} markers)\")\n",
    "            \n",
    "            # Match photos\n",
    "            # Check if tie points exist - MetaShape TiePoints object may not support len()\n",
    "            # Also check if cameras are already aligned (which requires tie points)\n",
    "            tie_points_exist = chunk.tie_points is not None\n",
    "            cameras_aligned = sum(1 for cam in chunk.cameras if cam.transform) > 0\n",
    "            \n",
    "            # If cameras are aligned, we definitely have tie points (even if we can't count them)\n",
    "            if cameras_aligned:\n",
    "                tie_points_exist = True\n",
    "                tie_points_count = 1  # Don't need exact count, just know they exist\n",
    "            elif tie_points_exist:\n",
    "                # Try to get count, but handle case where len() doesn't work on TiePoints object\n",
    "                try:\n",
    "                    tie_points_count = len(chunk.tie_points)\n",
    "                except (TypeError, AttributeError):\n",
    "                    # If len() doesn't work, check if we can iterate or get a count attribute\n",
    "                    try:\n",
    "                        # Try to get point count if available\n",
    "                        tie_points_count = chunk.tie_points.point_count if hasattr(chunk.tie_points, 'point_count') else None\n",
    "                        if tie_points_count is None:\n",
    "                            # Try to check if it's iterable and count items\n",
    "                            try:\n",
    "                                tie_points_count = sum(1 for _ in chunk.tie_points)\n",
    "                            except:\n",
    "                                tie_points_count = 1  # Assume it has points if it exists\n",
    "                    except:\n",
    "                        tie_points_count = 1  # Assume it has points if it exists\n",
    "            else:\n",
    "                tie_points_count = 0\n",
    "            \n",
    "            if not tie_points_exist or tie_points_count == 0:\n",
    "                print(\"üîç Matching photos... (this may take a while)\")\n",
    "                chunk.matchPhotos(\n",
    "                    downscale=photo_match_quality,\n",
    "                    tiepoint_limit=tiepoint_limit,\n",
    "                )\n",
    "                safe_save_document()  # Save after matching photos\n",
    "                # Try to get count after matching\n",
    "                if chunk.tie_points is not None:\n",
    "                    try:\n",
    "                        tie_points_count = len(chunk.tie_points)\n",
    "                    except (TypeError, AttributeError):\n",
    "                        try:\n",
    "                            tie_points_count = chunk.tie_points.point_count if hasattr(chunk.tie_points, 'point_count') else 0\n",
    "                        except:\n",
    "                            try:\n",
    "                                tie_points_count = sum(1 for _ in chunk.tie_points)\n",
    "                            except:\n",
    "                                tie_points_count = 0\n",
    "                else:\n",
    "                    tie_points_count = 0\n",
    "                print(f\"  ‚úì Photo matching complete ({tie_points_count} tie points)\")\n",
    "            else:\n",
    "                print(f\"  ‚úì Photos already matched ({tie_points_count} tie points)\")\n",
    "            \n",
    "            # Align cameras\n",
    "            aligned = sum(1 for cam in chunk.cameras if cam.transform)\n",
    "            cameras_aligned = aligned > 0\n",
    "            if aligned == 0:\n",
    "                print(\"üìê Aligning cameras... (this may take a while)\")\n",
    "                chunk.alignCameras()\n",
    "                safe_save_document()\n",
    "                aligned = sum(1 for cam in chunk.cameras if cam.transform)\n",
    "                cameras_aligned = aligned > 0\n",
    "                print(f\"  ‚úì Camera alignment complete ({aligned}/{len(chunk.cameras)} cameras aligned)\")\n",
    "            else:\n",
    "                print(f\"  ‚úì Cameras already aligned ({aligned}/{len(chunk.cameras)} cameras)\")\n",
    "            \n",
    "            # Build depth maps\n",
    "            # Check if depth maps exist - cameras must be aligned first\n",
    "            # DepthMaps object doesn't support len(), so just check if it exists\n",
    "            if not cameras_aligned:\n",
    "                print(\"  ‚ö†Ô∏è  Skipping depth maps - cameras not aligned yet\")\n",
    "                depth_maps_exist = False\n",
    "            else:\n",
    "                depth_maps_exist = chunk.depth_maps is not None\n",
    "                if not depth_maps_exist:\n",
    "                    print(\"üó∫Ô∏è  Building depth maps... (this may take a while)\")\n",
    "                    chunk.buildDepthMaps(\n",
    "                        downscale=depth_map_quality,\n",
    "                        filter_mode=Metashape.MildFiltering\n",
    "                    )\n",
    "                    safe_save_document()  # Save after building depth maps\n",
    "                    depth_maps_exist = chunk.depth_maps is not None\n",
    "                    if depth_maps_exist:\n",
    "                        # Try to get count for display, but handle if len() doesn't work\n",
    "                        try:\n",
    "                            depth_maps_count = len(chunk.depth_maps)\n",
    "                        except (TypeError, AttributeError):\n",
    "                            # Try alternative methods to get count\n",
    "                            try:\n",
    "                                depth_maps_count = chunk.depth_maps.count if hasattr(chunk.depth_maps, 'count') else None\n",
    "                            except:\n",
    "                                depth_maps_count = None\n",
    "                            if depth_maps_count is None:\n",
    "                                try:\n",
    "                                    depth_maps_count = sum(1 for _ in chunk.depth_maps)\n",
    "                                except:\n",
    "                                    depth_maps_count = \"?\"\n",
    "                        print(f\"  ‚úì Depth maps built ({depth_maps_count} depth maps)\" if depth_maps_count != \"?\" else \"  ‚úì Depth maps built\")\n",
    "                    else:\n",
    "                        print(\"  ‚ö†Ô∏è  Depth map building may have failed\")\n",
    "                else:\n",
    "                    # Try to get count for display\n",
    "                    try:\n",
    "                        depth_maps_count = len(chunk.depth_maps)\n",
    "                    except (TypeError, AttributeError):\n",
    "                        try:\n",
    "                            depth_maps_count = chunk.depth_maps.count if hasattr(chunk.depth_maps, 'count') else \"?\"\n",
    "                        except:\n",
    "                            depth_maps_count = \"?\"\n",
    "                        if depth_maps_count == \"?\":\n",
    "                            try:\n",
    "                                depth_maps_count = sum(1 for _ in chunk.depth_maps)\n",
    "                            except:\n",
    "                                depth_maps_count = \"?\"\n",
    "                    print(f\"  ‚úì Depth maps already built ({depth_maps_count} depth maps)\" if depth_maps_count != \"?\" else \"  ‚úì Depth maps already built\")\n",
    "            \n",
    "            # Build model\n",
    "            # Model requires depth maps\n",
    "            if not depth_maps_exist:\n",
    "                print(\"  ‚ö†Ô∏è  Skipping model - depth maps not built yet\")\n",
    "            elif chunk.model is None:\n",
    "                print(\"üèóÔ∏è  Building 3D model... (this may take a while)\")\n",
    "                chunk.buildModel()\n",
    "                safe_save_document()  # Save after building model\n",
    "                print(\"  ‚úì 3D model built\")\n",
    "            else:\n",
    "                print(\"  ‚úì 3D model already built\")\n",
    "            \n",
    "            # Build orthomosaic\n",
    "            # Orthomosaic requires model\n",
    "            if chunk.model is None:\n",
    "                print(\"  ‚ö†Ô∏è  Skipping orthomosaic - model not built yet\")\n",
    "            elif chunk.orthomosaic is None:\n",
    "                print(\"üñºÔ∏è  Building orthomosaic... (this may take a while)\")\n",
    "                chunk.buildOrthomosaic()\n",
    "                safe_save_document()  # Save after building orthomosaic\n",
    "                print(\"  ‚úì Orthomosaic built\")\n",
    "            else:\n",
    "                print(\"  ‚úì Orthomosaic already built\")\n",
    "            \n",
    "            # Export GeoTIFF\n",
    "            ortho_path = output_path / f\"{product_id}.tif\"\n",
    "            if not ortho_path.exists() or clean_intermediate_files:\n",
    "                print(f\"üíæ Exporting GeoTIFF to: {ortho_path}\")\n",
    "                compression = Metashape.ImageCompression()\n",
    "                compression.tiff_compression = Metashape.ImageCompression.TiffCompressionNone\n",
    "                compression.tiff_big = True\n",
    "                chunk.exportRaster(str(ortho_path), image_compression=compression)\n",
    "                safe_save_document()\n",
    "                if ortho_path.exists():\n",
    "                    file_size_mb = ortho_path.stat().st_size / (1024 * 1024)\n",
    "                    print(f\"  ‚úì GeoTIFF exported ({file_size_mb:.2f} MB)\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è  Export completed but file not found at: {ortho_path}\")\n",
    "            else:\n",
    "                file_size_mb = ortho_path.stat().st_size / (1024 * 1024)\n",
    "                print(f\"  ‚úì GeoTIFF already exists ({file_size_mb:.2f} MB)\")\n",
    "        \n",
    "        stats = {\n",
    "            'product_id': product_id,\n",
    "            'use_gcps': use_gcps,\n",
    "            'num_photos': len(chunk.cameras),\n",
    "            'num_markers': len(chunk.markers),\n",
    "            'ortho_path': str(ortho_path),\n",
    "            'project_path': str(project_path),\n",
    "            'log_file_path': str(log_file_path),\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ Processing complete! Log file: {log_file_path}\")\n",
    "        return stats\n",
    "    \n",
    "    print(\"‚úì MetaShape processing functions defined locally\")\n",
    "elif not METASHAPE_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è  MetaShape not available. Cannot process orthomosaics.\")\n",
    "else:\n",
    "    print(\"‚úì Using MetaShape processor from qualicum_beach_gcp_analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Processing Combined Orthomosaic - WITH GCPs...\n",
      "============================================================\n",
      "Combining images from:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset_dirs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCombining images from:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_dir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataset_dirs\u001b[49m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dataset_dir\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     22\u001b[0m         images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dataset_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(dataset_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.JPG\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_dirs' is not defined"
     ]
    }
   ],
   "source": [
    "# Check if METASHAPE_AVAILABLE is defined (from setup cell)\n",
    "try:\n",
    "    _ = METASHAPE_AVAILABLE\n",
    "except NameError:\n",
    "    METASHAPE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  METASHAPE_AVAILABLE not defined. Assuming MetaShape is not available.\")\n",
    "    print(\"   Please run the setup cell first to check for MetaShape installation.\")\n",
    "\n",
    "if not METASHAPE_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è  MetaShape not available. Skipping processing.\")\n",
    "else:\n",
    "    # Setup paths\n",
    "    intermediate_dir = output_dir / \"intermediate\"\n",
    "    ortho_output_dir = output_dir / \"orthomosaics\"\n",
    "    # Process orthomosaic WITH GCPs (using all three directories)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Processing Combined Orthomosaic - WITH GCPs...\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Combining images from:\")\n",
    "    # Define data_dir if not already defined\n",
    "    try:\n",
    "        _ = data_dir\n",
    "    except NameError:\n",
    "        # Default data directory path\n",
    "        data_dir = Path(\"/Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25\")\n",
    "        print(f\"‚ÑπÔ∏è  data_dir not defined, using default: {data_dir}\")\n",
    "    \n",
    "    # Define dataset_dirs if not already defined (from Step 2)\n",
    "    try:\n",
    "        _ = dataset_dirs\n",
    "    except NameError:\n",
    "        # Define the three dataset directories\n",
    "        dataset_dirs = [\n",
    "            data_dir / \"DJI_202510060955_017_25-3288\",\n",
    "            data_dir / \"DJI_202510060955_018_25-3288\",\n",
    "            data_dir / \"DJI_202510060955_019_25-3288\",\n",
    "        ]\n",
    "        print(\"‚ÑπÔ∏è  dataset_dirs not defined, using default paths from Step 2\")\n",
    "    \n",
    "    for dataset_dir in dataset_dirs:\n",
    "        if dataset_dir.exists():\n",
    "            images = list(dataset_dir.glob(\"*.jpg\")) + list(dataset_dir.glob(\"*.JPG\"))\n",
    "            print(f\"  - {dataset_dir.name}: {len(images)} images\")\n",
    "    \n",
    "    project_path_with_gcps = intermediate_dir / \"combined_with_gcps.psx\"\n",
    "    \n",
    "    # When using qualicum package, create temporary combined directory\n",
    "    # since it expects a single directory, not a list\n",
    "    if USE_QUALICUM_PACKAGE:\n",
    "        import tempfile\n",
    "        import shutil\n",
    "        from pathlib import Path\n",
    "        \n",
    "        # Create temporary directory for combined images\n",
    "        temp_combined_dir = Path(tempfile.mkdtemp(prefix=\"combined_images_\"))\n",
    "        print(f\"üìÅ Created temporary combined directory: {temp_combined_dir}\")\n",
    "        \n",
    "        # Collect all images from all directories and create symlinks\n",
    "        total_images = 0\n",
    "        for dataset_dir in dataset_dirs:\n",
    "            if dataset_dir.exists():\n",
    "                images = list(dataset_dir.glob(\"*.jpg\")) + list(dataset_dir.glob(\"*.JPG\"))\n",
    "                for img_path in images:\n",
    "                    # Create symlink in temp directory\n",
    "                    symlink_path = temp_combined_dir / img_path.name\n",
    "                    # Handle name conflicts by including parent dir name\n",
    "                    if symlink_path.exists():\n",
    "                        symlink_path = temp_combined_dir / f\"{dataset_dir.name}_{img_path.name}\"\n",
    "                    symlink_path.symlink_to(img_path.resolve())\n",
    "                    total_images += 1\n",
    "        \n",
    "        print(f\"  ‚úì Created {total_images} symlinks in temporary directory\")\n",
    "        photos_dir_for_qualicum = temp_combined_dir\n",
    "    else:\n",
    "        photos_dir_for_qualicum = dataset_dirs\n",
    "    \n",
    "    if USE_QUALICUM_PACKAGE:\n",
    "        stats_with_gcps = process_orthomosaic(\n",
    "            photos_dir=photos_dir_for_qualicum,\n",
    "            output_path=ortho_output_dir,\n",
    "            project_path=project_path_with_gcps,\n",
    "            gcp_file=gcp_file_for_processing,\n",
    "            product_id=\"combined_with_gcps\",\n",
    "            clean_intermediate_files=False,\n",
    "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
    "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
    "            tiepoint_limit=10000,\n",
    "            use_gcps=True,\n",
    "            gcp_accuracy=0.01  # Very low accuracy (1cm) for very high weight in bundle adjustment\n",
    "        )\n",
    "    else:\n",
    "        stats_with_gcps = process_orthomosaic(\n",
    "            photos_dir=dataset_dirs,\n",
    "            output_path=ortho_output_dir,\n",
    "            project_path=project_path_with_gcps,\n",
    "            gcp_file=gcp_file_for_processing,\n",
    "            product_id=\"combined_with_gcps\",\n",
    "            clean_intermediate_files=False,\n",
    "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
    "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
    "            tiepoint_limit=10000,\n",
    "            use_gcps=True,\n",
    "            gcp_accuracy=0.01  # Very low accuracy (1cm) for very high weight in bundle adjustment\n",
    "        )\n",
    "    \n",
    "    print(\"\\n‚úì Combined orthomosaic processing (with GCPs) complete!\")\n",
    "    print(f\"  Number of photos: {stats_with_gcps['num_photos']}\")\n",
    "    print(f\"  Number of markers: {stats_with_gcps.get('num_markers', 0)}\")\n",
    "    print(f\"  Orthomosaic: {stats_with_gcps['ortho_path']}\")\n",
    "    \n",
    "    ortho_with_gcps_path = Path(stats_with_gcps['ortho_path'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Process Dataset 1 - WITH GCPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if METASHAPE_AVAILABLE is defined (from setup cell)\n",
    "try:\n",
    "    _ = METASHAPE_AVAILABLE\n",
    "except NameError:\n",
    "    METASHAPE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  METASHAPE_AVAILABLE not defined. Assuming MetaShape is not available.\")\n",
    "    print(\"   Please run the setup cell first to check for MetaShape installation.\")\n",
    "\n",
    "if not METASHAPE_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è  MetaShape not available. Skipping processing.\")\n",
    "else:\n",
    "    # Process orthomosaic WITH GCPs\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Processing Dataset 1 - WITH GCPs...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    project_path1_with_gcps = intermediate_dir / \"dataset1_with_gcps.psx\"\n",
    "    \n",
    "    if USE_QUALICUM_PACKAGE:\n",
    "        stats1_with_gcps = process_orthomosaic(\n",
    "            photos_dir=dataset1_dir,\n",
    "            output_path=ortho_output_dir,\n",
    "            project_path=project_path1_with_gcps,\n",
    "            gcp_file=gcp_file_for_processing,\n",
    "            product_id=\"dataset1_with_gcps\",\n",
    "            clean_intermediate_files=False,\n",
    "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
    "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
    "            tiepoint_limit=10000,\n",
    "            use_gcps=True\n",
    "        )\n",
    "    else:\n",
    "        stats1_with_gcps = process_orthomosaic(\n",
    "            photos_dir=dataset1_dir,\n",
    "            output_path=ortho_output_dir,\n",
    "            project_path=project_path1_with_gcps,\n",
    "            gcp_file=gcp_file_for_processing,\n",
    "            product_id=\"dataset1_with_gcps\",\n",
    "            clean_intermediate_files=False,\n",
    "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
    "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
    "            tiepoint_limit=10000,\n",
    "            use_gcps=True\n",
    "        )\n",
    "    \n",
    "    print(\"\\n‚úì Dataset 1 processing (with GCPs) complete!\")\n",
    "    print(f\"  Number of photos: {stats1_with_gcps['num_photos']}\")\n",
    "    print(f\"  Number of markers: {stats1_with_gcps.get('num_markers', 0)}\")\n",
    "    print(f\"  Orthomosaic: {stats1_with_gcps['ortho_path']}\")\n",
    "    \n",
    "    ortho1_with_gcps_path = Path(stats1_with_gcps['ortho_path'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Process Dataset 2 - WITHOUT GCPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if METASHAPE_AVAILABLE is defined (from setup cell)\n",
    "try:\n",
    "    _ = METASHAPE_AVAILABLE\n",
    "except NameError:\n",
    "    METASHAPE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  METASHAPE_AVAILABLE not defined. Assuming MetaShape is not available.\")\n",
    "    print(\"   Please run the setup cell first to check for MetaShape installation.\")\n",
    "\n",
    "if not METASHAPE_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è  MetaShape not available. Skipping processing.\")\n",
    "else:\n",
    "    # Setup paths for Dataset 2\n",
    "    dataset2_dir = data_dir / \"DJI_202510060955_019_25-3288\"\n",
    "    \n",
    "    # Process orthomosaic WITHOUT GCPs\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Processing Dataset 2 - WITHOUT GCPs...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    project_path2_no_gcps = intermediate_dir / \"dataset2_no_gcps.psx\"\n",
    "    \n",
    "    if USE_QUALICUM_PACKAGE:\n",
    "        stats2_no_gcps = process_orthomosaic(\n",
    "            photos_dir=dataset2_dir,\n",
    "            output_path=ortho_output_dir,\n",
    "            project_path=project_path2_no_gcps,\n",
    "            product_id=\"dataset2_no_gcps\",\n",
    "            clean_intermediate_files=False,\n",
    "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
    "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
    "            tiepoint_limit=10000,\n",
    "            use_gcps=False\n",
    "        )\n",
    "    else:\n",
    "        stats2_no_gcps = process_orthomosaic(\n",
    "            photos_dir=dataset2_dir,\n",
    "            output_path=ortho_output_dir,\n",
    "            project_path=project_path2_no_gcps,\n",
    "            product_id=\"dataset2_no_gcps\",\n",
    "            clean_intermediate_files=False,\n",
    "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
    "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
    "            tiepoint_limit=10000,\n",
    "            use_gcps=False\n",
    "        )\n",
    "    \n",
    "    print(\"\\n‚úì Dataset 2 processing (without GCPs) complete!\")\n",
    "    print(f\"  Number of photos: {stats2_no_gcps['num_photos']}\")\n",
    "    print(f\"  Orthomosaic: {stats2_no_gcps['ortho_path']}\")\n",
    "    \n",
    "    ortho2_no_gcps_path = Path(stats2_no_gcps['ortho_path'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Process Dataset 2 - WITH GCPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if METASHAPE_AVAILABLE is defined (from setup cell)\n",
    "try:\n",
    "    _ = METASHAPE_AVAILABLE\n",
    "except NameError:\n",
    "    METASHAPE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  METASHAPE_AVAILABLE not defined. Assuming MetaShape is not available.\")\n",
    "    print(\"   Please run the setup cell first to check for MetaShape installation.\")\n",
    "\n",
    "if not METASHAPE_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è  MetaShape not available. Skipping processing.\")\n",
    "else:\n",
    "    # Process orthomosaic WITH GCPs\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Processing Dataset 2 - WITH GCPs...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    project_path2_with_gcps = intermediate_dir / \"dataset2_with_gcps.psx\"\n",
    "    \n",
    "    if USE_QUALICUM_PACKAGE:\n",
    "        stats2_with_gcps = process_orthomosaic(\n",
    "            photos_dir=dataset2_dir,\n",
    "            output_path=ortho_output_dir,\n",
    "            project_path=project_path2_with_gcps,\n",
    "            gcp_file=gcp_file_for_processing,\n",
    "            product_id=\"dataset2_with_gcps\",\n",
    "            clean_intermediate_files=False,\n",
    "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
    "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
    "            tiepoint_limit=10000,\n",
    "            use_gcps=True\n",
    "        )\n",
    "    else:\n",
    "        stats2_with_gcps = process_orthomosaic(\n",
    "            photos_dir=dataset2_dir,\n",
    "            output_path=ortho_output_dir,\n",
    "            project_path=project_path2_with_gcps,\n",
    "            gcp_file=gcp_file_for_processing,\n",
    "            product_id=\"dataset2_with_gcps\",\n",
    "            clean_intermediate_files=False,\n",
    "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
    "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
    "            tiepoint_limit=10000,\n",
    "            use_gcps=True\n",
    "        )\n",
    "    \n",
    "    print(\"\\n‚úì Dataset 2 processing (with GCPs) complete!\")\n",
    "    print(f\"  Number of photos: {stats2_with_gcps['num_photos']}\")\n",
    "    print(f\"  Number of markers: {stats2_with_gcps.get('num_markers', 0)}\")\n",
    "    print(f\"  Orthomosaic: {stats2_with_gcps['ortho_path']}\")\n",
    "    \n",
    "    ortho2_with_gcps_path = Path(stats2_with_gcps['ortho_path'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Download Reference Basemap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bounding box from GCPs for basemap download\n",
    "# Convert UTM bounds to lat/lon\n",
    "center_easting = (min_y + max_y) / 2  # Y column is easting\n",
    "center_northing = (min_x + max_x) / 2  # X column is northing\n",
    "lat_center, lon_center = utm.to_latlon(center_easting, center_northing, 10, 'N')\n",
    "\n",
    "# Convert bounds\n",
    "lat_min, lon_min = utm.to_latlon(min_y, min_x, 10, 'N')\n",
    "lat_max, lon_max = utm.to_latlon(max_y, max_x, 10, 'N')\n",
    "\n",
    "# Add padding\n",
    "padding = 0.001\n",
    "bbox = (lat_min - padding, lon_min - padding, lat_max + padding, lon_max + padding)\n",
    "\n",
    "print(f\"Basemap bounding box: {bbox}\")\n",
    "\n",
    "# Download basemap\n",
    "basemap_path = download_basemap(\n",
    "    bbox=bbox,\n",
    "    output_path=str(output_dir / \"basemap.tif\"),\n",
    "    source=\"esri_world_imagery\",\n",
    "    target_resolution=0.1  # 0.1m per pixel\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Basemap saved to: {basemap_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Compare Orthomosaics to Basemap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all orthomosaics to basemap\n",
    "comparison_dir = output_dir / \"comparisons\"\n",
    "comparison_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Determine orthomosaic paths\n",
    "if 'ortho1_no_gcps_path' not in locals():\n",
    "    ortho_output_dir = output_dir / \"orthomosaics\"\n",
    "    ortho1_no_gcps_path = ortho_output_dir / \"dataset1_no_gcps.tif\"\n",
    "    ortho1_with_gcps_path = ortho_output_dir / \"dataset1_with_gcps.tif\"\n",
    "    ortho2_no_gcps_path = ortho_output_dir / \"dataset2_no_gcps.tif\"\n",
    "    ortho2_with_gcps_path = ortho_output_dir / \"dataset2_with_gcps.tif\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Comparing orthomosaics to basemap...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare Dataset 1 - Without GCPs\n",
    "if ortho1_no_gcps_path.exists():\n",
    "    print(\"\\nDataset 1 - Without GCPs\")\n",
    "    print(\"-\" * 60)\n",
    "    metrics1_no_gcps = compare_orthomosaic_to_basemap(\n",
    "        str(ortho1_no_gcps_path),\n",
    "        str(basemap_path),\n",
    "        output_dir=str(comparison_dir / \"dataset1_no_gcps\")\n",
    "    )\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Orthomosaic not found: {ortho1_no_gcps_path}\")\n",
    "    metrics1_no_gcps = {}\n",
    "\n",
    "# Compare Dataset 1 - With GCPs\n",
    "if ortho1_with_gcps_path.exists():\n",
    "    print(\"\\nDataset 1 - With GCPs\")\n",
    "    print(\"-\" * 60)\n",
    "    metrics1_with_gcps = compare_orthomosaic_to_basemap(\n",
    "        str(ortho1_with_gcps_path),\n",
    "        str(basemap_path),\n",
    "        output_dir=str(comparison_dir / \"dataset1_with_gcps\")\n",
    "    )\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Orthomosaic not found: {ortho1_with_gcps_path}\")\n",
    "    metrics1_with_gcps = {}\n",
    "\n",
    "# Compare Dataset 2 - Without GCPs\n",
    "if ortho2_no_gcps_path.exists():\n",
    "    print(\"\\nDataset 2 - Without GCPs\")\n",
    "    print(\"-\" * 60)\n",
    "    metrics2_no_gcps = compare_orthomosaic_to_basemap(\n",
    "        str(ortho2_no_gcps_path),\n",
    "        str(basemap_path),\n",
    "        output_dir=str(comparison_dir / \"dataset2_no_gcps\")\n",
    "    )\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Orthomosaic not found: {ortho2_no_gcps_path}\")\n",
    "    metrics2_no_gcps = {}\n",
    "\n",
    "# Compare Dataset 2 - With GCPs\n",
    "if ortho2_with_gcps_path.exists():\n",
    "    print(\"\\nDataset 2 - With GCPs\")\n",
    "    print(\"-\" * 60)\n",
    "    metrics2_with_gcps = compare_orthomosaic_to_basemap(\n",
    "        str(ortho2_with_gcps_path),\n",
    "        str(basemap_path),\n",
    "        output_dir=str(comparison_dir / \"dataset2_with_gcps\")\n",
    "    )\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Orthomosaic not found: {ortho2_with_gcps_path}\")\n",
    "    metrics2_with_gcps = {}\n",
    "\n",
    "print(\"\\n‚úì All comparisons complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Generate Quality Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison summary\n",
    "import pandas as pd\n",
    "\n",
    "# Helper function to safely get metrics\n",
    "def get_metric(metrics_dict, key, default=0.0):\n",
    "    \"\"\"Safely get a metric value, handling different key formats.\"\"\"\n",
    "    if not metrics_dict:\n",
    "        return default\n",
    "    # Try direct key\n",
    "    if key in metrics_dict:\n",
    "        return metrics_dict[key]\n",
    "    # Try with _pixels suffix\n",
    "    if f\"{key}_pixels\" in metrics_dict:\n",
    "        return metrics_dict[f\"{key}_pixels\"]\n",
    "    return default\n",
    "\n",
    "# Collect metrics\n",
    "summary_data = {\n",
    "    'Dataset': ['Dataset 1', 'Dataset 1', 'Dataset 2', 'Dataset 2'],\n",
    "    'GCPs Used': ['No', 'Yes', 'No', 'Yes'],\n",
    "    'RMSE': [\n",
    "        get_metric(metrics1_no_gcps, 'rmse'),\n",
    "        get_metric(metrics1_with_gcps, 'rmse'),\n",
    "        get_metric(metrics2_no_gcps, 'rmse'),\n",
    "        get_metric(metrics2_with_gcps, 'rmse'),\n",
    "    ],\n",
    "    'MAE': [\n",
    "        get_metric(metrics1_no_gcps, 'mae'),\n",
    "        get_metric(metrics1_with_gcps, 'mae'),\n",
    "        get_metric(metrics2_no_gcps, 'mae'),\n",
    "        get_metric(metrics2_with_gcps, 'mae'),\n",
    "    ],\n",
    "    'Correlation': [\n",
    "        get_metric(metrics1_no_gcps, 'correlation'),\n",
    "        get_metric(metrics1_with_gcps, 'correlation'),\n",
    "        get_metric(metrics2_no_gcps, 'correlation'),\n",
    "        get_metric(metrics2_with_gcps, 'correlation'),\n",
    "    ],\n",
    "    'SSIM': [\n",
    "        get_metric(metrics1_no_gcps, 'ssim'),\n",
    "        get_metric(metrics1_with_gcps, 'ssim'),\n",
    "        get_metric(metrics2_no_gcps, 'ssim'),\n",
    "        get_metric(metrics2_with_gcps, 'ssim'),\n",
    "    ],\n",
    "    'Displacement (pixels)': [\n",
    "        get_metric(metrics1_no_gcps, 'displacement_magnitude'),\n",
    "        get_metric(metrics1_with_gcps, 'displacement_magnitude'),\n",
    "        get_metric(metrics2_no_gcps, 'displacement_magnitude'),\n",
    "        get_metric(metrics2_with_gcps, 'displacement_magnitude'),\n",
    "    ],\n",
    "    'Num Matches': [\n",
    "        get_metric(metrics1_no_gcps, 'num_matches', default=0),\n",
    "        get_metric(metrics1_with_gcps, 'num_matches', default=0),\n",
    "        get_metric(metrics2_no_gcps, 'num_matches', default=0),\n",
    "        get_metric(metrics2_with_gcps, 'num_matches', default=0),\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_csv = output_dir / \"quality_summary.csv\"\n",
    "df.to_csv(summary_csv, index=False)\n",
    "print(f\"\\n‚úì Summary saved to: {summary_csv}\")\n",
    "\n",
    "# Create visualizations\n",
    "available_metrics = []\n",
    "for col in ['RMSE', 'MAE', 'Correlation', 'SSIM', 'Displacement (pixels)', 'Num Matches']:\n",
    "    if col in df.columns and df[col].sum() > 0:\n",
    "        available_metrics.append(col)\n",
    "\n",
    "if available_metrics:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, metric in enumerate(available_metrics[:6]):\n",
    "        ax = axes[i]\n",
    "        df_pivot = df.pivot(index='Dataset', columns='GCPs Used', values=metric)\n",
    "        df_pivot.plot(kind='bar', ax=ax, rot=0)\n",
    "        ax.set_title(metric)\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend(title='GCPs Used')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(available_metrics), 6):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    report_plot_path = output_dir / \"quality_report.png\"\n",
    "    plt.savefig(report_plot_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"‚úì Quality report plot saved to: {report_plot_path}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No metrics available for visualization\")\n",
    "\n",
    "# Generate markdown report\n",
    "report_md = output_dir / \"quality_report.md\"\n",
    "with open(report_md, 'w') as f:\n",
    "    f.write(\"# Westminster Ground Truth Analysis - Quality Report\\n\\n\")\n",
    "    f.write(\"## Summary\\n\\n\")\n",
    "    f.write(df.to_markdown(index=False))\n",
    "    f.write(\"\\n\\n## Findings\\n\\n\")\n",
    "    f.write(\"### Dataset 1\\n\")\n",
    "    f.write(f\"- **Without GCPs**: RMSE={get_metric(metrics1_no_gcps, 'rmse'):.3f}, \")\n",
    "    f.write(f\"Displacement={get_metric(metrics1_no_gcps, 'displacement_magnitude'):.2f} pixels\\n\")\n",
    "    f.write(f\"- **With GCPs**: RMSE={get_metric(metrics1_with_gcps, 'rmse'):.3f}, \")\n",
    "    f.write(f\"Displacement={get_metric(metrics1_with_gcps, 'displacement_magnitude'):.2f} pixels\\n\\n\")\n",
    "    f.write(\"### Dataset 2\\n\")\n",
    "    f.write(f\"- **Without GCPs**: RMSE={get_metric(metrics2_no_gcps, 'rmse'):.3f}, \")\n",
    "    f.write(f\"Displacement={get_metric(metrics2_no_gcps, 'displacement_magnitude'):.2f} pixels\\n\")\n",
    "    f.write(f\"- **With GCPs**: RMSE={get_metric(metrics2_with_gcps, 'rmse'):.3f}, \")\n",
    "    f.write(f\"Displacement={get_metric(metrics2_with_gcps, 'displacement_magnitude'):.2f} pixels\\n\")\n",
    "\n",
    "print(f\"‚úì Markdown report saved to: {report_md}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}