{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Westminster Ground Truth Analysis with MetaShape\n",
        "\n",
        "This notebook demonstrates the complete workflow for creating orthomosaics from DJI drone imagery using **Agisoft MetaShape** and evaluating their accuracy:\n",
        "\n",
        "1. **Data Loading**: Load images and GCPs from CSV\n",
        "2. **GCP Conversion**: Convert UTM coordinates to WGS84 lat/lon for MetaShape\n",
        "3. **Orthomosaic Creation**: Generate orthomosaics with and without GCPs using MetaShape\n",
        "4. **Basemap Comparison**: Download basemaps and quantify absolute accuracy\n",
        "5. **Quality Report**: Generate comprehensive comparison report\n",
        "\n",
        "## Datasets:\n",
        "- **Dataset 1**: DJI_202510060955_017_25-3288 (543 images)\n",
        "- **Dataset 2**: DJI_202510060955_019_25-3288 (528 images)\n",
        "\n",
        "For each dataset, we create:\n",
        "- Orthomosaic **without** GCPs\n",
        "- Orthomosaic **with** GCPs\n",
        "\n",
        "All orthomosaics are compared against reference basemaps to evaluate accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Install Dependencies\n",
        "\n",
        "First, install the required packages. Note: This notebook requires **Agisoft MetaShape Python API** to be installed separately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Try to install from requirements.txt first\n",
        "requirements_file = Path(\"requirements.txt\")\n",
        "if requirements_file.exists():\n",
        "    print(\"Installing packages from requirements.txt...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(requirements_file)])\n",
        "    print(\"✓ Packages installed from requirements.txt\")\n",
        "else:\n",
        "    # Fallback: install packages individually\n",
        "    print(\"requirements.txt not found. Installing packages individually...\")\n",
        "    packages = [\n",
        "        \"numpy>=1.24.0\",\n",
        "        \"rasterio>=1.3.0\",\n",
        "        \"pillow>=10.0.0\",\n",
        "        \"matplotlib>=3.7.0\",\n",
        "        \"pandas>=2.0.0\",\n",
        "        \"pyproj>=3.6.0\",\n",
        "        \"requests>=2.31.0\",\n",
        "        \"utm>=0.7.0\",\n",
        "    ]\n",
        "    for package in packages:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "    print(\"✓ All packages installed\")\n",
        "\n",
        "# Check for MetaShape\n",
        "try:\n",
        "    import Metashape\n",
        "    print(\"✓ MetaShape Python API is available\")\n",
        "    METASHAPE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"⚠️  MetaShape Python API not found. Please install Agisoft MetaShape and its Python API.\")\n",
        "    print(\"   The API is typically installed with MetaShape at:\")\n",
        "    print(\"   - Windows: C:\\\\Program Files\\\\Agisoft\\\\Metashape Pro\\\\python\")\n",
        "    print(\"   - macOS: /Applications/Metashape Pro/Metashape.app/Contents/Frameworks/Python.framework/Versions/3.9\")\n",
        "    print(\"   - Linux: /opt/metashape-pro/lib/python3.9\")\n",
        "    METASHAPE_AVAILABLE = False\n",
        "\n",
        "print(\"\\nSetup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import logging\n",
        "import json\n",
        "import csv\n",
        "import xml.etree.ElementTree as ET\n",
        "import utm\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "# Add package to path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "# Import Westminster-specific modules\n",
        "from westminster_ground_truth_analysis import (\n",
        "    GCPParser,\n",
        "    download_basemap,\n",
        "    compare_orthomosaic_to_basemap,\n",
        ")\n",
        "\n",
        "# Try to import MetaShape processor from qualicum_beach package\n",
        "# If not available, we'll define the functions locally\n",
        "try:\n",
        "    from qualicum_beach_gcp_analysis import (\n",
        "        process_orthomosaic,\n",
        "        PhotoMatchQuality,\n",
        "        DepthMapQuality,\n",
        "        export_to_metashape_csv,\n",
        "        export_to_metashape_xml,\n",
        "    )\n",
        "    print(\"✓ Using MetaShape processor from qualicum_beach_gcp_analysis\")\n",
        "    USE_QUALICUM_PACKAGE = True\n",
        "except ImportError:\n",
        "    print(\"⚠️  qualicum_beach_gcp_analysis not found. Will define MetaShape functions locally.\")\n",
        "    USE_QUALICUM_PACKAGE = False\n",
        "    # We'll define these functions in the next cell if needed\n",
        "\n",
        "# Set up paths\n",
        "data_dir = Path(\"/Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25\")\n",
        "output_dir = Path(\"outputs\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"✓ Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Ground Control Points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse GCP file (UTM coordinates)\n",
        "gcp_file = data_dir / \"25-3288-CONTROL-NAD83-UTM10N-EGM2008.csv\"\n",
        "gcp_parser = GCPParser(str(gcp_file))\n",
        "\n",
        "gcps_utm = gcp_parser.get_gcps()\n",
        "print(f\"Loaded {len(gcps_utm)} ground control points (UTM format)\")\n",
        "\n",
        "# Display first few GCPs\n",
        "print(\"\\nFirst few GCPs (UTM):\")\n",
        "for gcp in gcps_utm[:5]:\n",
        "    print(f\"  {gcp.name}: X={gcp.x:.2f}, Y={gcp.y:.2f}, Z={gcp.z:.2f}\")\n",
        "\n",
        "# Get bounds\n",
        "min_x, min_y, max_x, max_y = gcp_parser.get_bounds()\n",
        "print(f\"\\nGCP Bounds (UTM): X=[{min_x:.2f}, {max_x:.2f}], Y=[{min_y:.2f}, {max_y:.2f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Convert GCPs to WGS84 Lat/Lon for MetaShape\n",
        "\n",
        "MetaShape expects GCPs in WGS84 lat/lon format. We need to convert from UTM Zone 10N.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert UTM to WGS84 lat/lon\n",
        "# NOTE: In the CSV, X column is actually Northing (~5.45M) and Y column is Easting (~500k)\n",
        "# utm.to_latlon expects (easting, northing), so we need to swap them\n",
        "gcps_wgs84 = []\n",
        "\n",
        "for gcp in gcps_utm:\n",
        "    # Convert UTM to lat/lon (UTM Zone 10N)\n",
        "    # X is Northing, Y is Easting\n",
        "    lat, lon = utm.to_latlon(gcp.y, gcp.x, 10, 'N')\n",
        "    \n",
        "    gcp_dict = {\n",
        "        'id': gcp.name,\n",
        "        'label': gcp.name,\n",
        "        'lat': lat,\n",
        "        'lon': lon,\n",
        "        'z': gcp.z,\n",
        "        'accuracy': 0.1  # Default accuracy in meters\n",
        "    }\n",
        "    gcps_wgs84.append(gcp_dict)\n",
        "\n",
        "print(f\"Converted {len(gcps_wgs84)} GCPs to WGS84 lat/lon\")\n",
        "print(\"\\nFirst few GCPs (WGS84):\")\n",
        "for gcp in gcps_wgs84[:5]:\n",
        "    print(f\"  {gcp['id']}: ({gcp['lat']:.6f}, {gcp['lon']:.6f}, z={gcp['z']:.2f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Export GCPs for MetaShape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory for GCP files\n",
        "gcp_output_dir = output_dir / \"gcps\"\n",
        "gcp_output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Export to MetaShape XML format (preferred by MetaShape)\n",
        "if USE_QUALICUM_PACKAGE:\n",
        "    gcp_xml_path = gcp_output_dir / \"gcps_metashape.xml\"\n",
        "    export_to_metashape_xml(gcps_wgs84, str(gcp_xml_path))\n",
        "    print(f\"✓ GCPs exported to XML: {gcp_xml_path}\")\n",
        "    \n",
        "    # Also export CSV for reference\n",
        "    gcp_csv_path = gcp_output_dir / \"gcps_metashape.csv\"\n",
        "    export_to_metashape_csv(gcps_wgs84, str(gcp_csv_path))\n",
        "    print(f\"✓ GCPs also exported to CSV: {gcp_csv_path}\")\n",
        "else:\n",
        "    # Define export functions locally if qualicum package not available\n",
        "    def export_to_metashape_xml_local(gcps, output_path):\n",
        "        output_path = Path(output_path)\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        root = ET.Element('document')\n",
        "        chunks = ET.SubElement(root, 'chunks')\n",
        "        chunk = ET.SubElement(chunks, 'chunk')\n",
        "        markers = ET.SubElement(chunk, 'markers')\n",
        "        \n",
        "        for gcp in gcps:\n",
        "            marker = ET.SubElement(markers, 'marker')\n",
        "            marker.set('label', gcp.get('id', gcp.get('label', 'GCP')))\n",
        "            marker.set('reference', 'true')\n",
        "            \n",
        "            position = ET.SubElement(marker, 'position')\n",
        "            position.set('x', str(gcp.get('lon', 0.0)))\n",
        "            position.set('y', str(gcp.get('lat', 0.0)))\n",
        "            position.set('z', str(gcp.get('z', 0.0)))\n",
        "            \n",
        "            accuracy = gcp.get('accuracy', 1.0)\n",
        "            accuracy_elem = ET.SubElement(marker, 'accuracy')\n",
        "            accuracy_elem.set('x', str(accuracy))\n",
        "            accuracy_elem.set('y', str(accuracy))\n",
        "            accuracy_elem.set('z', str(accuracy))\n",
        "        \n",
        "        tree = ET.ElementTree(root)\n",
        "        ET.indent(tree, space='  ')\n",
        "        tree.write(output_path, encoding='utf-8', xml_declaration=True)\n",
        "        print(f\"Exported {len(gcps)} GCPs to MetaShape XML: {output_path}\")\n",
        "        return str(output_path)\n",
        "    \n",
        "    def export_to_metashape_csv_local(gcps, output_path):\n",
        "        output_path = Path(output_path)\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        with open(output_path, 'w', newline='') as f:\n",
        "            writer = csv.writer(f, delimiter='\\t')\n",
        "            writer.writerow(['Label', 'X', 'Y', 'Z', 'Accuracy', 'Enabled'])\n",
        "            \n",
        "            for gcp in gcps:\n",
        "                label = gcp.get('id', gcp.get('label', 'GCP'))\n",
        "                lon = gcp.get('lon', 0.0)\n",
        "                lat = gcp.get('lat', 0.0)\n",
        "                z = gcp.get('z', 0.0)\n",
        "                accuracy = gcp.get('accuracy', 1.0)\n",
        "                writer.writerow([label, lon, lat, z, accuracy, '1'])\n",
        "        \n",
        "        print(f\"Exported {len(gcps)} GCPs to MetaShape CSV: {output_path}\")\n",
        "        return str(output_path)\n",
        "    \n",
        "    gcp_xml_path = gcp_output_dir / \"gcps_metashape.xml\"\n",
        "    export_to_metashape_xml_local(gcps_wgs84, str(gcp_xml_path))\n",
        "    print(f\"✓ GCPs exported to XML: {gcp_xml_path}\")\n",
        "    \n",
        "    gcp_csv_path = gcp_output_dir / \"gcps_metashape.csv\"\n",
        "    export_to_metashape_csv_local(gcps_wgs84, str(gcp_csv_path))\n",
        "    print(f\"✓ GCPs also exported to CSV: {gcp_csv_path}\")\n",
        "\n",
        "# Use XML file for processing (MetaShape's native format)\n",
        "gcp_file_for_processing = gcp_xml_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define MetaShape processing functions if qualicum package not available\n",
        "if not USE_QUALICUM_PACKAGE and METASHAPE_AVAILABLE:\n",
        "    from enum import IntEnum\n",
        "    from typing import Optional\n",
        "    \n",
        "    class PhotoMatchQuality(IntEnum):\n",
        "        LowestQuality = 0\n",
        "        LowQuality = 1\n",
        "        MediumQuality = 2\n",
        "        HighQuality = 4\n",
        "        HighestQuality = 8\n",
        "    \n",
        "    class DepthMapQuality(IntEnum):\n",
        "        LowestQuality = 1\n",
        "        LowQuality = 2\n",
        "        MediumQuality = 4\n",
        "        HighQuality = 8\n",
        "        UltraQuality = 16\n",
        "    \n",
        "    def process_orthomosaic(\n",
        "        photos_dir: Path,\n",
        "        output_path: Path,\n",
        "        project_path: Path,\n",
        "        gcp_file: Optional[Path] = None,\n",
        "        product_id: str = \"orthomosaic\",\n",
        "        clean_intermediate_files: bool = False,\n",
        "        photo_match_quality: int = PhotoMatchQuality.MediumQuality,\n",
        "        depth_map_quality: int = DepthMapQuality.MediumQuality,\n",
        "        tiepoint_limit: int = 10000,\n",
        "        use_gcps: bool = False\n",
        "    ) -> dict:\n",
        "        \"\"\"Process orthomosaic using MetaShape.\"\"\"\n",
        "        import Metashape\n",
        "        \n",
        "        # Configure GPU\n",
        "        Metashape.app.gpu_mask = ~0\n",
        "        \n",
        "        # Setup paths\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "        project_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Check if project exists\n",
        "        project_exists = project_path.exists()\n",
        "        \n",
        "        if project_exists and not clean_intermediate_files:\n",
        "            logging.info(f\"Loading existing project: {project_path}\")\n",
        "            doc = Metashape.Document()\n",
        "            doc.open(str(project_path))\n",
        "            if len(doc.chunks) > 0:\n",
        "                chunk = doc.chunks[0]\n",
        "            else:\n",
        "                chunk = doc.addChunk()\n",
        "        else:\n",
        "            if clean_intermediate_files and project_exists:\n",
        "                project_path.unlink()\n",
        "            doc = Metashape.Document()\n",
        "            doc.save(str(project_path))\n",
        "            chunk = doc.addChunk()\n",
        "        \n",
        "        # Add photos\n",
        "        if len(chunk.cameras) == 0:\n",
        "            logging.info(f\"Adding photos from: {photos_dir}\")\n",
        "            photos = list(photos_dir.glob(\"*.jpg\")) + list(photos_dir.glob(\"*.JPG\"))\n",
        "            if not photos:\n",
        "                raise ValueError(f\"No images found in {photos_dir}\")\n",
        "            chunk.addPhotos([str(p) for p in photos])\n",
        "            doc.save()\n",
        "        \n",
        "        # Add GCPs if requested\n",
        "        if use_gcps and gcp_file and gcp_file.exists():\n",
        "            if len(chunk.markers) == 0:\n",
        "                logging.info(f\"Loading GCPs from: {gcp_file}\")\n",
        "                chunk.importMarkers(str(gcp_file))\n",
        "                doc.save()\n",
        "        \n",
        "        # Match photos\n",
        "        if chunk.tie_points is None or len(chunk.tie_points) == 0:\n",
        "            logging.info(\"Matching photos...\")\n",
        "            chunk.matchPhotos(\n",
        "                downscale=photo_match_quality,\n",
        "                tiepoint_limit=tiepoint_limit,\n",
        "            )\n",
        "            doc.save()\n",
        "        \n",
        "        # Align cameras\n",
        "        aligned = sum(1 for cam in chunk.cameras if cam.transform)\n",
        "        if aligned == 0:\n",
        "            logging.info(\"Aligning cameras...\")\n",
        "            chunk.alignCameras()\n",
        "            doc.save()\n",
        "        \n",
        "        # Build depth maps\n",
        "        if len(chunk.depth_maps) == 0:\n",
        "            logging.info(\"Building depth maps...\")\n",
        "            chunk.buildDepthMaps(\n",
        "                downscale=depth_map_quality,\n",
        "                filter_mode=Metashape.MildFiltering\n",
        "            )\n",
        "            doc.save()\n",
        "        \n",
        "        # Build model\n",
        "        if chunk.model is None:\n",
        "            logging.info(\"Building 3D model...\")\n",
        "            chunk.buildModel()\n",
        "            doc.save()\n",
        "        \n",
        "        # Build orthomosaic\n",
        "        if chunk.orthomosaic is None:\n",
        "            logging.info(\"Building orthomosaic...\")\n",
        "            chunk.buildOrthomosaic()\n",
        "            doc.save()\n",
        "        \n",
        "        # Export GeoTIFF\n",
        "        ortho_path = output_path / f\"{product_id}.tif\"\n",
        "        if not ortho_path.exists() or clean_intermediate_files:\n",
        "            logging.info(f\"Exporting GeoTIFF to: {ortho_path}\")\n",
        "            compression = Metashape.ImageCompression()\n",
        "            compression.tiff_compression = Metashape.ImageCompression.TiffCompressionNone\n",
        "            compression.tiff_big = True\n",
        "            chunk.exportRaster(str(ortho_path), image_compression=compression)\n",
        "            doc.save()\n",
        "        \n",
        "        stats = {\n",
        "            'product_id': product_id,\n",
        "            'use_gcps': use_gcps,\n",
        "            'num_photos': len(chunk.cameras),\n",
        "            'num_markers': len(chunk.markers),\n",
        "            'ortho_path': str(ortho_path),\n",
        "            'project_path': str(project_path),\n",
        "        }\n",
        "        \n",
        "        return stats\n",
        "    \n",
        "    print(\"✓ MetaShape processing functions defined locally\")\n",
        "elif not METASHAPE_AVAILABLE:\n",
        "    print(\"⚠️  MetaShape not available. Cannot process orthomosaics.\")\n",
        "else:\n",
        "    print(\"✓ Using MetaShape processor from qualicum_beach_gcp_analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not METASHAPE_AVAILABLE:\n",
        "    print(\"⚠️  MetaShape not available. Skipping processing.\")\n",
        "else:\n",
        "    # Setup paths for Dataset 1\n",
        "    dataset1_dir = data_dir / \"DJI_202510060955_017_25-3288\"\n",
        "    intermediate_dir = output_dir / \"intermediate\"\n",
        "    ortho_output_dir = output_dir / \"orthomosaics\"\n",
        "    \n",
        "    # Process orthomosaic WITHOUT GCPs\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Processing Dataset 1 - WITHOUT GCPs...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    project_path1_no_gcps = intermediate_dir / \"dataset1_no_gcps.psx\"\n",
        "    \n",
        "    if USE_QUALICUM_PACKAGE:\n",
        "        stats1_no_gcps = process_orthomosaic(\n",
        "            photos_dir=dataset1_dir,\n",
        "            output_path=ortho_output_dir,\n",
        "            project_path=project_path1_no_gcps,\n",
        "            product_id=\"dataset1_no_gcps\",\n",
        "            clean_intermediate_files=False,\n",
        "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
        "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
        "            tiepoint_limit=10000,\n",
        "            use_gcps=False\n",
        "        )\n",
        "    else:\n",
        "        stats1_no_gcps = process_orthomosaic(\n",
        "            photos_dir=dataset1_dir,\n",
        "            output_path=ortho_output_dir,\n",
        "            project_path=project_path1_no_gcps,\n",
        "            product_id=\"dataset1_no_gcps\",\n",
        "            clean_intermediate_files=False,\n",
        "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
        "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
        "            tiepoint_limit=10000,\n",
        "            use_gcps=False\n",
        "        )\n",
        "    \n",
        "    print(\"\\n✓ Dataset 1 processing (without GCPs) complete!\")\n",
        "    print(f\"  Number of photos: {stats1_no_gcps['num_photos']}\")\n",
        "    print(f\"  Orthomosaic: {stats1_no_gcps['ortho_path']}\")\n",
        "    \n",
        "    ortho1_no_gcps_path = Path(stats1_no_gcps['ortho_path'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Process Dataset 1 - WITH GCPs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not METASHAPE_AVAILABLE:\n",
        "    print(\"⚠️  MetaShape not available. Skipping processing.\")\n",
        "else:\n",
        "    # Process orthomosaic WITH GCPs\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Processing Dataset 1 - WITH GCPs...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    project_path1_with_gcps = intermediate_dir / \"dataset1_with_gcps.psx\"\n",
        "    \n",
        "    if USE_QUALICUM_PACKAGE:\n",
        "        stats1_with_gcps = process_orthomosaic(\n",
        "            photos_dir=dataset1_dir,\n",
        "            output_path=ortho_output_dir,\n",
        "            project_path=project_path1_with_gcps,\n",
        "            gcp_file=gcp_file_for_processing,\n",
        "            product_id=\"dataset1_with_gcps\",\n",
        "            clean_intermediate_files=False,\n",
        "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
        "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
        "            tiepoint_limit=10000,\n",
        "            use_gcps=True\n",
        "        )\n",
        "    else:\n",
        "        stats1_with_gcps = process_orthomosaic(\n",
        "            photos_dir=dataset1_dir,\n",
        "            output_path=ortho_output_dir,\n",
        "            project_path=project_path1_with_gcps,\n",
        "            gcp_file=gcp_file_for_processing,\n",
        "            product_id=\"dataset1_with_gcps\",\n",
        "            clean_intermediate_files=False,\n",
        "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
        "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
        "            tiepoint_limit=10000,\n",
        "            use_gcps=True\n",
        "        )\n",
        "    \n",
        "    print(\"\\n✓ Dataset 1 processing (with GCPs) complete!\")\n",
        "    print(f\"  Number of photos: {stats1_with_gcps['num_photos']}\")\n",
        "    print(f\"  Number of markers: {stats1_with_gcps.get('num_markers', 0)}\")\n",
        "    print(f\"  Orthomosaic: {stats1_with_gcps['ortho_path']}\")\n",
        "    \n",
        "    ortho1_with_gcps_path = Path(stats1_with_gcps['ortho_path'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Process Dataset 2 - WITHOUT GCPs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not METASHAPE_AVAILABLE:\n",
        "    print(\"⚠️  MetaShape not available. Skipping processing.\")\n",
        "else:\n",
        "    # Setup paths for Dataset 2\n",
        "    dataset2_dir = data_dir / \"DJI_202510060955_019_25-3288\"\n",
        "    \n",
        "    # Process orthomosaic WITHOUT GCPs\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Processing Dataset 2 - WITHOUT GCPs...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    project_path2_no_gcps = intermediate_dir / \"dataset2_no_gcps.psx\"\n",
        "    \n",
        "    if USE_QUALICUM_PACKAGE:\n",
        "        stats2_no_gcps = process_orthomosaic(\n",
        "            photos_dir=dataset2_dir,\n",
        "            output_path=ortho_output_dir,\n",
        "            project_path=project_path2_no_gcps,\n",
        "            product_id=\"dataset2_no_gcps\",\n",
        "            clean_intermediate_files=False,\n",
        "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
        "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
        "            tiepoint_limit=10000,\n",
        "            use_gcps=False\n",
        "        )\n",
        "    else:\n",
        "        stats2_no_gcps = process_orthomosaic(\n",
        "            photos_dir=dataset2_dir,\n",
        "            output_path=ortho_output_dir,\n",
        "            project_path=project_path2_no_gcps,\n",
        "            product_id=\"dataset2_no_gcps\",\n",
        "            clean_intermediate_files=False,\n",
        "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
        "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
        "            tiepoint_limit=10000,\n",
        "            use_gcps=False\n",
        "        )\n",
        "    \n",
        "    print(\"\\n✓ Dataset 2 processing (without GCPs) complete!\")\n",
        "    print(f\"  Number of photos: {stats2_no_gcps['num_photos']}\")\n",
        "    print(f\"  Orthomosaic: {stats2_no_gcps['ortho_path']}\")\n",
        "    \n",
        "    ortho2_no_gcps_path = Path(stats2_no_gcps['ortho_path'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Process Dataset 2 - WITH GCPs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not METASHAPE_AVAILABLE:\n",
        "    print(\"⚠️  MetaShape not available. Skipping processing.\")\n",
        "else:\n",
        "    # Process orthomosaic WITH GCPs\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Processing Dataset 2 - WITH GCPs...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    project_path2_with_gcps = intermediate_dir / \"dataset2_with_gcps.psx\"\n",
        "    \n",
        "    if USE_QUALICUM_PACKAGE:\n",
        "        stats2_with_gcps = process_orthomosaic(\n",
        "            photos_dir=dataset2_dir,\n",
        "            output_path=ortho_output_dir,\n",
        "            project_path=project_path2_with_gcps,\n",
        "            gcp_file=gcp_file_for_processing,\n",
        "            product_id=\"dataset2_with_gcps\",\n",
        "            clean_intermediate_files=False,\n",
        "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
        "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
        "            tiepoint_limit=10000,\n",
        "            use_gcps=True\n",
        "        )\n",
        "    else:\n",
        "        stats2_with_gcps = process_orthomosaic(\n",
        "            photos_dir=dataset2_dir,\n",
        "            output_path=ortho_output_dir,\n",
        "            project_path=project_path2_with_gcps,\n",
        "            gcp_file=gcp_file_for_processing,\n",
        "            product_id=\"dataset2_with_gcps\",\n",
        "            clean_intermediate_files=False,\n",
        "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
        "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
        "            tiepoint_limit=10000,\n",
        "            use_gcps=True\n",
        "        )\n",
        "    \n",
        "    print(\"\\n✓ Dataset 2 processing (with GCPs) complete!\")\n",
        "    print(f\"  Number of photos: {stats2_with_gcps['num_photos']}\")\n",
        "    print(f\"  Number of markers: {stats2_with_gcps.get('num_markers', 0)}\")\n",
        "    print(f\"  Orthomosaic: {stats2_with_gcps['ortho_path']}\")\n",
        "    \n",
        "    ortho2_with_gcps_path = Path(stats2_with_gcps['ortho_path'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Download Reference Basemap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate bounding box from GCPs for basemap download\n",
        "# Convert UTM bounds to lat/lon\n",
        "center_easting = (min_y + max_y) / 2  # Y column is easting\n",
        "center_northing = (min_x + max_x) / 2  # X column is northing\n",
        "lat_center, lon_center = utm.to_latlon(center_easting, center_northing, 10, 'N')\n",
        "\n",
        "# Convert bounds\n",
        "lat_min, lon_min = utm.to_latlon(min_y, min_x, 10, 'N')\n",
        "lat_max, lon_max = utm.to_latlon(max_y, max_x, 10, 'N')\n",
        "\n",
        "# Add padding\n",
        "padding = 0.001\n",
        "bbox = (lat_min - padding, lon_min - padding, lat_max + padding, lon_max + padding)\n",
        "\n",
        "print(f\"Basemap bounding box: {bbox}\")\n",
        "\n",
        "# Download basemap\n",
        "basemap_path = download_basemap(\n",
        "    bbox=bbox,\n",
        "    output_path=str(output_dir / \"basemap.tif\"),\n",
        "    source=\"esri_world_imagery\",\n",
        "    target_resolution=0.1  # 0.1m per pixel\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Basemap saved to: {basemap_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Compare Orthomosaics to Basemap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all orthomosaics to basemap\n",
        "comparison_dir = output_dir / \"comparisons\"\n",
        "comparison_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Determine orthomosaic paths\n",
        "if 'ortho1_no_gcps_path' not in locals():\n",
        "    ortho_output_dir = output_dir / \"orthomosaics\"\n",
        "    ortho1_no_gcps_path = ortho_output_dir / \"dataset1_no_gcps.tif\"\n",
        "    ortho1_with_gcps_path = ortho_output_dir / \"dataset1_with_gcps.tif\"\n",
        "    ortho2_no_gcps_path = ortho_output_dir / \"dataset2_no_gcps.tif\"\n",
        "    ortho2_with_gcps_path = ortho_output_dir / \"dataset2_with_gcps.tif\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Comparing orthomosaics to basemap...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Compare Dataset 1 - Without GCPs\n",
        "if ortho1_no_gcps_path.exists():\n",
        "    print(\"\\nDataset 1 - Without GCPs\")\n",
        "    print(\"-\" * 60)\n",
        "    metrics1_no_gcps = compare_orthomosaic_to_basemap(\n",
        "        str(ortho1_no_gcps_path),\n",
        "        str(basemap_path),\n",
        "        output_dir=str(comparison_dir / \"dataset1_no_gcps\")\n",
        "    )\n",
        "else:\n",
        "    print(f\"\\n⚠️  Orthomosaic not found: {ortho1_no_gcps_path}\")\n",
        "    metrics1_no_gcps = {}\n",
        "\n",
        "# Compare Dataset 1 - With GCPs\n",
        "if ortho1_with_gcps_path.exists():\n",
        "    print(\"\\nDataset 1 - With GCPs\")\n",
        "    print(\"-\" * 60)\n",
        "    metrics1_with_gcps = compare_orthomosaic_to_basemap(\n",
        "        str(ortho1_with_gcps_path),\n",
        "        str(basemap_path),\n",
        "        output_dir=str(comparison_dir / \"dataset1_with_gcps\")\n",
        "    )\n",
        "else:\n",
        "    print(f\"\\n⚠️  Orthomosaic not found: {ortho1_with_gcps_path}\")\n",
        "    metrics1_with_gcps = {}\n",
        "\n",
        "# Compare Dataset 2 - Without GCPs\n",
        "if ortho2_no_gcps_path.exists():\n",
        "    print(\"\\nDataset 2 - Without GCPs\")\n",
        "    print(\"-\" * 60)\n",
        "    metrics2_no_gcps = compare_orthomosaic_to_basemap(\n",
        "        str(ortho2_no_gcps_path),\n",
        "        str(basemap_path),\n",
        "        output_dir=str(comparison_dir / \"dataset2_no_gcps\")\n",
        "    )\n",
        "else:\n",
        "    print(f\"\\n⚠️  Orthomosaic not found: {ortho2_no_gcps_path}\")\n",
        "    metrics2_no_gcps = {}\n",
        "\n",
        "# Compare Dataset 2 - With GCPs\n",
        "if ortho2_with_gcps_path.exists():\n",
        "    print(\"\\nDataset 2 - With GCPs\")\n",
        "    print(\"-\" * 60)\n",
        "    metrics2_with_gcps = compare_orthomosaic_to_basemap(\n",
        "        str(ortho2_with_gcps_path),\n",
        "        str(basemap_path),\n",
        "        output_dir=str(comparison_dir / \"dataset2_with_gcps\")\n",
        "    )\n",
        "else:\n",
        "    print(f\"\\n⚠️  Orthomosaic not found: {ortho2_with_gcps_path}\")\n",
        "    metrics2_with_gcps = {}\n",
        "\n",
        "print(\"\\n✓ All comparisons complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Generate Quality Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison summary\n",
        "import pandas as pd\n",
        "\n",
        "# Helper function to safely get metrics\n",
        "def get_metric(metrics_dict, key, default=0.0):\n",
        "    \"\"\"Safely get a metric value, handling different key formats.\"\"\"\n",
        "    if not metrics_dict:\n",
        "        return default\n",
        "    # Try direct key\n",
        "    if key in metrics_dict:\n",
        "        return metrics_dict[key]\n",
        "    # Try with _pixels suffix\n",
        "    if f\"{key}_pixels\" in metrics_dict:\n",
        "        return metrics_dict[f\"{key}_pixels\"]\n",
        "    return default\n",
        "\n",
        "# Collect metrics\n",
        "summary_data = {\n",
        "    'Dataset': ['Dataset 1', 'Dataset 1', 'Dataset 2', 'Dataset 2'],\n",
        "    'GCPs Used': ['No', 'Yes', 'No', 'Yes'],\n",
        "    'RMSE': [\n",
        "        get_metric(metrics1_no_gcps, 'rmse'),\n",
        "        get_metric(metrics1_with_gcps, 'rmse'),\n",
        "        get_metric(metrics2_no_gcps, 'rmse'),\n",
        "        get_metric(metrics2_with_gcps, 'rmse'),\n",
        "    ],\n",
        "    'MAE': [\n",
        "        get_metric(metrics1_no_gcps, 'mae'),\n",
        "        get_metric(metrics1_with_gcps, 'mae'),\n",
        "        get_metric(metrics2_no_gcps, 'mae'),\n",
        "        get_metric(metrics2_with_gcps, 'mae'),\n",
        "    ],\n",
        "    'Correlation': [\n",
        "        get_metric(metrics1_no_gcps, 'correlation'),\n",
        "        get_metric(metrics1_with_gcps, 'correlation'),\n",
        "        get_metric(metrics2_no_gcps, 'correlation'),\n",
        "        get_metric(metrics2_with_gcps, 'correlation'),\n",
        "    ],\n",
        "    'SSIM': [\n",
        "        get_metric(metrics1_no_gcps, 'ssim'),\n",
        "        get_metric(metrics1_with_gcps, 'ssim'),\n",
        "        get_metric(metrics2_no_gcps, 'ssim'),\n",
        "        get_metric(metrics2_with_gcps, 'ssim'),\n",
        "    ],\n",
        "    'Displacement (pixels)': [\n",
        "        get_metric(metrics1_no_gcps, 'displacement_magnitude'),\n",
        "        get_metric(metrics1_with_gcps, 'displacement_magnitude'),\n",
        "        get_metric(metrics2_no_gcps, 'displacement_magnitude'),\n",
        "        get_metric(metrics2_with_gcps, 'displacement_magnitude'),\n",
        "    ],\n",
        "    'Num Matches': [\n",
        "        get_metric(metrics1_no_gcps, 'num_matches', default=0),\n",
        "        get_metric(metrics1_with_gcps, 'num_matches', default=0),\n",
        "        get_metric(metrics2_no_gcps, 'num_matches', default=0),\n",
        "        get_metric(metrics2_with_gcps, 'num_matches', default=0),\n",
        "    ],\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(summary_data)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SUMMARY COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Save summary to CSV\n",
        "summary_csv = output_dir / \"quality_summary.csv\"\n",
        "df.to_csv(summary_csv, index=False)\n",
        "print(f\"\\n✓ Summary saved to: {summary_csv}\")\n",
        "\n",
        "# Create visualizations\n",
        "available_metrics = []\n",
        "for col in ['RMSE', 'MAE', 'Correlation', 'SSIM', 'Displacement (pixels)', 'Num Matches']:\n",
        "    if col in df.columns and df[col].sum() > 0:\n",
        "        available_metrics.append(col)\n",
        "\n",
        "if available_metrics:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, metric in enumerate(available_metrics[:6]):\n",
        "        ax = axes[i]\n",
        "        df_pivot = df.pivot(index='Dataset', columns='GCPs Used', values=metric)\n",
        "        df_pivot.plot(kind='bar', ax=ax, rot=0)\n",
        "        ax.set_title(metric)\n",
        "        ax.set_ylabel('Value')\n",
        "        ax.legend(title='GCPs Used')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for i in range(len(available_metrics), 6):\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    report_plot_path = output_dir / \"quality_report.png\"\n",
        "    plt.savefig(report_plot_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"✓ Quality report plot saved to: {report_plot_path}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"⚠️  No metrics available for visualization\")\n",
        "\n",
        "# Generate markdown report\n",
        "report_md = output_dir / \"quality_report.md\"\n",
        "with open(report_md, 'w') as f:\n",
        "    f.write(\"# Westminster Ground Truth Analysis - Quality Report\\n\\n\")\n",
        "    f.write(\"## Summary\\n\\n\")\n",
        "    f.write(df.to_markdown(index=False))\n",
        "    f.write(\"\\n\\n## Findings\\n\\n\")\n",
        "    f.write(\"### Dataset 1\\n\")\n",
        "    f.write(f\"- **Without GCPs**: RMSE={get_metric(metrics1_no_gcps, 'rmse'):.3f}, \")\n",
        "    f.write(f\"Displacement={get_metric(metrics1_no_gcps, 'displacement_magnitude'):.2f} pixels\\n\")\n",
        "    f.write(f\"- **With GCPs**: RMSE={get_metric(metrics1_with_gcps, 'rmse'):.3f}, \")\n",
        "    f.write(f\"Displacement={get_metric(metrics1_with_gcps, 'displacement_magnitude'):.2f} pixels\\n\\n\")\n",
        "    f.write(\"### Dataset 2\\n\")\n",
        "    f.write(f\"- **Without GCPs**: RMSE={get_metric(metrics2_no_gcps, 'rmse'):.3f}, \")\n",
        "    f.write(f\"Displacement={get_metric(metrics2_no_gcps, 'displacement_magnitude'):.2f} pixels\\n\")\n",
        "    f.write(f\"- **With GCPs**: RMSE={get_metric(metrics2_with_gcps, 'rmse'):.3f}, \")\n",
        "    f.write(f\"Displacement={get_metric(metrics2_with_gcps, 'displacement_magnitude'):.2f} pixels\\n\")\n",
        "\n",
        "print(f\"✓ Markdown report saved to: {report_md}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
