{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Westminster Ground Truth Analysis with MetaShape (Spexi Data)\n",
    "\n",
    "This notebook processes drone imagery from Spexi data to create orthomosaics using **Agisoft MetaShape**:\n",
    "\n",
    "1. **KMZ Processing**: Load KMZ file and extract H3 cells\n",
    "2. **Image Download**: Download images from 6 manifest files (with caching to avoid re-downloading)\n",
    "3. **GCP Loading**: Load GCPs from CSV (UTM) and convert to WGS84 for MetaShape\n",
    "4. **Orthomosaic Creation**: Generate orthomosaics with and without GCPs using MetaShape\n",
    "5. **Intermediate File Saving**: All MetaShape intermediate files are saved to avoid recomputation\n",
    "\n",
    "## Data Sources:\n",
    "- **KMZ File**: NewWest_AOI.kmz (contains H3 cell information)\n",
    "- **Manifest Files**: 6 manifest files in Cells/ortho/ directory\n",
    "- **GCPs**: 25-3288-CONTROL-NAD83-UTM10N-EGM2008.csv (UTM Zone 10N)\n",
    "\n",
    "We create two orthomosaics:\n",
    "- Orthomosaic **without** GCPs (using only image matching)\n",
    "- Orthomosaic **with** GCPs (using image matching + ground control points)\n",
    "\n",
    "**Note**: GeoTIFF orthomosaics are exported with LZW lossless compression to reduce file size without losing visual information.\n",
    "\n",
    "\n",
    "**Note**: This notebook is designed for Google Colab. Upload your data to Google Drive and mount it to use this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies and Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install numpy>=1.24.0 rasterio>=1.3.0 pillow>=10.0.0 matplotlib>=3.7.0\n",
    "!pip install pandas>=2.0.0 pyproj>=3.6.0 requests>=2.31.0 utm>=0.7.0\n",
    "!pip install h3>=3.7.0 boto3>=1.28.0\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check for MetaShape (unlikely to be available in Colab)\n",
    "try:\n",
    "    import Metashape\n",
    "    print(\"‚úì MetaShape Python API is available\")\n",
    "    METASHAPE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  MetaShape Python API not found.\")\n",
    "    print(\"   MetaShape typically requires local installation and cannot run in Colab.\")\n",
    "    print(\"   Please use the local Jupyter notebook version instead.\")\n",
    "    METASHAPE_AVAILABLE = False\n",
    "\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Package Files\n",
    "\n",
    "Upload the `research-qualicum_beach_gcp_analysis` package directory to `/content/` or clone from repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have the package in Google Drive, add it to path\n",
    "# Or clone from repository if available\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Update this path to point to your package location\n",
    "# Example: package_dir = Path(\"/content/drive/MyDrive/research-qualicum_beach_gcp_analysis\")\n",
    "package_dir = Path(\"/content\")\n",
    "sys.path.insert(0, str(package_dir))\n",
    "\n",
    "print(f\"Package directory: {package_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import logging\n",
    "import csv\n",
    "import utm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Add qualicum package to path (if available)\n",
    "package_dir = Path.cwd()\n",
    "sys.path.insert(0, str(package_dir))\n",
    "\n",
    "# Try to import from qualicum_beach_gcp_analysis package\n",
    "try:\n",
    "    from qualicum_beach_gcp_analysis import (\n",
    "        load_gcps_from_kmz,\n",
    "        calculate_gcp_bbox,\n",
    "        bbox_to_h3_cells,\n",
    "        download_all_images_from_input_dir,\n",
    "        export_to_metashape_csv,\n",
    "        export_to_metashape_xml,\n",
    "        process_orthomosaic,\n",
    "        PhotoMatchQuality,\n",
    "        DepthMapQuality,\n",
    "    )\n",
    "    USE_QUALICUM_PACKAGE = True\n",
    "    print(\"‚úì Using qualicum_beach_gcp_analysis package\")\n",
    "except ImportError:\n",
    "    USE_QUALICUM_PACKAGE = False\n",
    "    print(\"‚ö†Ô∏è  qualicum_beach_gcp_analysis package not available\")\n",
    "    print(\"   Some functionality may be limited\")\n",
    "\n",
    "print(\"‚úì Imports successful!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load KMZ File and Extract H3 Cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the KMZ file\n",
    "kmz_path = Path(\"/content/drive/MyDrive/New Westminster Oct _25/NewWest_AOI.kmz\")\n",
    "\n",
    "if not kmz_path.exists():\n",
    "    raise FileNotFoundError(f\"KMZ file not found: {kmz_path}\")\n",
    "\n",
    "print(f\"Loading KMZ file: {kmz_path}\")\n",
    "\n",
    "if USE_QUALICUM_PACKAGE:\n",
    "    # Load GCPs from KMZ (these contain H3 cell IDs)\n",
    "    gcps_from_kmz = load_gcps_from_kmz(str(kmz_path))\n",
    "    print(f\"\\n‚úì Loaded {len(gcps_from_kmz)} placemarks from KMZ\")\n",
    "    \n",
    "    # Extract H3 cell IDs from GCP IDs (they are the H3 cell identifiers)\n",
    "    h3_cells = [gcp.get('id', '') for gcp in gcps_from_kmz if gcp.get('id')]\n",
    "    print(f\"\\n‚úì Extracted {len(h3_cells)} H3 cells from KMZ\")\n",
    "    \n",
    "    # Also calculate H3 cells from bounding box for verification\n",
    "    bbox = calculate_gcp_bbox(gcps_from_kmz, padding=0.01)\n",
    "    h3_cells_from_bbox = bbox_to_h3_cells(bbox, resolution=12)\n",
    "    print(f\"\\n‚úì Calculated {len(h3_cells_from_bbox)} H3 cells from bounding box (resolution 12)\")\n",
    "    \n",
    "    # Display first few H3 cells\n",
    "    if h3_cells:\n",
    "        print(\"\\nFirst few H3 cells from KMZ:\")\n",
    "        for i, cell_id in enumerate(h3_cells[:10]):\n",
    "            print(f\"  {i+1}. {cell_id}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cannot load KMZ without qualicum_beach_gcp_analysis package\")\n",
    "    h3_cells = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download Images from Manifest Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "data_dir = Path(\"/content/drive/MyDrive/New Westminster Oct _25\")\n",
    "manifest_dir = data_dir / \"Cells\" / \"ortho\"\n",
    "photos_dir = Path(\"input/images\")\n",
    "\n",
    "if not manifest_dir.exists():\n",
    "    raise FileNotFoundError(f\"Manifest directory not found: {manifest_dir}\")\n",
    "\n",
    "print(f\"Manifest directory: {manifest_dir}\")\n",
    "print(f\"Photos will be downloaded to: {photos_dir.absolute()}\")\n",
    "\n",
    "if USE_QUALICUM_PACKAGE:\n",
    "    # Download all images from manifest files\n",
    "    # skip_existing=True ensures files are not re-downloaded if they already exist\n",
    "    print(\"\\nDownloading images from S3...\")\n",
    "    print(\"=\" * 60)\n",
    "    download_stats = download_all_images_from_input_dir(\n",
    "        input_dir=manifest_dir,\n",
    "        photos_dir=photos_dir,\n",
    "        skip_existing=True  # Don't re-download if images already exist\n",
    "    )\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úì Image download complete\")\n",
    "    \n",
    "    # Count total images downloaded\n",
    "    total_images = sum(s.get('total', 0) for s in download_stats.values())\n",
    "    total_downloaded = sum(s.get('downloaded', 0) for s in download_stats.values())\n",
    "    total_skipped = sum(s.get('skipped', 0) for s in download_stats.values())\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Total images: {total_images}\")\n",
    "    print(f\"  Downloaded: {total_downloaded}\")\n",
    "    print(f\"  Skipped (already exist): {total_skipped}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cannot download images without qualicum_beach_gcp_analysis package\")\n",
    "    print(\"   Please install the package or manually download images to:\")\n",
    "    print(f\"   {photos_dir.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Convert GCPs to WGS84\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to GCP CSV file (UTM coordinates)\n",
    "gcp_file = data_dir / \"25-3288-CONTROL-NAD83-UTM10N-EGM2008.csv\"\n",
    "\n",
    "if not gcp_file.exists():\n",
    "    raise FileNotFoundError(f\"GCP file not found: {gcp_file}\")\n",
    "\n",
    "print(f\"Loading GCPs from: {gcp_file}\")\n",
    "\n",
    "# Parse GCP file (UTM coordinates)\n",
    "gcps_utm = []\n",
    "with open(gcp_file, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        try:\n",
    "            name = row.get('Name', row.get('name', ''))\n",
    "            x = float(row.get('X', row.get('x', 0)))\n",
    "            y = float(row.get('Y', row.get('y', 0)))\n",
    "            z = float(row.get('Z', row.get('z', row.get('Elevation', row.get('elevation', 0)))))\n",
    "            \n",
    "            gcps_utm.append({\n",
    "                'name': name,\n",
    "                'x': x,  # Northing\n",
    "                'y': y,  # Easting\n",
    "                'z': z\n",
    "            })\n",
    "        except (ValueError, KeyError) as e:\n",
    "            print(f\"‚ö†Ô∏è  Skipping invalid row: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\n‚úì Loaded {len(gcps_utm)} GCPs from CSV (UTM Zone 10N)\")\n",
    "\n",
    "# Convert UTM to WGS84 lat/lon for MetaShape\n",
    "# NOTE: In the CSV, X column is actually Northing (~5.45M) and Y column is Easting (~500k)\n",
    "# utm.to_latlon expects (easting, northing), so we need to swap them\n",
    "gcps_wgs84 = []\n",
    "\n",
    "for gcp in gcps_utm:\n",
    "    # Convert UTM to lat/lon (UTM Zone 10N)\n",
    "    # X is Northing, Y is Easting\n",
    "    lat, lon = utm.to_latlon(gcp['y'], gcp['x'], 10, 'N')\n",
    "    \n",
    "    gcp_dict = {\n",
    "        'id': gcp['name'],\n",
    "        'label': gcp['name'],\n",
    "        'lat': lat,\n",
    "        'lon': lon,\n",
    "        'z': gcp['z'],\n",
    "        'accuracy': 0.01  # Very high accuracy (1cm) for high weight in bundle adjustment\n",
    "    }\n",
    "    gcps_wgs84.append(gcp_dict)\n",
    "\n",
    "print(f\"\\n‚úì Converted {len(gcps_wgs84)} GCPs to WGS84 lat/lon\")\n",
    "print(\"\\nFirst few GCPs (WGS84):\")\n",
    "for gcp in gcps_wgs84[:5]:\n",
    "    print(f\"  {gcp['id']}: ({gcp['lat']:.6f}, {gcp['lon']:.6f}, z={gcp['z']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Export GCPs for MetaShape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "gcp_output_dir = output_dir / \"gcps\"\n",
    "gcp_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if USE_QUALICUM_PACKAGE:\n",
    "    # Export GCPs to MetaShape XML format (preferred by MetaShape)\n",
    "    gcp_xml_path = gcp_output_dir / \"gcps_metashape.xml\"\n",
    "    export_to_metashape_xml(gcps_wgs84, str(gcp_xml_path))\n",
    "    print(f\"‚úì GCPs exported to XML: {gcp_xml_path}\")\n",
    "    \n",
    "    # Also export CSV for reference\n",
    "    gcp_csv_path = gcp_output_dir / \"gcps_metashape.csv\"\n",
    "    export_to_metashape_csv(gcps_wgs84, str(gcp_csv_path))\n",
    "    print(f\"‚úì GCPs also exported to CSV: {gcp_csv_path}\")\n",
    "    \n",
    "    # Use CSV file for processing (more reliable than XML)\n",
    "    gcp_file_for_processing = gcp_csv_path\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cannot export GCPs without qualicum_beach_gcp_analysis package\")\n",
    "    print(\"   GCPs are already in WGS84 format and can be used directly\")\n",
    "    gcp_file_for_processing = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process Orthomosaic WITHOUT GCPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if METASHAPE_AVAILABLE is defined\n",
    "try:\n",
    "    _ = METASHAPE_AVAILABLE\n",
    "except NameError:\n",
    "    METASHAPE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  METASHAPE_AVAILABLE not defined. Assuming MetaShape is not available.\")\n",
    "\n",
    "if not METASHAPE_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è  MetaShape not available. Skipping processing.\")\n",
    "else:\n",
    "    # Setup paths for processing\n",
    "    intermediate_dir = output_dir / \"intermediate\"\n",
    "    ortho_output_dir = output_dir / \"orthomosaics\"\n",
    "    \n",
    "    # Process orthomosaic WITHOUT GCPs\n",
    "    # Note: clean_intermediate_files=False will reuse existing processing steps\n",
    "    # Set to True to start fresh and delete previous work\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Processing orthomosaic WITHOUT GCPs...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    project_path_no_gcps = intermediate_dir / \"orthomosaic_no_gcps.psx\"\n",
    "    \n",
    "    if USE_QUALICUM_PACKAGE:\n",
    "        stats_no_gcps = process_orthomosaic(\n",
    "            photos_dir=photos_dir,\n",
    "            output_path=ortho_output_dir,\n",
    "            project_path=project_path_no_gcps,\n",
    "            product_id=\"orthomosaic_no_gcps\",\n",
    "            clean_intermediate_files=False,  # Reuse existing processing if available\n",
    "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
    "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
    "            tiepoint_limit=10000,\n",
    "            use_gcps=False\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Cannot process orthomosaic without qualicum_beach_gcp_analysis package\")\n",
    "        stats_no_gcps = None\n",
    "    \n",
    "    if stats_no_gcps:\n",
    "        print(\"\\n‚úì Orthomosaic processing (without GCPs) complete!\")\n",
    "        print(f\"  Number of photos: {stats_no_gcps['num_photos']}\")\n",
    "        print(f\"\\nüìÅ Output Files:\")\n",
    "        ortho_path_no_gcps = Path(stats_no_gcps['ortho_path'])\n",
    "        if ortho_path_no_gcps.exists():\n",
    "            file_size_mb = ortho_path_no_gcps.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  ‚úì Orthomosaic GeoTIFF: {ortho_path_no_gcps.absolute()}\")\n",
    "            print(f\"    Size: {file_size_mb:.2f} MB (LZW compressed, lossless)\")\n",
    "        if 'log_file_path' in stats_no_gcps:\n",
    "            print(f\"  üìù Log file: {stats_no_gcps['log_file_path']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Process Orthomosaic WITH GCPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if METASHAPE_AVAILABLE is defined\n",
    "try:\n",
    "    _ = METASHAPE_AVAILABLE\n",
    "except NameError:\n",
    "    METASHAPE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  METASHAPE_AVAILABLE not defined. Assuming MetaShape is not available.\")\n",
    "\n",
    "if not METASHAPE_AVAILABLE:\n",
    "    print(\"‚ö†Ô∏è  MetaShape not available. Skipping processing.\")\n",
    "else:\n",
    "    # Setup paths for processing\n",
    "    intermediate_dir = output_dir / \"intermediate\"\n",
    "    ortho_output_dir = output_dir / \"orthomosaics\"\n",
    "    \n",
    "    # Process orthomosaic WITH GCPs\n",
    "    # Note: clean_intermediate_files=False will reuse existing processing steps\n",
    "    # Set to True to start fresh and delete previous work\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Processing orthomosaic WITH GCPs...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    project_path_with_gcps = intermediate_dir / \"orthomosaic_with_gcps.psx\"\n",
    "    \n",
    "    if USE_QUALICUM_PACKAGE and gcp_file_for_processing:\n",
    "        stats_with_gcps = process_orthomosaic(\n",
    "            photos_dir=photos_dir,\n",
    "            output_path=ortho_output_dir,\n",
    "            project_path=project_path_with_gcps,\n",
    "            gcp_file=gcp_file_for_processing,\n",
    "            product_id=\"orthomosaic_with_gcps\",\n",
    "            clean_intermediate_files=False,  # Reuse existing processing if available\n",
    "            photo_match_quality=PhotoMatchQuality.MediumQuality,\n",
    "            depth_map_quality=DepthMapQuality.MediumQuality,\n",
    "            tiepoint_limit=10000,\n",
    "            use_gcps=True,\n",
    "            gcp_accuracy=0.01  # Very low accuracy (1cm) for very high weight in bundle adjustment\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Cannot process orthomosaic with GCPs without qualicum_beach_gcp_analysis package\")\n",
    "        stats_with_gcps = None\n",
    "    \n",
    "    if stats_with_gcps:\n",
    "        print(\"\\n‚úì Orthomosaic processing (with GCPs) complete!\")\n",
    "        print(f\"  Number of photos: {stats_with_gcps['num_photos']}\")\n",
    "        print(f\"  Number of markers: {stats_with_gcps.get('num_markers', 0)}\")\n",
    "        print(f\"\\nüìÅ Output Files:\")\n",
    "        ortho_path_with_gcps = Path(stats_with_gcps['ortho_path'])\n",
    "        if ortho_path_with_gcps.exists():\n",
    "            file_size_mb = ortho_path_with_gcps.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  ‚úì Orthomosaic GeoTIFF: {ortho_path_with_gcps.absolute()}\")\n",
    "            print(f\"    Size: {file_size_mb:.2f} MB (LZW compressed, lossless)\")\n",
    "        if 'log_file_path' in stats_with_gcps:\n",
    "            print(f\"  üìù Log file: {stats_with_gcps['log_file_path']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has:\n",
    "1. ‚úì Loaded KMZ file and extracted H3 cells\n",
    "2. ‚úì Downloaded images from 6 manifest files (with caching to avoid re-downloading)\n",
    "3. ‚úì Loaded and converted GCPs from UTM to WGS84\n",
    "4. ‚úì Created orthomosaics with and without GCPs using MetaShape\n",
    "5. ‚úì Saved all intermediate files to avoid recomputation\n",
    "\n",
    "**Note**: \n",
    "- All MetaShape processing steps check for existing intermediate results and skip recomputation if they already exist. This means you can safely re-run cells without losing progress.\n",
    "- GeoTIFF orthomosaics are exported with **LZW lossless compression** to reduce file size by 30-50% without any visual quality loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to install from requirements.txt first\n",
    "requirements_file = Path(\"requirements.txt\")\n",
    "if requirements_file.exists():\n",
    "    print(\"Installing packages from requirements.txt...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(requirements_file)])\n",
    "    print(\"‚úì Packages installed from requirements.txt\")\n",
    "else:\n",
    "    # Fallback: install packages individually\n",
    "    print(\"requirements.txt not found. Installing packages individually...\")\n",
    "    packages = [\n",
    "        \"numpy>=1.24.0\",\n",
    "        \"rasterio>=1.3.0\",\n",
    "        \"pillow>=10.0.0\",\n",
    "        \"matplotlib>=3.7.0\",\n",
    "        \"pandas>=2.0.0\",\n",
    "        \"pyproj>=3.6.0\",\n",
    "        \"requests>=2.31.0\",\n",
    "        \"utm>=0.7.0\",\n",
    "        \"h3>=3.7.0\",\n",
    "        \"boto3>=1.28.0\",\n",
    "    ]\n",
    "    for package in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    print(\"‚úì All packages installed\")\n",
    "\n",
    "# Check for MetaShape\n",
    "try:\n",
    "    import Metashape\n",
    "    print(\"‚úì MetaShape Python API is available\")\n",
    "    METASHAPE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  MetaShape Python API not found. Please install Agisoft MetaShape and its Python API.\")\n",
    "    METASHAPE_AVAILABLE = False\n",
    "\n",
    "print(\"\\nSetup complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
