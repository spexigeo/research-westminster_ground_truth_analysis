{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing opencv-python...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in /Users/mauriciohessflores/Library/Python/3.9/lib/python/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /Users/mauriciohessflores/Library/Python/3.9/lib/python/site-packages (from opencv-python) (2.0.2)\n",
      "Installing pillow...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pillow in /Users/mauriciohessflores/Library/Python/3.9/lib/python/site-packages (11.3.0)\n",
      "\u2713 Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if needed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = [\n",
    "    'rasterio',\n",
    "    'numpy',\n",
    "    'matplotlib',\n",
    "    'opencv-python',\n",
    "    'scipy',\n",
    "    'utm',\n",
    "    'pillow'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "\n",
    "print(\"\u2713 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup - Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Output directory: outputs/gcp_matching\n",
      "  - Patches: outputs/gcp_matching/patches\n",
      "  - Matches: outputs/gcp_matching/matches\n",
      "  - Registered: outputs/gcp_matching/registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import xy\n",
    "from rasterio.warp import transform as transform_coords\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import json\n",
    "import csv\n",
    "import utm\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "data_dir = Path(\"/Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25\")\n",
    "output_dir = Path(\"outputs\")\n",
    "\n",
    "# Input files\n",
    "basemap_path = data_dir / \"Michael_RTK_orthos\" / \"TestsiteNewWest_Spexigeo_RTK.tiff\"\n",
    "gcp_csv_path = data_dir / \"25-3288-CONTROL-NAD83-UTM10N-EGM2008.csv\"\n",
    "ortho_no_gcps_path = output_dir / \"orthomosaics\" / \"orthomosaic_no_gcps.tif\"\n",
    "ortho_with_gcps_path = output_dir / \"orthomosaics\" / \"orthomosaic_with_gcps.tif\"\n",
    "\n",
    "# Output directories\n",
    "gcp_matching_dir = output_dir / \"gcp_matching\"\n",
    "gcp_matching_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "patches_dir = gcp_matching_dir / \"patches\"\n",
    "patches_dir.mkdir(exist_ok=True)\n",
    "\n",
    "matches_dir = gcp_matching_dir / \"matches\"\n",
    "matches_dir.mkdir(exist_ok=True)\n",
    "\n",
    "registered_dir = gcp_matching_dir / \"registered\"\n",
    "registered_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\u2713 Output directory: {gcp_matching_dir}\")\n",
    "print(f\"  - Patches: {patches_dir}\")\n",
    "print(f\"  - Matches: {matches_dir}\")\n",
    "print(f\"  - Registered: {registered_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GCPs from UTM CSV file\n",
    "def load_gcps_from_csv(csv_path: Path) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load GCPs from UTM CSV file and convert to WGS84.\n",
    "    \n",
    "    Expected format: ID, Northing, Easting, Elevation, Name\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    \n",
    "    gcps = []\n",
    "    \n",
    "    with open(csv_path, 'r') as f:\n",
    "        # Try to detect if there's a header\n",
    "        first_line = f.readline().strip()\n",
    "        f.seek(0)  # Reset to beginning\n",
    "        \n",
    "        # Check if first line is numeric (no header)\n",
    "        try:\n",
    "            float(first_line.split(',')[0])\n",
    "            has_header = False\n",
    "        except (ValueError, IndexError):\n",
    "            has_header = True\n",
    "        \n",
    "        reader = csv.reader(f) if not has_header else csv.DictReader(f)\n",
    "        \n",
    "        for row_idx, row in enumerate(reader):\n",
    "            try:\n",
    "                if has_header:\n",
    "                    # Try to find columns\n",
    "                    northing = float(row.get('Northing', row.get('northing', row.get('Y', 0))))\n",
    "                    easting = float(row.get('Easting', row.get('easting', row.get('X', 0))))\n",
    "                    gcp_id = row.get('Name', row.get('name', row.get('ID', row.get('id', f\"GCP_{row_idx+1}\"))))\n",
    "                else:\n",
    "                    # Positional format: ID, Northing, Easting, Elevation, Name\n",
    "                    if len(row) < 3:\n",
    "                        continue\n",
    "                    gcp_id = row[0].strip() if row[0] else f\"GCP_{row_idx+1}\"\n",
    "                    northing = float(row[1])  # Column 1 = Northing\n",
    "                    easting = float(row[2])   # Column 2 = Easting\n",
    "                \n",
    "                # Convert UTM to WGS84 (UTM Zone 10N)\n",
    "                lat, lon = utm.to_latlon(easting, northing, 10, 'N')\n",
    "                \n",
    "                gcps.append({\n",
    "                    'id': gcp_id,\n",
    "                    'lat': lat,\n",
    "                    'lon': lon,\n",
    "                    'x_utm': easting,\n",
    "                    'y_utm': northing\n",
    "                })\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"\u26a0\ufe0f  Skipping row {row_idx+1}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return gcps\n",
    "\n",
    "## Step 2: Load GCPs from CSV and Convert to WGS84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GCPs from GeoJSON: outputs/ground_control_comparison/gcps_wgs84.geojson\n",
      "\u2713 Loaded 23 GCPs from GeoJSON\n",
      "\n",
      "First few GCPs:\n",
      "  GCP1: UTM=(506914.12, 5450945.53), WGS84=(49.211262, -122.905068)\n",
      "  GCP2: UTM=(506657.79, 5450730.01), WGS84=(49.209326, -122.908591)\n",
      "  GCP3: UTM=(506577.77, 5450480.01), WGS84=(49.207078, -122.909694)\n"
     ]
    }
   ],
   "source": [
    "# Load GCPs - try existing WGS84 files first, otherwise parse CSV\n",
    "import json\n",
    "\n",
    "# Check for existing WGS84 GCP files from ground control comparison\n",
    "gcps_wgs84_geojson = output_dir / \"ground_control_comparison\" / \"gcps_wgs84.geojson\"\n",
    "gcps_wgs84_csv = output_dir / \"ground_control_comparison\" / \"gcps_wgs84.csv\"\n",
    "\n",
    "gcps = []\n",
    "\n",
    "# Try GeoJSON first (preferred)\n",
    "if gcps_wgs84_geojson.exists():\n",
    "    print(f\"Loading GCPs from GeoJSON: {gcps_wgs84_geojson}\")\n",
    "    with open(gcps_wgs84_geojson, 'r') as f:\n",
    "        geojson_data = json.load(f)\n",
    "    \n",
    "    if 'features' in geojson_data:\n",
    "        for feature in geojson_data['features']:\n",
    "            props = feature.get('properties', {})\n",
    "            geom = feature.get('geometry', {})\n",
    "            \n",
    "            if geom.get('type') == 'Point':\n",
    "                coords = geom.get('coordinates', [])\n",
    "                if len(coords) >= 2:\n",
    "                    lon, lat = coords[0], coords[1]\n",
    "                    \n",
    "                    # Get UTM coordinates from properties or convert\n",
    "                    x_utm = props.get('x_utm')\n",
    "                    y_utm = props.get('y_utm')\n",
    "                    \n",
    "                    if x_utm is None or y_utm is None:\n",
    "                        # Convert WGS84 to UTM\n",
    "                        import utm\n",
    "                        x_utm, y_utm, zone_num, zone_letter = utm.from_latlon(lat, lon)\n",
    "                    \n",
    "                    gcps.append({\n",
    "                        'id': props.get('id', props.get('name', f\"GCP_{len(gcps)+1}\")),\n",
    "                        'lat': lat,\n",
    "                        'lon': lon,\n",
    "                        'x_utm': float(x_utm),\n",
    "                        'y_utm': float(y_utm)\n",
    "                    })\n",
    "    \n",
    "    print(f\"\u2713 Loaded {len(gcps)} GCPs from GeoJSON\")\n",
    "\n",
    "# Try CSV if GeoJSON not found\n",
    "elif gcps_wgs84_csv.exists():\n",
    "    print(f\"Loading GCPs from WGS84 CSV: {gcps_wgs84_csv}\")\n",
    "    with open(gcps_wgs84_csv, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                lat = float(row.get('lat', row.get('latitude', 0)))\n",
    "                lon = float(row.get('lon', row.get('longitude', row.get('lon', 0))))\n",
    "                gcp_id = row.get('id', row.get('name', row.get('label', f\"GCP_{len(gcps)+1}\")))\n",
    "                \n",
    "                # Get UTM from row or convert\n",
    "                x_utm_str = row.get('x_utm', '')\n",
    "                y_utm_str = row.get('y_utm', '')\n",
    "                \n",
    "                if x_utm_str and y_utm_str:\n",
    "                    x_utm = float(x_utm_str)\n",
    "                    y_utm = float(y_utm_str)\n",
    "                else:\n",
    "                    import utm\n",
    "                    x_utm, y_utm, zone_num, zone_letter = utm.from_latlon(lat, lon)\n",
    "                \n",
    "                gcps.append({\n",
    "                    'id': gcp_id,\n",
    "                    'lat': lat,\n",
    "                    'lon': lon,\n",
    "                    'x_utm': x_utm,\n",
    "                    'y_utm': y_utm\n",
    "                })\n",
    "            except (ValueError, KeyError) as e:\n",
    "                print(f\"\u26a0\ufe0f  Skipping row: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"\u2713 Loaded {len(gcps)} GCPs from WGS84 CSV\")\n",
    "\n",
    "# Fallback to parsing UTM CSV\n",
    "if len(gcps) == 0:\n",
    "    print(f\"\\nNo WGS84 GCP files found, parsing UTM CSV: {gcp_csv_path}\")\n",
    "    gcps = load_gcps_from_csv(gcp_csv_path)\n",
    "    print(f\"\u2713 Loaded {len(gcps)} GCPs from UTM CSV\")\n",
    "\n",
    "if len(gcps) > 0:\n",
    "    print(f\"\\nFirst few GCPs:\")\n",
    "    for gcp in gcps[:3]:\n",
    "        print(f\"  {gcp['id']}: UTM=({gcp['x_utm']:.2f}, {gcp['y_utm']:.2f}), WGS84=({gcp['lat']:.6f}, {gcp['lon']:.6f})\")\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f  No GCPs loaded!\")\n",
    "    print(f\"   Checked:\")\n",
    "    print(f\"   - {gcps_wgs84_geojson}\")\n",
    "    print(f\"   - {gcps_wgs84_csv}\")\n",
    "    print(f\"   - {gcp_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert GCPs to Pixel Coordinates in Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basemap CRS: EPSG:32610\n",
      "Basemap dimensions: 90129x90188\n",
      "Basemap bounds: BoundingBox(left=506424.37839793676, bottom=5450017.622213458, right=507501.0951215451, top=5451095.043774429)\n",
      "Basemap transform: | 0.01, 0.00, 506424.38|\n",
      "| 0.00,-0.01, 5451095.04|\n",
      "| 0.00, 0.00, 1.00|\n",
      "\n",
      "GCP GCP1: UTM=(506914.12, 5450945.53)\n",
      "  \u2713 Found at pixel: col=40995, row=12515\n",
      "\n",
      "GCP GCP2: UTM=(506657.79, 5450730.01)\n",
      "  \u2713 Found at pixel: col=19538, row=30556\n",
      "\n",
      "GCP GCP3: UTM=(506577.77, 5450480.01)\n",
      "  \u2713 Found at pixel: col=12840, row=51482\n",
      "\n",
      "GCP GCP4: UTM=(506765.03, 5450578.63)\n",
      "  \u2713 Found at pixel: col=28515, row=43227\n",
      "\n",
      "GCP GCP5: UTM=(506926.13, 5450715.96)\n",
      "  \u2713 Found at pixel: col=42000, row=31732\n",
      "\n",
      "GCP GCP6: UTM=(507071.92, 5450992.66)\n",
      "  \u2713 Found at pixel: col=54203, row=8570\n",
      "\n",
      "GCP GCP7: UTM=(507089.40, 5450794.23)\n",
      "  \u2713 Found at pixel: col=55667, row=25180\n",
      "\n",
      "GCP GCP8: UTM=(507315.01, 5450717.85)\n",
      "  \u2713 Found at pixel: col=74551, row=31574\n",
      "\n",
      "GCP GCP9: UTM=(507252.65, 5450536.03)\n",
      "  \u2713 Found at pixel: col=69332, row=46793\n",
      "\n",
      "GCP GCP10: UTM=(507072.02, 5450500.33)\n",
      "  \u2713 Found at pixel: col=54212, row=49782\n",
      "\n",
      "GCP GCP11: UTM=(507020.05, 5450595.29)\n",
      "  \u2713 Found at pixel: col=49861, row=41832\n",
      "\n",
      "GCP GCP12: UTM=(506885.91, 5450346.64)\n",
      "  \u2713 Found at pixel: col=38633, row=62646\n",
      "\n",
      "GCP GCP13: UTM=(506586.74, 5450323.69)\n",
      "  \u2713 Found at pixel: col=13590, row=64568\n",
      "\n",
      "GCP GCP14: UTM=(506746.87, 5450378.55)\n",
      "  \u2713 Found at pixel: col=26994, row=59975\n",
      "\n",
      "GCP GCP15: UTM=(506903.07, 5450193.68)\n",
      "  \u2713 Found at pixel: col=40069, row=75450\n",
      "\n",
      "GCP GCP16: UTM=(506977.00, 5450109.82)\n",
      "  \u2713 Found at pixel: col=46258, row=82470\n",
      "\n",
      "GCP CP1: UTM=(507060.63, 5450345.17)\n",
      "  \u2713 Found at pixel: col=53258, row=62769\n",
      "\n",
      "GCP CP2: UTM=(506911.89, 5450506.71)\n",
      "  \u2713 Found at pixel: col=40808, row=49247\n",
      "\n",
      "GCP CP3: UTM=(506821.64, 5450292.79)\n",
      "  \u2713 Found at pixel: col=33254, row=67154\n",
      "\n",
      "GCP CP4: UTM=(507148.30, 5450712.13)\n",
      "  \u2713 Found at pixel: col=60597, row=32052\n",
      "\n",
      "GCP CP5: UTM=(506800.86, 5450859.45)\n",
      "  \u2713 Found at pixel: col=31514, row=19720\n",
      "\n",
      "GCP OMON 6076: UTM=(506864.18, 5450326.10)\n",
      "  \u2713 Found at pixel: col=36814, row=64366\n",
      "\n",
      "GCP OMON 6066: UTM=(507163.74, 5450716.44)\n",
      "  \u2713 Found at pixel: col=61889, row=31691\n",
      "\n",
      "\u2713 Found 23 GCPs within basemap bounds\n",
      "\n",
      "First few GCP pixel coordinates:\n",
      "  GCP1: col=40995, row=12515\n",
      "  GCP2: col=19538, row=30556\n",
      "  GCP3: col=12840, row=51482\n"
     ]
    }
   ],
   "source": [
    "# Convert GCPs (UTM) to pixel coordinates in basemap\n",
    "def gcp_to_pixel_coords_from_utm(gcp_x_utm: float, gcp_y_utm: float, raster_path: Path) -> Optional[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Convert GCP UTM coordinates to pixel coordinates in raster.\n",
    "    \n",
    "    Args:\n",
    "        gcp_x_utm: UTM Easting (EPSG:32610)\n",
    "        gcp_y_utm: UTM Northing (EPSG:32610)\n",
    "        raster_path: Path to raster file\n",
    "    \n",
    "    Returns:\n",
    "        (col, row) or None if outside bounds.\n",
    "    \"\"\"\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Raster should be in EPSG:32610 (UTM Zone 10N)\n",
    "        if src.crs != 'EPSG:32610':\n",
    "            # Transform UTM to raster CRS if needed\n",
    "            x, y = transform_coords(\n",
    "                'EPSG:32610',\n",
    "                src.crs,\n",
    "                [gcp_x_utm],\n",
    "                [gcp_y_utm]\n",
    "            )\n",
    "            utm_x, utm_y = x[0], y[0]\n",
    "        else:\n",
    "            utm_x, utm_y = gcp_x_utm, gcp_y_utm\n",
    "        \n",
    "        # Convert to pixel coordinates\n",
    "        row, col = rasterio.transform.rowcol(src.transform, utm_x, utm_y)\n",
    "        \n",
    "        # Check if within bounds\n",
    "        if 0 <= row < src.height and 0 <= col < src.width:\n",
    "            return (col, row)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Get basemap CRS and transform\n",
    "with rasterio.open(basemap_path) as basemap_src:\n",
    "    basemap_crs = basemap_src.crs\n",
    "    basemap_transform = basemap_src.transform\n",
    "    basemap_width = basemap_src.width\n",
    "    basemap_height = basemap_src.height\n",
    "    basemap_bounds = basemap_src.bounds\n",
    "\n",
    "print(f\"Basemap CRS: {basemap_crs}\")\n",
    "print(f\"Basemap dimensions: {basemap_width}x{basemap_height}\")\n",
    "print(f\"Basemap bounds: {basemap_bounds}\")\n",
    "print(f\"Basemap transform: {basemap_transform}\")\n",
    "\n",
    "# Convert all GCPs to pixel coordinates\n",
    "gcp_pixel_coords = {}\n",
    "for gcp in gcps:\n",
    "    # Debug: show GCP UTM coordinates\n",
    "    print(f\"\\nGCP {gcp['id']}: UTM=({gcp['x_utm']:.2f}, {gcp['y_utm']:.2f})\")\n",
    "    \n",
    "    pixel_coords = gcp_to_pixel_coords_from_utm(gcp['x_utm'], gcp['y_utm'], basemap_path)\n",
    "    if pixel_coords:\n",
    "        gcp_pixel_coords[gcp['id']] = {\n",
    "            'gcp': gcp,\n",
    "            'pixel_col': pixel_coords[0],\n",
    "            'pixel_row': pixel_coords[1],\n",
    "            'utm_x': gcp.get('x_utm'),\n",
    "            'utm_y': gcp.get('y_utm'),\n",
    "        }\n",
    "        print(f\"  \u2713 Found at pixel: col={pixel_coords[0]}, row={pixel_coords[1]}\")\n",
    "    else:\n",
    "        # Debug: show why it's outside bounds\n",
    "        with rasterio.open(basemap_path) as src:\n",
    "            row, col = rasterio.transform.rowcol(src.transform, gcp['x_utm'], gcp['y_utm'])\n",
    "            print(f\"  \u26a0\ufe0f  Outside bounds: col={col}, row={row}\")\n",
    "            print(f\"     Basemap: {src.width}x{src.height}\")\n",
    "            print(f\"     Basemap bounds: {src.bounds}\")\n",
    "            # Check if coordinates are in bounds in UTM space\n",
    "            in_x = src.bounds.left <= gcp['x_utm'] <= src.bounds.right\n",
    "            in_y = src.bounds.bottom <= gcp['y_utm'] <= src.bounds.top\n",
    "            print(f\"     UTM X in bounds: {in_x} ({src.bounds.left:.2f} <= {gcp['x_utm']:.2f} <= {src.bounds.right:.2f})\")\n",
    "            print(f\"     UTM Y in bounds: {in_y} ({src.bounds.bottom:.2f} <= {gcp['y_utm']:.2f} <= {src.bounds.top:.2f})\")\n",
    "\n",
    "print(f\"\\n\u2713 Found {len(gcp_pixel_coords)} GCPs within basemap bounds\")\n",
    "if len(gcp_pixel_coords) > 0:\n",
    "    print(f\"\\nFirst few GCP pixel coordinates:\")\n",
    "    for gcp_id, coords in list(gcp_pixel_coords.items())[:3]:\n",
    "        print(f\"  {gcp_id}: col={coords['pixel_col']}, row={coords['pixel_row']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract Patches from Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Extracted 23 patches of size 49x49\n",
      "\u2713 Extracted 23 patches of size 59x59\n",
      "\u2713 Extracted 23 patches of size 79x79\n",
      "\u2713 Extracted 23 patches of size 99x99\n",
      "\u2713 Extracted 23 patches of size 119x119\n",
      "\n",
      "\u2713 Patch extraction complete!\n"
     ]
    }
   ],
   "source": [
    "# Extract patches from basemap centered on GCPs\n",
    "def extract_patch(raster_path: Path, center_col: int, center_row: int, patch_size: int) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract a patch from raster centered on given pixel coordinates.\n",
    "    \n",
    "    Args:\n",
    "        raster_path: Path to raster file\n",
    "        center_col: Center column (x)\n",
    "        center_row: Center row (y)\n",
    "        patch_size: Size of patch (must be odd, e.g., 29, 39, 49)\n",
    "    \n",
    "    Returns:\n",
    "        Patch array (H, W, C) or None if out of bounds\n",
    "    \"\"\"\n",
    "    half_size = patch_size // 2\n",
    "    \n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Calculate bounds\n",
    "        col_start = max(0, center_col - half_size)\n",
    "        col_end = min(src.width, center_col + half_size + 1)\n",
    "        row_start = max(0, center_row - half_size)\n",
    "        row_end = min(src.height, center_row + half_size + 1)\n",
    "        \n",
    "        # Check if patch would be out of bounds\n",
    "        if col_end - col_start < patch_size or row_end - row_start < patch_size:\n",
    "            return None\n",
    "        \n",
    "        # Read patch\n",
    "        patch = src.read(\n",
    "            window=rasterio.windows.Window(col_start, row_start, col_end - col_start, row_end - row_start)\n",
    "        )\n",
    "        \n",
    "        # Transpose to (H, W, C) format\n",
    "        if len(patch.shape) == 3:\n",
    "            patch = np.transpose(patch, (1, 2, 0))\n",
    "        \n",
    "        # If single band, convert to 3-channel grayscale\n",
    "        if len(patch.shape) == 2:\n",
    "            patch = np.stack([patch, patch, patch], axis=-1)\n",
    "        \n",
    "        return patch\n",
    "\n",
    "def create_gcp_patch_visualization(\n",
    "    patch: np.ndarray,\n",
    "    patch_size: int,\n",
    "    output_path: Path\n",
    "):\n",
    "    \"\"\"\n",
    "    Create visualization of patch with GCP location marked.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as mpatches\n",
    "    \n",
    "    # Normalize patch if needed\n",
    "    if patch.dtype != np.uint8:\n",
    "        patch_min = patch.min()\n",
    "        patch_max = patch.max()\n",
    "        if patch_max > patch_min:\n",
    "            patch = ((patch - patch_min) / (patch_max - patch_min) * 255).astype(np.uint8)\n",
    "        else:\n",
    "            patch = np.zeros_like(patch, dtype=np.uint8)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    ax.imshow(patch)\n",
    "    \n",
    "    # Mark center (GCP location) with bright red dot\n",
    "    center_row, center_col = patch.shape[0] // 2, patch.shape[1] // 2\n",
    "    ax.plot(center_col, center_row, 'ro', markersize=15, markeredgewidth=2, markeredgecolor='white')\n",
    "    \n",
    "    # Draw yellow square around patch boundary\n",
    "    rect = mpatches.Rectangle(\n",
    "        (0, 0), patch.shape[1], patch.shape[0],\n",
    "        linewidth=3, edgecolor='yellow', facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_title(f'Matched Patch ({patch_size}x{patch_size})', fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Extract patches for different patch sizes\n",
    "patch_sizes = [49, 59, 79, 99, 119]  # Larger patches for better matching\n",
    "basemap_patches = {}\n",
    "\n",
    "for patch_size in patch_sizes:\n",
    "    basemap_patches[patch_size] = {}\n",
    "    \n",
    "    for gcp_id, coords in gcp_pixel_coords.items():\n",
    "        patch = extract_patch(\n",
    "            basemap_path,\n",
    "            coords['pixel_col'],\n",
    "            coords['pixel_row'],\n",
    "            patch_size\n",
    "        )\n",
    "        \n",
    "        if patch is not None:\n",
    "            basemap_patches[patch_size][gcp_id] = patch\n",
    "            \n",
    "            # Save patch as image for visualization\n",
    "            patch_path = patches_dir / f\"basemap_{gcp_id}_{patch_size}x{patch_size}.png\"\n",
    "            plt.imsave(patch_path, patch.astype(np.uint8))\n",
    "    \n",
    "    print(f\"\u2713 Extracted {len(basemap_patches[patch_size])} patches of size {patch_size}x{patch_size}\")\n",
    "\n",
    "print(f\"\\n\u2713 Patch extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Reproject Orthomosaics to Match Basemap CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found existing reprojected file: outputs/test_matching/reprojected/no_gcps_reprojected.tif\n",
      "  \u2713 Already exists: outputs/gcp_matching/reprojected/no_gcps_reprojected.tif\n",
      "\n",
      "Found existing reprojected file: outputs/test_matching/reprojected/with_gcps_reprojected.tif\n",
      "  \u2713 Already exists: outputs/gcp_matching/reprojected/with_gcps_reprojected.tif\n",
      "\n",
      "\u2713 Reprojection complete!\n"
     ]
    }
   ],
   "source": [
    "from rasterio.warp import calculate_default_transform, reproject, Resampling, transform_bounds\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio.enums import Resampling as RasterioResampling\n",
    "from affine import Affine\n",
    "\n",
    "# Reproject orthos to match basemap CRS and resolution\n",
    "def reproject_ortho_to_basemap(ortho_path: Path, basemap_path: Path, output_path: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Reproject orthomosaic to match basemap CRS and bounds.\n",
    "    Uses manual transform construction to avoid CPLE_AppDefinedError.\n",
    "    \"\"\"\n",
    "    if output_path.exists():\n",
    "        print(f\"  \u2713 Already reprojected: {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    with rasterio.open(basemap_path) as basemap_src:\n",
    "        target_crs = basemap_src.crs\n",
    "        target_bounds = basemap_src.bounds\n",
    "        target_transform = basemap_src.transform\n",
    "        target_width = basemap_src.width\n",
    "        target_height = basemap_src.height\n",
    "    \n",
    "    with rasterio.open(ortho_path) as ortho_src:\n",
    "        source_crs = ortho_src.crs\n",
    "        source_bounds = ortho_src.bounds\n",
    "        \n",
    "        if source_crs == target_crs:\n",
    "            print(f\"  \u2713 Already in target CRS\")\n",
    "            import shutil\n",
    "            shutil.copy(ortho_path, output_path)\n",
    "            return output_path\n",
    "        \n",
    "        # Transform source bounds to target CRS\n",
    "        print(f\"  Transforming source bounds to target CRS...\")\n",
    "        src_bounds_target_crs = transform_bounds(\n",
    "            source_crs, target_crs,\n",
    "            source_bounds.left, source_bounds.bottom,\n",
    "            source_bounds.right, source_bounds.top\n",
    "        )\n",
    "        \n",
    "        print(f\"  Source bounds in target CRS: {src_bounds_target_crs}\")\n",
    "        \n",
    "        # Get target pixel size\n",
    "        target_pixel_size_x = abs(target_transform[0])\n",
    "        target_pixel_size_y = abs(target_transform[4])\n",
    "        \n",
    "        # Use intersection of bounds\n",
    "        output_left = max(src_bounds_target_crs[0], target_bounds.left)\n",
    "        output_bottom = max(src_bounds_target_crs[1], target_bounds.bottom)\n",
    "        output_right = min(src_bounds_target_crs[2], target_bounds.right)\n",
    "        output_top = min(src_bounds_target_crs[3], target_bounds.top)\n",
    "        \n",
    "        print(f\"  Output bounds (intersection): left={output_left:.2f}, bottom={output_bottom:.2f}, right={output_right:.2f}, top={output_top:.2f}\")\n",
    "        \n",
    "        # Validate bounds\n",
    "        if output_right <= output_left or output_top <= output_bottom:\n",
    "            raise ValueError(f\"Invalid output bounds: width={output_right-output_left}, height={output_top-output_bottom}\")\n",
    "        \n",
    "        # Calculate dimensions using target pixel size\n",
    "        width = int((output_right - output_left) / target_pixel_size_x)\n",
    "        height = int((output_top - output_bottom) / target_pixel_size_y)\n",
    "        \n",
    "        # Validate dimensions\n",
    "        if width <= 0 or height <= 0:\n",
    "            raise ValueError(f\"Invalid dimensions: width={width}, height={height}\")\n",
    "        \n",
    "        # Create transform for output\n",
    "        transform = Affine.translation(output_left, output_top) * Affine.scale(target_pixel_size_x, -target_pixel_size_y)\n",
    "        \n",
    "        print(f\"  \u2713 Transform calculated: {width}x{height} pixels\")\n",
    "        \n",
    "        # Read source data\n",
    "        source_data = ortho_src.read()\n",
    "        source_count = ortho_src.count\n",
    "        \n",
    "        # Reproject\n",
    "        reprojected_data = np.zeros((source_count, height, width), dtype=source_data.dtype)\n",
    "        \n",
    "        for band_idx in range(1, source_count + 1):\n",
    "            reproject(\n",
    "                source=rasterio.band(ortho_src, band_idx),\n",
    "                destination=reprojected_data[band_idx - 1],\n",
    "                src_transform=ortho_src.transform,\n",
    "                src_crs=source_crs,\n",
    "                dst_transform=transform,\n",
    "                dst_crs=target_crs,\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "        \n",
    "        # Save\n",
    "        with rasterio.open(\n",
    "            output_path,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=height,\n",
    "            width=width,\n",
    "            count=source_count,\n",
    "            dtype=reprojected_data.dtype,\n",
    "            crs=target_crs,\n",
    "            transform=transform,\n",
    "            compress='lzw',\n",
    "            BIGTIFF='YES',\n",
    "            tiled=True,\n",
    "            blockxsize=512,\n",
    "            blockysize=512\n",
    "        ) as dst:\n",
    "            dst.write(reprojected_data)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Check for existing reprojected files from test_matching notebook\n",
    "existing_reprojected_dir = output_dir / \"test_matching\" / \"reprojected\"\n",
    "reprojected_dir = gcp_matching_dir / \"reprojected\"\n",
    "reprojected_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ortho_paths = {\n",
    "    'no_gcps': ortho_no_gcps_path,\n",
    "    'with_gcps': ortho_with_gcps_path\n",
    "}\n",
    "\n",
    "reprojected_paths = {}\n",
    "for ortho_name, ortho_path in ortho_paths.items():\n",
    "    if not ortho_path.exists():\n",
    "        print(f\"\u26a0\ufe0f  Ortho not found: {ortho_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Check for existing reprojected file from test_matching\n",
    "    existing_reprojected = existing_reprojected_dir / f\"{ortho_name}_reprojected.tif\"\n",
    "    if existing_reprojected.exists():\n",
    "        print(f\"\\nFound existing reprojected file: {existing_reprojected}\")\n",
    "        # Copy to our directory\n",
    "        import shutil\n",
    "        reprojected_path = reprojected_dir / f\"{ortho_name}_reprojected.tif\"\n",
    "        if not reprojected_path.exists():\n",
    "            shutil.copy(existing_reprojected, reprojected_path)\n",
    "            print(f\"  \u2713 Copied to: {reprojected_path}\")\n",
    "        else:\n",
    "            print(f\"  \u2713 Already exists: {reprojected_path}\")\n",
    "        reprojected_paths[ortho_name] = reprojected_path\n",
    "        continue\n",
    "    \n",
    "    # Otherwise, reproject\n",
    "    print(f\"\\nReprojecting {ortho_name}...\")\n",
    "    reprojected_path = reproject_ortho_to_basemap(\n",
    "        ortho_path,\n",
    "        basemap_path,\n",
    "        reprojected_dir / f\"{ortho_name}_reprojected.tif\"\n",
    "    )\n",
    "    reprojected_paths[ortho_name] = reprojected_path\n",
    "\n",
    "print(f\"\\n\u2713 Reprojection complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Find GCP Patches in Orthomosaics Using Template Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch size 49x49: 21/23 matches\n",
      "  Patch size 59x59: 21/23 matches\n",
      "  Patch size 79x79: 19/23 matches\n",
      "  Patch size 99x99: 18/23 matches\n",
      "  Patch size 119x119: 18/23 matches\n",
      "\n",
      "  \u2713 Best patch size: 49x49 (21 matches)\n",
      "\n",
      "  \u2713 Best patch size: 49x49 (21 matches)\n",
      "  Saving matching patches to outputs/gcp_matching/matching_patches/no_gcps...\n",
      "  \u2713 Saved 21 matching patches\n",
      "  Patch size 49x49: 21/23 matches\n",
      "  Patch size 59x59: 20/23 matches\n",
      "  Patch size 79x79: 19/23 matches\n",
      "  Patch size 99x99: 18/23 matches\n",
      "  Patch size 119x119: 17/23 matches\n",
      "\n",
      "  \u2713 Best patch size: 49x49 (21 matches)\n",
      "\n",
      "  \u2713 Best patch size: 49x49 (21 matches)\n",
      "  Saving matching patches to outputs/gcp_matching/matching_patches/with_gcps...\n",
      "  \u2713 Saved 21 matching patches\n",
      "\n",
      "\u2713 Patch matching complete!\n",
      "\n",
      "Creating visualization of GCP matches...\n",
      "  \u2713 Visualization already exists: outputs/gcp_matching/matches/gcp_matching_visualization_no_gcps.png\n",
      "  Skipping visualization creation...\n",
      "  \u2713 Visualization already exists: outputs/gcp_matching/matches/gcp_matching_visualization_with_gcps.png\n",
      "  Skipping visualization creation...\n"
     ]
    }
   ],
   "source": [
    "# Find GCP patches in orthomosaics using template matching\n",
    "def find_patch_in_ortho(\n",
    "    template_patch: np.ndarray,\n",
    "    ortho_path: Path,\n",
    "    search_center_col: int,\n",
    "    search_center_row: int,\n",
    "    search_radius: int = 300  # Reduced for more precise matching\n",
    ") -> Optional[Tuple[int, int, float]]:\n",
    "    \"\"\"\n",
    "    Find template patch in orthomosaic using template matching.\n",
    "    \n",
    "    Returns:\n",
    "        (col, row, confidence) or None if not found\n",
    "    \"\"\"\n",
    "    # Convert template to grayscale if needed\n",
    "    if len(template_patch.shape) == 3:\n",
    "        template_gray = cv2.cvtColor(template_patch.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        template_gray = template_patch.astype(np.uint8)\n",
    "    \n",
    "    with rasterio.open(ortho_path) as ortho_src:\n",
    "        # Define search window\n",
    "        search_col_start = max(0, search_center_col - search_radius)\n",
    "        search_col_end = min(ortho_src.width, search_center_col + search_radius)\n",
    "        search_row_start = max(0, search_center_row - search_radius)\n",
    "        search_row_end = min(ortho_src.height, search_center_row + search_radius)\n",
    "        \n",
    "        # Read search region\n",
    "        search_window = rasterio.windows.Window(\n",
    "            search_col_start,\n",
    "            search_row_start,\n",
    "            search_col_end - search_col_start,\n",
    "            search_row_end - search_row_start\n",
    "        )\n",
    "        \n",
    "        search_region = ortho_src.read(window=search_window)\n",
    "        \n",
    "        # Convert to (H, W, C) and then grayscale\n",
    "        if len(search_region.shape) == 3:\n",
    "            search_region = np.transpose(search_region, (1, 2, 0))\n",
    "            if search_region.shape[2] == 1:\n",
    "                search_gray = search_region[:, :, 0]\n",
    "            else:\n",
    "                search_gray = cv2.cvtColor(search_region.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            search_gray = search_region\n",
    "        \n",
    "        # Normalize to uint8\n",
    "        if search_gray.dtype != np.uint8:\n",
    "            search_min = search_gray.min()\n",
    "            search_max = search_gray.max()\n",
    "            if search_max > search_min:\n",
    "                search_gray = ((search_gray - search_min) / (search_max - search_min) * 255).astype(np.uint8)\n",
    "            else:\n",
    "                search_gray = np.zeros_like(search_gray, dtype=np.uint8)\n",
    "        \n",
    "        # Template matching\n",
    "        result = cv2.matchTemplate(search_gray, template_gray, cv2.TM_CCOEFF_NORMED)\n",
    "        \n",
    "        # Find best match\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "        \n",
    "        # Convert back to global coordinates\n",
    "        match_col = search_col_start + max_loc[0] + template_gray.shape[1] // 2\n",
    "        match_row = search_row_start + max_loc[1] + template_gray.shape[0] // 2\n",
    "        \n",
    "        # Return if confidence is high enough\n",
    "        if max_val > 0.5:  # Threshold for match confidence\n",
    "            return (match_col, match_row, float(max_val))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Find GCPs in each orthomosaic\n",
    "# Create directory for matching patches\n",
    "matching_patches_dir = gcp_matching_dir / \"matching_patches\"\n",
    "matching_patches_dir.mkdir(exist_ok=True)\n",
    "matching_patches_dir.mkdir(exist_ok=True)\n",
    "\n",
    "matching_results = {}\n",
    "\n",
    "for ortho_name, reprojected_path in reprojected_paths.items():\n",
    "\n",
    "    matching_results[ortho_name] = {}\n",
    "    \n",
    "    # Get ortho transform for coordinate conversion\n",
    "    with rasterio.open(reprojected_path) as ortho_src:\n",
    "        ortho_transform = ortho_src.transform\n",
    "    \n",
    "    # Try different patch sizes\n",
    "    best_patch_size = None\n",
    "    best_matches = 0\n",
    "    \n",
    "    for patch_size in patch_sizes:\n",
    "        matches_found = 0\n",
    "        \n",
    "        for gcp_id, coords in gcp_pixel_coords.items():\n",
    "            if gcp_id not in basemap_patches[patch_size]:\n",
    "                continue\n",
    "            \n",
    "            template = basemap_patches[patch_size][gcp_id]\n",
    "            \n",
    "            # Convert GCP UTM coordinates to pixel coordinates in THIS ortho\n",
    "            gcp_utm_x = coords.get('utm_x') or coords.get('x_utm')\n",
    "            gcp_utm_y = coords.get('utm_y') or coords.get('y_utm')\n",
    "            \n",
    "            if gcp_utm_x is not None and gcp_utm_y is not None:\n",
    "                # Convert UTM to pixel coordinates using ortho's transform\n",
    "                expected_col, expected_row = ~ortho_transform * (gcp_utm_x, gcp_utm_y)\n",
    "                expected_col = int(expected_col)\n",
    "                expected_row = int(expected_row)\n",
    "            else:\n",
    "                # Fallback: use pixel coordinates from basemap\n",
    "                expected_col = coords['pixel_col']\n",
    "                expected_row = coords['pixel_row']\n",
    "            \n",
    "            # Search for patch using multi-scale matching\n",
    "            if 'find_patch_in_ortho_multiscale' in globals():\n",
    "                match = find_patch_in_ortho_multiscale(\n",
    "                    template,\n",
    "                    reprojected_path,\n",
    "                    expected_col,\n",
    "                    expected_row,\n",
    "                    search_radius=300\n",
    "                )\n",
    "            else:\n",
    "                match = find_patch_in_ortho(\n",
    "                    template,\n",
    "                    reprojected_path,\n",
    "                    expected_col,\n",
    "                    expected_row,\n",
    "                    search_radius=300\n",
    "                )\n",
    "            \n",
    "            # Validate match quality\n",
    "            if match and match[2] < 0.3:  # Confidence threshold\n",
    "                match = None\n",
    "            \n",
    "            if match:\n",
    "                match_col, match_row, confidence = match\n",
    "                matches_found += 1\n",
    "                \n",
    "                if gcp_id not in matching_results[ortho_name]:\n",
    "                    matching_results[ortho_name][gcp_id] = {}\n",
    "                \n",
    "                matching_results[ortho_name][gcp_id][patch_size] = {\n",
    "                    'expected_col': expected_col,\n",
    "                    'expected_row': expected_row,\n",
    "                    'matched_col': match_col,\n",
    "                    'matched_row': match_row,\n",
    "                    'offset_col': match_col - expected_col,\n",
    "                    'offset_row': match_row - expected_row,\n",
    "                    'confidence': confidence\n",
    "                }\n",
    "        \n",
    "        print(f\"  Patch size {patch_size}x{patch_size}: {matches_found}/{len(gcp_pixel_coords)} matches\")\n",
    "        \n",
    "        if matches_found > best_matches:\n",
    "            best_matches = matches_found\n",
    "            best_patch_size = patch_size\n",
    "    \n",
    "    print(f\"\\n  \u2713 Best patch size: {best_patch_size}x{best_patch_size} ({best_matches} matches)\")\n",
    "\n",
    "    print(f\"\\n  \u2713 Best patch size: {best_patch_size}x{best_patch_size} ({best_matches} matches)\")\n",
    "\n",
    "    # Create subdirectory for this ortho's matching patches\n",
    "    ortho_patches_dir = matching_patches_dir / ortho_name\n",
    "    ortho_patches_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Save matching patches for visual verification\n",
    "    print(f\"  Saving matching patches to {ortho_patches_dir}...\")\n",
    "    for gcp_id, match_data in matching_results[ortho_name].items():\n",
    "        if best_patch_size in match_data:\n",
    "            match = match_data[best_patch_size]\n",
    "            \n",
    "            # Extract patch from ortho at matched location\n",
    "            matched_col = match['matched_col']\n",
    "            matched_row = match['matched_row']\n",
    "            \n",
    "            # Extract patch (same size as template)\n",
    "            patch = extract_patch(\n",
    "                reprojected_path,\n",
    "                matched_col,\n",
    "                matched_row,\n",
    "                best_patch_size\n",
    "            )\n",
    "            \n",
    "            if patch is not None:\n",
    "                # Normalize patch for saving\n",
    "                if patch.dtype != np.uint8:\n",
    "                    patch_min = patch.min()\n",
    "                    patch_max = patch.max()\n",
    "                    if patch_max > patch_min:\n",
    "                        patch = ((patch - patch_min) / (patch_max - patch_min) * 255).astype(np.uint8)\n",
    "                    else:\n",
    "                        patch = np.zeros_like(patch, dtype=np.uint8)\n",
    "                \n",
    "                # Save matching patch\n",
    "                match_patch_path = ortho_patches_dir / f\"{gcp_id}_{best_patch_size}x{best_patch_size}_matched.png\"\n",
    "                plt.imsave(match_patch_path, patch)\n",
    "                \n",
    "                # Also create visualization with GCP location marked\n",
    "                vis_patch_path = ortho_patches_dir / f\"{gcp_id}_{best_patch_size}x{best_patch_size}_matched_vis.png\"\n",
    "                create_gcp_patch_visualization(patch, best_patch_size, vis_patch_path)\n",
    "    \n",
    "    print(f\"  \u2713 Saved {len([g for g in matching_results[ortho_name].keys() if best_patch_size in matching_results[ortho_name][g]])} matching patches\")\n",
    "\n",
    "print(f\"\\n\u2713 Patch matching complete!\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "print(f\"\\nCreating visualization of GCP matches...\")\n",
    "\n",
    "def create_gcp_matching_visualization(\n",
    "    basemap_path: Path,\n",
    "    ortho_paths: Dict[str, Path],\n",
    "    gcp_pixel_coords: Dict,\n",
    "    matching_results: Dict,\n",
    "    output_path: Path,\n",
    "    max_dimension: int = 4000\n",
    "):\n",
    "    \"\"\"\n",
    "    Create visualization showing basemap with GCPs and orthos with matched patches.\n",
    "    \"\"\"\n",
    "    # Load basemap\n",
    "    with rasterio.open(basemap_path) as src:\n",
    "        basemap_data = src.read()\n",
    "        basemap_transform = src.transform\n",
    "        \n",
    "        # Convert to (H, W, C)\n",
    "        if len(basemap_data.shape) == 3:\n",
    "            basemap_img = np.transpose(basemap_data, (1, 2, 0))\n",
    "            if basemap_img.shape[2] == 1:\n",
    "                basemap_img = np.stack([basemap_img[:, :, 0]] * 3, axis=-1)\n",
    "            elif basemap_img.shape[2] == 4:\n",
    "                basemap_img = basemap_img[:, :, :3]  # Take RGB\n",
    "        else:\n",
    "            basemap_img = np.stack([basemap_data] * 3, axis=-1)\n",
    "        \n",
    "        # Normalize to uint8\n",
    "        if basemap_img.dtype != np.uint8:\n",
    "            basemap_min = basemap_img.min()\n",
    "            basemap_max = basemap_img.max()\n",
    "            if basemap_max > basemap_min:\n",
    "                basemap_img = ((basemap_img - basemap_min) / (basemap_max - basemap_min) * 255).astype(np.uint8)\n",
    "            else:\n",
    "                basemap_img = np.zeros_like(basemap_img, dtype=np.uint8)\n",
    "    \n",
    "    # Downsample if too large\n",
    "    h, w = basemap_img.shape[:2]\n",
    "    if max(h, w) > max_dimension:\n",
    "        scale = max_dimension / max(h, w)\n",
    "        new_h, new_w = int(h * scale), int(w * scale)\n",
    "        basemap_img = cv2.resize(basemap_img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        scale_factor = scale\n",
    "    else:\n",
    "        scale_factor = 1.0\n",
    "    \n",
    "    # Load orthos and create panels\n",
    "    num_orthos = len(ortho_paths)\n",
    "    fig, axes = plt.subplots(1, num_orthos + 1, figsize=(8 * (num_orthos + 1), 8))\n",
    "    \n",
    "    # Basemap panel (left)\n",
    "    ax = axes[0]\n",
    "    basemap_display = basemap_img.copy()\n",
    "    \n",
    "    # Draw GCP positions on basemap\n",
    "    for gcp_id, coords in gcp_pixel_coords.items():\n",
    "        # Scale coordinates\n",
    "        col = int(coords['pixel_col'] * scale_factor)\n",
    "        row = int(coords['pixel_row'] * scale_factor)\n",
    "        \n",
    "        if 0 <= row < basemap_display.shape[0] and 0 <= col < basemap_display.shape[1]:\n",
    "            # Draw red circle\n",
    "            cv2.circle(basemap_display, (col, row), 10, (255, 0, 0), 3)\n",
    "    \n",
    "    ax.imshow(basemap_display)\n",
    "    ax.set_title('Basemap with GCP Locations', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add GCP labels\n",
    "    for gcp_id, coords in gcp_pixel_coords.items():\n",
    "        col = int(coords['pixel_col'] * scale_factor)\n",
    "        row = int(coords['pixel_row'] * scale_factor)\n",
    "        if 0 <= row < basemap_display.shape[0] and 0 <= col < basemap_display.shape[1]:\n",
    "            ax.text(col, row - 15, gcp_id, color='red', fontsize=8, fontweight='bold',\n",
    "                   ha='center', va='bottom')\n",
    "    \n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Ortho panels (right)\n",
    "    for ortho_idx, (ortho_name, ortho_path) in enumerate(ortho_paths.items(), 1):\n",
    "        ax = axes[ortho_idx]\n",
    "        \n",
    "        # Load ortho\n",
    "        with rasterio.open(ortho_path) as src:\n",
    "            ortho_data = src.read()\n",
    "            \n",
    "            # Convert to (H, W, C)\n",
    "            if len(ortho_data.shape) == 3:\n",
    "                ortho_img = np.transpose(ortho_data, (1, 2, 0))\n",
    "                if ortho_img.shape[2] == 1:\n",
    "                    ortho_img = np.stack([ortho_img[:, :, 0]] * 3, axis=-1)\n",
    "                elif ortho_img.shape[2] == 4:\n",
    "                    ortho_img = ortho_img[:, :, :3]\n",
    "            else:\n",
    "                ortho_img = np.stack([ortho_data] * 3, axis=-1)\n",
    "            \n",
    "            # Normalize\n",
    "            if ortho_img.dtype != np.uint8:\n",
    "                ortho_min = ortho_img.min()\n",
    "                ortho_max = ortho_img.max()\n",
    "                if ortho_max > ortho_min:\n",
    "                    ortho_img = ((ortho_img - ortho_min) / (ortho_max - ortho_min) * 255).astype(np.uint8)\n",
    "                else:\n",
    "                    ortho_img = np.zeros_like(ortho_img, dtype=np.uint8)\n",
    "        \n",
    "        # Downsample if too large\n",
    "        h, w = ortho_img.shape[:2]\n",
    "        if max(h, w) > max_dimension:\n",
    "            scale = max_dimension / max(h, w)\n",
    "            new_h, new_w = int(h * scale), int(w * scale)\n",
    "            ortho_img = cv2.resize(ortho_img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "            ortho_scale = scale\n",
    "        else:\n",
    "            ortho_scale = 1.0\n",
    "        \n",
    "        ortho_display = ortho_img.copy()\n",
    "        \n",
    "        # Draw matched patch centers\n",
    "        if ortho_name in matching_results:\n",
    "            for gcp_id, match_data in matching_results[ortho_name].items():\n",
    "                # Get best patch size match\n",
    "                best_patch_size = max(match_data.keys()) if match_data else None\n",
    "                if best_patch_size:\n",
    "                    match = match_data[best_patch_size]\n",
    "                    matched_col = int(match['matched_col'] * ortho_scale)\n",
    "                    matched_row = int(match['matched_row'] * ortho_scale)\n",
    "                    \n",
    "                    if 0 <= matched_row < ortho_display.shape[0] and 0 <= matched_col < ortho_display.shape[1]:\n",
    "                        # Draw yellow circle\n",
    "                        cv2.circle(ortho_display, (matched_col, matched_row), 10, (255, 255, 0), 3)\n",
    "        \n",
    "        ax.imshow(ortho_display)\n",
    "        ax.set_title(f'{ortho_name.replace(\"_\", \" \").title()} with Matched Patches', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Add labels\n",
    "        if ortho_name in matching_results:\n",
    "            for gcp_id, match_data in matching_results[ortho_name].items():\n",
    "                best_patch_size = max(match_data.keys()) if match_data else None\n",
    "                if best_patch_size:\n",
    "                    match = match_data[best_patch_size]\n",
    "                    matched_col = int(match['matched_col'] * ortho_scale)\n",
    "                    matched_row = int(match['matched_row'] * ortho_scale)\n",
    "                    if 0 <= matched_row < ortho_display.shape[0] and 0 <= matched_col < ortho_display.shape[1]:\n",
    "                        ax.text(matched_col, matched_row - 15, gcp_id, color='yellow', fontsize=8, fontweight='bold',\n",
    "                               ha='center', va='bottom')\n",
    "        \n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight', format='PNG')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\u2713 Visualization saved: {output_path}\")\n",
    "\n",
    "# Create visualization for each ortho\n",
    "for ortho_name in reprojected_paths.keys():\n",
    "    if ortho_name not in matching_results:\n",
    "        continue\n",
    "    \n",
    "    vis_path = matches_dir / f\"gcp_matching_visualization_{ortho_name}.png\"\n",
    "\n",
    "    # Check if visualization already exists\n",
    "    if vis_path.exists():\n",
    "        print(f\"  \u2713 Visualization already exists: {vis_path}\")\n",
    "        print(f\"  Skipping visualization creation...\")\n",
    "        continue\n",
    "\n",
    "    \n",
    "    create_gcp_matching_visualization(\n",
    "        basemap_path,\n",
    "        {ortho_name: reprojected_paths[ortho_name]},\n",
    "        gcp_pixel_coords,\n",
    "        matching_results,\n",
    "        vis_path,\n",
    "        max_dimension=4000\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compute 2D Shift or Affine Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import cv2\n",
    "\n",
    "def remove_outliers_ransac(src_points: np.ndarray, dst_points: np.ndarray, threshold: float = 50.0, min_samples: int = 3) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Remove outliers using RANSAC with proper model fitting.\n",
    "    \n",
    "    Returns:\n",
    "        (inlier_src, inlier_dst, inlier_mask)\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(src_points) < min_samples:\n",
    "        mask = np.ones(len(src_points), dtype=bool)\n",
    "        return src_points, dst_points, mask\n",
    "    \n",
    "    # Convert to numpy arrays if needed\n",
    "    src_points = np.array(src_points, dtype=np.float32)\n",
    "    dst_points = np.array(dst_points, dtype=np.float32)\n",
    "    \n",
    "    # Use RANSAC for X and Y separately, then combine\n",
    "    # For 2D shift, we fit a simple translation model\n",
    "    # Compute median shift as initial estimate\n",
    "    shifts = dst_points - src_points\n",
    "    median_shift = np.median(shifts, axis=0)\n",
    "    \n",
    "    # Compute distances from median shift\n",
    "    expected_dst = src_points + median_shift\n",
    "    distances = np.sqrt(np.sum((dst_points - expected_dst)**2, axis=1))\n",
    "    \n",
    "    # Use IQR method for outlier detection\n",
    "    q1 = np.percentile(distances, 25)\n",
    "    q3 = np.percentile(distances, 75)\n",
    "    iqr = q3 - q1\n",
    "    outlier_threshold = q3 + 2.5 * iqr  # More aggressive (was 1.5)\n",
    "    \n",
    "    # Also use absolute threshold (in pixels)\n",
    "    absolute_threshold = max(threshold, 100.0)  # At least 100 pixels\n",
    "    \n",
    "    # Mark outliers\n",
    "    inlier_mask = (distances <= outlier_threshold) & (distances <= absolute_threshold)\n",
    "    \n",
    "    # Ensure we have at least min_samples inliers\n",
    "    if np.sum(inlier_mask) < min_samples:\n",
    "        # Keep the min_samples points closest to the median\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        inlier_mask = np.zeros(len(src_points), dtype=bool)\n",
    "        inlier_mask[sorted_indices[:min_samples]] = True\n",
    "    \n",
    "    # Ensure inlier_mask is a proper boolean array\n",
    "    inlier_mask = np.asarray(inlier_mask, dtype=bool)\n",
    "    \n",
    "    # Return filtered points\n",
    "    return src_points[inlier_mask], dst_points[inlier_mask], inlier_mask\n",
    "\n",
    "def compute_transformation(matches: Dict, transformation_type: str = 'shift', match_distances: Optional[List[float]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute transformation from GCP matches.\n",
    "    \n",
    "    Args:\n",
    "        matches: Dictionary with GCP matches\n",
    "        transformation_type: 'shift', 'affine', 'homography', or 'deformable'\n",
    "        match_distances: Optional list of match distances for RANSAC weighting\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with transformation parameters\n",
    "    \"\"\"\n",
    "    # Collect source and destination points\n",
    "    src_points = []\n",
    "    dst_points = []\n",
    "    \n",
    "    for gcp_id, match_data in matches.items():\n",
    "        # Use the best patch size match\n",
    "        best_patch_size = max(match_data.keys())\n",
    "        match = match_data[best_patch_size]\n",
    "        \n",
    "        src_points.append([match['expected_col'], match['expected_row']])\n",
    "        dst_points.append([match['matched_col'], match['matched_row']])\n",
    "    \n",
    "    src_points = np.array(src_points, dtype=np.float32)\n",
    "    dst_points = np.array(dst_points, dtype=np.float32)\n",
    "\n",
    "    # Remove outliers using RANSAC with distance weighting\n",
    "    src_points, dst_points, inlier_mask = remove_outliers_ransac(src_points, dst_points, threshold=100.0, min_samples=3)\n",
    "    \n",
    "    if len(src_points) < 3:\n",
    "        return {'type': 'insufficient_points', 'error': 'Need at least 3 matches after outlier removal'}\n",
    "    \n",
    "    # Compute transformation based on type\n",
    "    if transformation_type == 'shift':\n",
    "        # Compute 2D shift (mean offset)\n",
    "        offsets = dst_points - src_points\n",
    "        shift_x = float(np.mean(offsets[:, 0]))\n",
    "        shift_y = float(np.mean(offsets[:, 1]))\n",
    "        \n",
    "        # Compute RMSE\n",
    "        errors = offsets - np.array([shift_x, shift_y])\n",
    "        rmse = float(np.sqrt(np.mean(np.sum(errors**2, axis=1))))\n",
    "        \n",
    "        return {\n",
    "            'type': 'shift',\n",
    "            'shift_x': shift_x,\n",
    "            'shift_y': shift_y,\n",
    "            'rmse': rmse,\n",
    "            'num_points': len(src_points)\n",
    "        }\n",
    "    \n",
    "    elif transformation_type == 'affine':\n",
    "        # Compute affine transformation using least squares (all points)\n",
    "        if len(src_points) < 3:\n",
    "            return {'type': 'insufficient_points', 'error': 'Need at least 3 points for affine'}\n",
    "        \n",
    "        # Build system: A * params = b\n",
    "        A = np.zeros((2 * len(src_points), 6))\n",
    "        b = np.zeros(2 * len(src_points))\n",
    "        \n",
    "        for k in range(len(src_points)):\n",
    "            x, y = src_points[k]\n",
    "            xp, yp = dst_points[k]\n",
    "            A[2*k, :] = [x, y, 1, 0, 0, 0]\n",
    "            b[2*k] = xp\n",
    "            A[2*k+1, :] = [0, 0, 0, x, y, 1]\n",
    "            b[2*k+1] = yp\n",
    "        \n",
    "        # Solve using least squares\n",
    "        params, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "        transform_matrix = params.reshape(2, 3)\n",
    "        \n",
    "        # Apply to all points to compute error\n",
    "        ones = np.ones((len(src_points), 1))\n",
    "        src_homogeneous = np.hstack([src_points, ones])\n",
    "        transformed = (transform_matrix @ src_homogeneous.T).T\n",
    "        \n",
    "        errors = dst_points - transformed\n",
    "        rmse = float(np.sqrt(np.mean(np.sum(errors**2, axis=1))))\n",
    "        \n",
    "        return {\n",
    "            'type': 'affine',\n",
    "            'matrix': transform_matrix.tolist(),\n",
    "            'rmse': rmse,\n",
    "            'num_points': len(src_points)\n",
    "        }\n",
    "    \n",
    "    elif transformation_type == 'homography':\n",
    "        # Compute homography transformation (8 parameters, requires at least 4 points)\n",
    "        if len(src_points) < 4:\n",
    "            return {'type': 'insufficient_points', 'error': 'Need at least 4 points for homography'}\n",
    "        \n",
    "        # Use cv2.findHomography with RANSAC\n",
    "        try:\n",
    "            homography_matrix, inlier_mask = cv2.findHomography(\n",
    "                src_points.reshape(-1, 1, 2),\n",
    "                dst_points.reshape(-1, 1, 2),\n",
    "                method=cv2.RANSAC,\n",
    "                ransacReprojThreshold=5.0,\n",
    "                maxIters=2000,\n",
    "                confidence=0.99\n",
    "            )\n",
    "            \n",
    "            if homography_matrix is not None:\n",
    "                inlier_src = src_points[inlier_mask.ravel() == 1]\n",
    "                \n",
    "                # Apply to all points to compute error\n",
    "                ones = np.ones((len(src_points), 1))\n",
    "                src_homogeneous = np.hstack([src_points, ones])\n",
    "                transformed = (homography_matrix @ src_homogeneous.T).T\n",
    "                transformed = transformed[:, :2] / transformed[:, 2:3]\n",
    "                \n",
    "                errors = dst_points - transformed\n",
    "                rmse = float(np.sqrt(np.mean(np.sum(errors**2, axis=1))))\n",
    "                \n",
    "                return {\n",
    "                    'type': 'homography',\n",
    "                    'matrix': homography_matrix.tolist(),\n",
    "                    'rmse': rmse,\n",
    "                    'num_points': len(inlier_src),\n",
    "                    'num_inliers': int(np.sum(inlier_mask))\n",
    "                }\n",
    "            else:\n",
    "                return {'type': 'homography_failed', 'error': 'Homography computation failed'}\n",
    "        except Exception as e:\n",
    "            return {'type': 'homography_error', 'error': str(e)}\n",
    "    \n",
    "    elif transformation_type == 'deformable':\n",
    "        # Compute deformable (curvilinear) transformation using thin-plate spline\n",
    "        if len(src_points) < 3:\n",
    "            return {'type': 'insufficient_points', 'error': 'Need at least 3 points for deformable transformation'}\n",
    "        \n",
    "        try:\n",
    "            from scipy.interpolate import RBFInterpolator\n",
    "            \n",
    "            # Use RANSAC to select inliers based on distance\n",
    "            if match_distances is not None and len(match_distances) == len(src_points):\n",
    "                # Weight by inverse distance (lower distance = higher weight)\n",
    "                weights = 1.0 / (np.array(match_distances) + 1e-6)\n",
    "                weights = weights / weights.sum()\n",
    "                # Select top 80% of points by weight (inliers)\n",
    "                sorted_indices = np.argsort(weights)[::-1]\n",
    "                num_inliers = max(3, int(0.8 * len(src_points)))\n",
    "                inlier_indices = sorted_indices[:num_inliers]\n",
    "                inlier_src = src_points[inlier_indices]\n",
    "                inlier_dst = dst_points[inlier_indices]\n",
    "            else:\n",
    "                inlier_src = src_points\n",
    "                inlier_dst = dst_points\n",
    "            \n",
    "            # Fit RBF interpolator (thin-plate spline)\n",
    "            rbf = RBFInterpolator(inlier_src, inlier_dst, kernel='thin_plate_spline', smoothing=0.0)\n",
    "            \n",
    "            # Evaluate on all points to compute error\n",
    "            transformed = rbf(src_points)\n",
    "            errors = dst_points - transformed\n",
    "            rmse = float(np.sqrt(np.mean(np.sum(errors**2, axis=1))))\n",
    "            \n",
    "            return {\n",
    "                'type': 'deformable',\n",
    "                'rmse': rmse,\n",
    "                'num_points': len(inlier_src),\n",
    "                'inlier_indices': inlier_indices.tolist() if 'inlier_indices' in locals() else None,\n",
    "                'src_points': inlier_src.tolist() if 'inlier_src' in locals() else src_points.tolist(),\n",
    "                'dst_points': inlier_dst.tolist() if 'inlier_dst' in locals() else dst_points.tolist()\n",
    "            }\n",
    "        except ImportError:\n",
    "            return {'type': 'deformable_error', 'error': 'scipy.interpolate.RBFInterpolator not available'}\n",
    "        except Exception as e:\n",
    "            return {'type': 'deformable_error', 'error': str(e)}\n",
    "    \n",
    "    else:\n",
    "        return {'type': 'unknown', 'error': f'Unknown transformation type: {transformation_type}'}\n",
    "\n",
    "# Compute transformations for each ortho\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 7: Compute Transformations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if matching_results is defined, load from file if not\n",
    "try:\n",
    "    _ = matching_results\n",
    "    print(\"\u2713 matching_results found in memory\")\n",
    "except NameError:\n",
    "    print(\"matching_results not in memory, attempting to load from file...\")\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    try:\n",
    "        try:\n",
    "            _ = output_dir\n",
    "        except NameError:\n",
    "            output_dir = Path(\"outputs\")\n",
    "        try:\n",
    "            _ = gcp_matching_dir\n",
    "        except NameError:\n",
    "            gcp_matching_dir = output_dir / \"gcp_matching\"\n",
    "        matches_dir = gcp_matching_dir / \"matches\"\n",
    "        \n",
    "        matching_results_file = matches_dir / \"matching_results.json\"\n",
    "        if matching_results_file.exists():\n",
    "            with open(matching_results_file, 'r') as f:\n",
    "                matching_results = json.load(f)\n",
    "            print(f\"\u2713 Loaded matching_results from {matching_results_file}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"matching_results.json not found at {matching_results_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Could not load matching_results: {e}\")\n",
    "        print(\"Please run Step 6 (Patch Matching) first.\")\n",
    "        raise\n",
    "\n",
    "# Compute transformations for each ortho\n",
    "transformations = {}\n",
    "\n",
    "for ortho_name in matching_results.keys():\n",
    "    print(f\"\\nProcessing {ortho_name}...\")\n",
    "    \n",
    "    # Get matches for this ortho\n",
    "    matches = matching_results[ortho_name]\n",
    "    \n",
    "    if len(matches) < 3:\n",
    "        print(f\"  \u26a0\ufe0f  Insufficient matches ({len(matches)}) for {ortho_name}\")\n",
    "        transformations[ortho_name] = {'error': f'Insufficient matches: {len(matches)}'}\n",
    "        continue\n",
    "    \n",
    "    # Extract match distances for RANSAC weighting\n",
    "    match_distances = []\n",
    "    for gcp_id, match_data in matches.items():\n",
    "        best_patch_size = max(match_data.keys())\n",
    "        match = match_data[best_patch_size]\n",
    "        if 'distance_m' in match:\n",
    "            match_distances.append(match['distance_m'])\n",
    "    \n",
    "    if len(match_distances) != len(matches):\n",
    "        match_distances = None\n",
    "    \n",
    "    # Compute all transformation types\n",
    "    transformation_results = {}\n",
    "    \n",
    "    for trans_type in ['shift', 'affine', 'homography', 'deformable']:\n",
    "        try:\n",
    "            result = compute_transformation(matches, trans_type, match_distances)\n",
    "            if 'error' not in result:\n",
    "                transformation_results[trans_type] = result\n",
    "                print(f\"  \u2713 {trans_type}: RMSE = {result.get('rmse', 'N/A'):.2f} pixels\")\n",
    "            else:\n",
    "                print(f\"  \u26a0\ufe0f  {trans_type}: {result.get('error', 'Unknown error')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  \u274c {trans_type} failed: {e}\")\n",
    "    \n",
    "    # Select top two transformations by RMSE\n",
    "    if len(transformation_results) == 0:\n",
    "        print(f\"  \u274c No valid transformations for {ortho_name}\")\n",
    "        transformations[ortho_name] = {'error': 'No valid transformations'}\n",
    "        continue\n",
    "    \n",
    "    # Sort by RMSE (lower is better)\n",
    "    sorted_transforms = sorted(\n",
    "        transformation_results.items(),\n",
    "        key=lambda x: x[1].get('rmse', float('inf'))\n",
    "    )\n",
    "    \n",
    "    # Primary (best) transformation\n",
    "    primary_type, primary_trans = sorted_transforms[0]\n",
    "    print(f\"\\n  Primary: {primary_type} (RMSE: {primary_trans.get('rmse', 'N/A'):.2f} pixels)\")\n",
    "    \n",
    "    # Secondary (second best) transformation if available\n",
    "    secondary_trans = None\n",
    "    if len(sorted_transforms) > 1:\n",
    "        secondary_type, secondary_trans = sorted_transforms[1]\n",
    "        print(f\"  Secondary: {secondary_type} (RMSE: {secondary_trans.get('rmse', 'N/A'):.2f} pixels)\")\n",
    "    \n",
    "    # Store transformations\n",
    "    transformations[ortho_name] = {\n",
    "        'primary': primary_trans,\n",
    "        'secondary': secondary_trans\n",
    "    }\n",
    "\n",
    "# Save transformations to file\n",
    "try:\n",
    "    _ = matches_dir\n",
    "except NameError:\n",
    "    from pathlib import Path\n",
    "    try:\n",
    "        _ = output_dir\n",
    "    except NameError:\n",
    "        output_dir = Path(\"outputs\")\n",
    "    try:\n",
    "        _ = gcp_matching_dir\n",
    "    except NameError:\n",
    "        gcp_matching_dir = output_dir / \"gcp_matching\"\n",
    "    matches_dir = gcp_matching_dir / \"matches\"\n",
    "    matches_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "transformations_file = matches_dir / \"transformations.json\"\n",
    "\n",
    "# Convert numpy types to native Python types for JSON serialization\n",
    "def convert_to_native_types(obj):\n",
    "    \"\"\"Recursively convert numpy types to native Python types.\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_native_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native_types(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "transformations_serializable = convert_to_native_types(transformations)\n",
    "\n",
    "with open(transformations_file, 'w') as f:\n",
    "    json.dump(transformations_serializable, f, indent=2)\n",
    "\n",
    "print(f\"\\n\u2713 Transformations saved to {transformations_file}\")\n",
    "print(f\"  Total orthos processed: {len(transformations)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Apply Transformation and Register Orthomosaics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformations not in memory, attempting to load from file...\n",
      "\u274c Could not load transformations: Expecting value: line 5 column 14 (char 75)\n",
      "Please run Step 7 (Compute Transformation) first.\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 5 column 14 (char 75)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 257\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(transformations_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 257\u001b[0m         transformations \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\u2713 Loaded transformations from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformations_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 5 column 14 (char 75)"
     ]
    }
   ],
   "source": [
    "# Apply transformation to orthomosaic\n",
    "def apply_transformation(\n",
    "    ortho_path: Path,\n",
    "    transformation: Dict,\n",
    "    output_path: Path,\n",
    "    basemap_path: Path\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Apply transformation to register orthomosaic to basemap.\n",
    "    \"\"\"\n",
    "    with rasterio.open(basemap_path) as basemap_src:\n",
    "        target_width = basemap_src.width\n",
    "        target_height = basemap_src.height\n",
    "        target_transform = basemap_src.transform\n",
    "        target_crs = basemap_src.crs\n",
    "    \n",
    "    with rasterio.open(ortho_path) as ortho_src:\n",
    "        source_data = ortho_src.read()\n",
    "        source_count = ortho_src.count\n",
    "        \n",
    "        # Apply transformation\n",
    "        if transformation['type'] == 'shift':\n",
    "            # Apply 2D shift using scipy\n",
    "            shift_x = transformation['shift_x']\n",
    "            shift_y = transformation['shift_y']\n",
    "            \n",
    "            registered_data = np.zeros((source_count, target_height, target_width), dtype=source_data.dtype)\n",
    "            \n",
    "            for band_idx in range(source_count):\n",
    "                shifted = ndimage.shift(\n",
    "                    source_data[band_idx],\n",
    "                    (shift_y, shift_x),\n",
    "                    mode='constant',\n",
    "                    cval=0,\n",
    "                    order=1\n",
    "                )\n",
    "                \n",
    "                # Crop or pad to match target dimensions\n",
    "                if shifted.shape[0] > target_height:\n",
    "                    shifted = shifted[:target_height, :]\n",
    "                elif shifted.shape[0] < target_height:\n",
    "                    padded = np.zeros((target_height, shifted.shape[1]), dtype=shifted.dtype)\n",
    "                    padded[:shifted.shape[0], :] = shifted\n",
    "                    shifted = padded\n",
    "                \n",
    "                if shifted.shape[1] > target_width:\n",
    "                    shifted = shifted[:, :target_width]\n",
    "                elif shifted.shape[1] < target_width:\n",
    "                    padded = np.zeros((target_height, target_width), dtype=shifted.dtype)\n",
    "                    padded[:, :shifted.shape[1]] = shifted\n",
    "                    shifted = padded\n",
    "                \n",
    "                registered_data[band_idx] = shifted\n",
    "        \n",
    "        elif transformation['type'] == 'affine':\n",
    "            # Apply affine transformation using scipy (handles large images)\n",
    "            transform_matrix = np.array(transformation['matrix'], dtype=np.float32)\n",
    "            \n",
    "            # Extract transformation components\n",
    "            matrix_2x2 = transform_matrix[:2, :2]\n",
    "            offset = transform_matrix[:2, 2]\n",
    "            \n",
    "            # Create output array\n",
    "            registered_data = np.zeros((source_count, target_height, target_width), dtype=source_data.dtype)\n",
    "            \n",
    "            # Apply transformation per band\n",
    "            for band_idx in range(source_count):\n",
    "                transformed = ndimage.affine_transform(\n",
    "                    source_data[band_idx],\n",
    "                    matrix=matrix_2x2.T,\n",
    "                    offset=offset[::-1],\n",
    "                    output_shape=(target_height, target_width),\n",
    "                    order=1,\n",
    "                    mode='constant',\n",
    "                    cval=0\n",
    "                )\n",
    "                registered_data[band_idx] = transformed\n",
    "\n",
    "                registered_data[band_idx] = transformed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        elif transformation['type'] == 'homography':\n",
    "            # Apply homography transformation\n",
    "            homography_matrix = np.array(transformation['matrix'], dtype=np.float32)\n",
    "            \n",
    "            # Create output array\n",
    "            registered_data = np.zeros((source_count, target_height, target_width), dtype=source_data.dtype)\n",
    "            \n",
    "            # Apply transformation per band\n",
    "            for band_idx in range(source_count):\n",
    "                # Use cv2.warpPerspective for smaller images, scipy for large ones\n",
    "                if target_height < 32767 and target_width < 32767:\n",
    "                    transformed = cv2.warpPerspective(\n",
    "                        source_data[band_idx].astype(np.float32),\n",
    "                        homography_matrix,\n",
    "                        (target_width, target_height),\n",
    "                        flags=cv2.INTER_LINEAR,\n",
    "                        borderMode=cv2.BORDER_CONSTANT,\n",
    "                        borderValue=0\n",
    "                    )\n",
    "                    registered_data[band_idx] = transformed.astype(source_data.dtype)\n",
    "\n",
    "                else:\n",
    "                    # Use scipy for large images\n",
    "                    y_coords, x_coords = np.mgrid[0:target_height, 0:target_width].astype(np.float32)\n",
    "                    coords = np.stack([x_coords.ravel(), y_coords.ravel(), np.ones(target_height * target_width)]).T\n",
    "                    inv_homography = np.linalg.inv(homography_matrix)\n",
    "                    src_coords = (inv_homography @ coords.T).T\n",
    "                    src_coords = src_coords[:, :2] / src_coords[:, 2:3]\n",
    "                    src_x = src_coords[:, 0].reshape(target_height, target_width)\n",
    "                    src_y = src_coords[:, 1].reshape(target_height, target_width)\n",
    "                    transformed = ndimage.map_coordinates(\n",
    "                        source_data[band_idx],\n",
    "                        [src_y, src_x],\n",
    "                        order=1,\n",
    "                        mode='constant',\n",
    "                        cval=0\n",
    "                    )\n",
    "                    registered_data[band_idx] = transformed.astype(source_data.dtype)\n",
    "\n",
    "        elif transformation['type'] == 'deformable':\n",
    "            # Apply deformable (thin-plate spline) transformation\n",
    "            from scipy.interpolate import RBFInterpolator\n",
    "        \n",
    "            # Recreate RBF interpolator from stored points\n",
    "            src_points = np.array(transformation.get('src_points', []), dtype=np.float32)\n",
    "            dst_points = np.array(transformation.get('dst_points', []), dtype=np.float32)\n",
    "        \n",
    "            if len(src_points) < 3:\n",
    "                raise ValueError('Insufficient points for deformable transformation')\n",
    "        \n",
    "            # Fit RBF interpolator\n",
    "            rbf = RBFInterpolator(src_points, dst_points, kernel='thin_plate_spline', smoothing=0.0)\n",
    "        \n",
    "            # Create output array\n",
    "            registered_data = np.zeros((source_count, target_height, target_width), dtype=source_data.dtype)\n",
    "        \n",
    "            # Create coordinate grids for target image\n",
    "            y_coords, x_coords = np.mgrid[0:target_height, 0:target_width].astype(np.float32)\n",
    "        \n",
    "            # Convert pixel coordinates to source image coordinates using inverse transform\n",
    "            # We need to map target pixels back to source pixels\n",
    "            # The RBF maps from source to destination, so we need to invert it\n",
    "            # For simplicity, we'll use the forward mapping: source -> target\n",
    "        \n",
    "            # Apply transformation per band\n",
    "            for band_idx in range(source_count):\n",
    "                # For each target pixel, find corresponding source pixel\n",
    "                # We need to invert the RBF: find source coords that map to target coords\n",
    "                # This is complex, so we'll use a simpler approach:\n",
    "                # Map source image through RBF to get warped coordinates, then resample\n",
    "            \n",
    "                # Create source coordinate grid\n",
    "            src_y, src_x = np.mgrid[0:source_height, 0:source_width].astype(np.float32)\n",
    "            \n",
    "            # Apply RBF to get warped coordinates\n",
    "            src_coords = np.stack([src_x.ravel(), src_y.ravel()], axis=1)\n",
    "            warped_coords = rbf(src_coords)\n",
    "            \n",
    "            # Reshape warped coordinates\n",
    "            warped_x = warped_coords[:, 0].reshape(source_height, source_width)\n",
    "            warped_y = warped_coords[:, 1].reshape(source_height, source_width)\n",
    "            \n",
    "            # Map warped source to target grid using scipy.ndimage.map_coordinates\n",
    "            # We need to find where each target pixel maps to in the warped source\n",
    "            from scipy import ndimage\n",
    "            \n",
    "            # For deformable transformation, we'll use a simpler approach:\n",
    "            # Interpolate the warped source image onto the target grid\n",
    "            # Create a temporary warped image\n",
    "            warped_band = ndimage.map_coordinates(\n",
    "                source_data[band_idx],\n",
    "                [warped_y, warped_x],\n",
    "                order=1,\n",
    "                mode='constant',\n",
    "                cval=0\n",
    "            )\n",
    "            \n",
    "            # Now resample warped_band to target dimensions\n",
    "            if warped_band.shape != (target_height, target_width):\n",
    "                from scipy.ndimage import zoom\n",
    "                zoom_y = target_height / warped_band.shape[0]\n",
    "                zoom_x = target_width / warped_band.shape[1]\n",
    "                warped_band = zoom(warped_band, (zoom_y, zoom_x), order=1)\n",
    "            \n",
    "            registered_data[band_idx] = warped_band[:target_height, :target_width].astype(source_data.dtype)\n",
    "\n",
    "\n",
    "            # Apply deformable (thin-plate spline) transformation\n",
    "            from scipy.interpolate import RBFInterpolator\n",
    "            from scipy import ndimage\n",
    "        \n",
    "            # Recreate RBF interpolator from stored points\n",
    "            src_points = np.array(transformation.get('src_points', []), dtype=np.float32)\n",
    "            dst_points = np.array(transformation.get('dst_points', []), dtype=np.float32)\n",
    "        \n",
    "            if len(src_points) < 3:\n",
    "            raise ValueError('Insufficient points for deformable transformation')\n",
    "        \n",
    "            # Fit RBF interpolator\n",
    "            rbf = RBFInterpolator(src_points, dst_points, kernel='thin_plate_spline', smoothing=0.0)\n",
    "        \n",
    "            # Create output array\n",
    "            registered_data = np.zeros((source_count, target_height, target_width), dtype=source_data.dtype)\n",
    "        \n",
    "            # Apply transformation per band\n",
    "            for band_idx in range(source_count):\n",
    "            # Create source coordinate grid\n",
    "            src_y, src_x = np.mgrid[0:source_height, 0:source_width].astype(np.float32)\n",
    "            \n",
    "            # Apply RBF to get warped coordinates\n",
    "            src_coords = np.stack([src_x.ravel(), src_y.ravel()], axis=1)\n",
    "            warped_coords = rbf(src_coords)\n",
    "            \n",
    "            # Reshape warped coordinates\n",
    "            warped_x = warped_coords[:, 0].reshape(source_height, source_width)\n",
    "            warped_y = warped_coords[:, 1].reshape(source_height, source_width)\n",
    "            \n",
    "            # Map coordinates to sample from source\n",
    "            # The warped coordinates tell us where each source pixel maps to\n",
    "            # We need to invert this to find which source pixels map to each target pixel\n",
    "            # For simplicity, we'll create a warped image and then resample it\n",
    "            \n",
    "            # Create target coordinate grid\n",
    "            tgt_y, tgt_x = np.mgrid[0:target_height, 0:target_width].astype(np.float32)\n",
    "            \n",
    "            # For each target pixel, find corresponding source pixel\n",
    "            # We need to solve: rbf(src_coord) = tgt_coord for src_coord\n",
    "            # This is complex, so we'll use a simpler approach:\n",
    "            # 1. Create a mapping from source to warped coordinates\n",
    "            # 2. For each target pixel, find nearest warped coordinate and use corresponding source\n",
    "            \n",
    "            # Simplified: use the warped coordinates directly to resample\n",
    "            # Map warped coordinates back to source space (inverse mapping)\n",
    "            # For now, use a simpler approach: resample the source using warped coordinates\n",
    "            \n",
    "            # Create output by sampling source at warped coordinates\n",
    "            # Clamp warped coordinates to source bounds\n",
    "            warped_x_clamped = np.clip(warped_x, 0, source_width - 1)\n",
    "            warped_y_clamped = np.clip(warped_y, 0, source_height - 1)\n",
    "            \n",
    "            # Sample from source using warped coordinates\n",
    "            warped_band = ndimage.map_coordinates(\n",
    "                source_data[band_idx],\n",
    "                [warped_y_clamped, warped_x_clamped],\n",
    "                order=1,\n",
    "                mode='constant',\n",
    "                cval=0\n",
    "            )\n",
    "            \n",
    "            # Resample to target dimensions if needed\n",
    "            if warped_band.shape != (target_height, target_width):\n",
    "                from scipy.ndimage import zoom\n",
    "                zoom_y = target_height / warped_band.shape[0]\n",
    "                zoom_x = target_width / warped_band.shape[1]\n",
    "                warped_band = zoom(warped_band, (zoom_y, zoom_x), order=1)\n",
    "            \n",
    "            # Crop to exact target size\n",
    "            registered_data[band_idx] = warped_band[:target_height, :target_width].astype(source_data.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate Accuracy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for required variables and set defaults if needed\n",
    "try:\n",
    "    _ = output_dir\n",
    "except NameError:\n",
    "    from pathlib import Path\n",
    "    output_dir = Path(\"outputs\")\n",
    "    print(f\"output_dir not defined, using default: {output_dir}\")\n",
    "\n",
    "try:\n",
    "    _ = gcp_matching_dir\n",
    "except NameError:\n",
    "    gcp_matching_dir = output_dir / \"gcp_matching\"\n",
    "    print(f\"gcp_matching_dir not defined, using default: {gcp_matching_dir}\")\n",
    "\n",
    "try:\n",
    "    _ = matches_dir\n",
    "except NameError:\n",
    "    matches_dir = gcp_matching_dir / \"matches\"\n",
    "    matches_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"matches_dir not defined, using default: {matches_dir}\")\n",
    "\n",
    "try:\n",
    "    _ = registered_dir\n",
    "except NameError:\n",
    "    registered_dir = gcp_matching_dir / \"registered\"\n",
    "    registered_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"registered_dir not defined, using default: {registered_dir}\")\n",
    "\n",
    "try:\n",
    "    _ = basemap_path\n",
    "except NameError:\n",
    "    from pathlib import Path\n",
    "    data_dir = Path(\"/Users/mauriciohessflores/Documents/Code/Data/New Westminster Oct _25\")\n",
    "    basemap_path = data_dir / \"Michael_RTK_orthos\" / \"TestsiteNewWest_Spexigeo_RTK.tiff\"\n",
    "    print(f\"basemap_path not defined, using default: {basemap_path}\")\n",
    "\n",
    "try:\n",
    "    _ = gcps\n",
    "except NameError:\n",
    "    print(\"\u26a0\ufe0f  gcps not defined. Please run Step 2 first.\")\n",
    "    gcps = []\n",
    "\n",
    "# Compare registered orthos to basemap\n",
    "\n",
    "# Import required modules\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_accuracy(ortho_path: Path, basemap_path: Path, gcps: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate accuracy by comparing pixel values at GCP locations.\n",
    "    \"\"\"\n",
    "    with rasterio.open(basemap_path) as basemap_src:\n",
    "        basemap_data = basemap_src.read()\n",
    "    \n",
    "    with rasterio.open(ortho_path) as ortho_src:\n",
    "        ortho_data = ortho_src.read()\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    for gcp in gcps:\n",
    "        pixel_coords = gcp_to_pixel_coords_from_utm(gcp['x_utm'], gcp['y_utm'], basemap_path)\n",
    "        if not pixel_coords:\n",
    "            continue\n",
    "        \n",
    "        col, row = pixel_coords\n",
    "        \n",
    "        if 0 <= row < basemap_data.shape[1] and 0 <= col < basemap_data.shape[2]:\n",
    "            basemap_pixel = basemap_data[:, row, col]\n",
    "            \n",
    "            if 0 <= row < ortho_data.shape[1] and 0 <= col < ortho_data.shape[2]:\n",
    "                ortho_pixel = ortho_data[:, row, col]\n",
    "                \n",
    "                # Compute error (Euclidean distance in pixel space)\n",
    "                error = np.sqrt(np.sum((basemap_pixel.astype(float) - ortho_pixel.astype(float))**2))\n",
    "                errors.append(error)\n",
    "    \n",
    "    if errors:\n",
    "        return {\n",
    "            'mean_error': float(np.mean(errors)),\n",
    "            'rmse': float(np.sqrt(np.mean(np.array(errors)**2))),\n",
    "            'max_error': float(np.max(errors)),\n",
    "            'min_error': float(np.min(errors)),\n",
    "            'num_points': len(errors)\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'mean_error': 0.0,\n",
    "            'rmse': 0.0,\n",
    "            'max_error': 0.0,\n",
    "            'min_error': 0.0,\n",
    "            'num_points': 0\n",
    "        }\n",
    "\n",
    "\n",
    "# Evaluate accuracy for each registered ortho\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Evaluating Accuracy Improvement\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Check if registered_paths is defined\n",
    "try:\n",
    "    _ = registered_paths\n",
    "except NameError:\n",
    "    print(\"\u26a0\ufe0f  registered_paths not defined. Please run Step 8 first.\")\n",
    "    registered_paths = {}\n",
    "\n",
    "    # Try to load from transformations file or reconstruct from registered directory\n",
    "    try:\n",
    "        # Check if transformations file exists (from Step 7)\n",
    "        transformations_file = matches_dir / \"transformations.json\"\n",
    "        if transformations_file.exists():\n",
    "            with open(transformations_file, 'r') as f:\n",
    "                transformations = json.load(f)\n",
    "            \n",
    "            # Reconstruct registered_paths from transformations\n",
    "            registered_paths = {}\n",
    "            for ortho_name in transformations.keys():\n",
    "                registered_path = registered_dir / f\"{ortho_name}_registered.tif\"\n",
    "                if registered_path.exists():\n",
    "                    registered_paths[ortho_name] = registered_path\n",
    "            \n",
    "            if len(registered_paths) > 0:\n",
    "                print(f\"\u2713 Reconstructed registered_paths from {transformations_file}\")\n",
    "                print(f\"  Found {len(registered_paths)} registered orthos\")\n",
    "        else:\n",
    "            # Try to find registered files directly\n",
    "            if registered_dir.exists():\n",
    "                registered_files = list(registered_dir.glob(\"*_registered.tif\"))\n",
    "                if registered_files:\n",
    "                    registered_paths = {}\n",
    "                    for reg_file in registered_files:\n",
    "                        # Extract ortho name from filename (e.g., 'no_gcps_registered.tif' -> 'no_gcps')\n",
    "                        ortho_name = reg_file.stem.replace('_registered', '')\n",
    "                        registered_paths[ortho_name] = reg_file\n",
    "                    \n",
    "                    if len(registered_paths) > 0:\n",
    "                        print(f\"\u2713 Found {len(registered_paths)} registered orthos in {registered_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f  Could not reconstruct registered_paths: {e}\")\n",
    "\n",
    "if len(registered_paths) == 0:\n",
    "    print(\"\u26a0\ufe0f  No registered orthos found. Please run Step 8 first.\")\n",
    "else:\n",
    "    for ortho_name in registered_paths.keys():\n",
    "        print(f\"\\nEvaluating {ortho_name}...\")\n",
    "        \n",
    "        registered_path = registered_paths[ortho_name]\n",
    "        \n",
    "        if not registered_path.exists():\n",
    "            print(f\"  \u26a0\ufe0f  Registered ortho not found: {registered_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Evaluate accuracy\n",
    "        try:\n",
    "            accuracy_metrics = evaluate_accuracy(\n",
    "                registered_path,\n",
    "                basemap_path,\n",
    "                gcps\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n  Accuracy Metrics:\")\n",
    "            mean_err = accuracy_metrics.get('mean_error', 'N/A')\n",
    "            if isinstance(mean_err, (int, float)):\n",
    "                print(f\"    Mean Error: {mean_err:.3f} m\")\n",
    "            else:\n",
    "                print(f\"    Mean Error: {mean_err}\")\n",
    "            \n",
    "            rmse = accuracy_metrics.get('rmse', 'N/A')\n",
    "            if isinstance(rmse, (int, float)):\n",
    "                print(f\"    RMSE: {rmse:.3f} m\")\n",
    "            else:\n",
    "                print(f\"    RMSE: {rmse}\")\n",
    "            \n",
    "            max_err = accuracy_metrics.get('max_error', 'N/A')\n",
    "            if isinstance(max_err, (int, float)):\n",
    "                print(f\"    Max Error: {max_err:.3f} m\")\n",
    "            else:\n",
    "                print(f\"    Max Error: {max_err}\")\n",
    "            \n",
    "            print(f\"    Points Evaluated: {accuracy_metrics.get('num_points', 0)}\")\n",
    "\n",
    "# Generate LaTeX report\n",
    "latex_report_path = output_dir / \"gcp_matching\" / \"accuracy_report.tex\"\n",
    "latex_report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "latex_content = []\n",
    "latex_content.append(\"\\\\documentclass[11pt]{article}\")\n",
    "latex_content.append(\"\\\\usepackage[utf8]{inputenc}\")\n",
    "latex_content.append(\"\\\\usepackage{graphicx}\")\n",
    "latex_content.append(\"\\\\usepackage{geometry}\")\n",
    "latex_content.append(\"\\\\geometry{a4paper, margin=1in}\")\n",
    "latex_content.append(\"\\\\usepackage{booktabs}\")\n",
    "latex_content.append(\"\\\\usepackage{float}\")\n",
    "latex_content.append(\"\\\\usepackage{caption}\")\n",
    "latex_content.append(\"\\\\begin{document}\")\n",
    "latex_content.append(\"\\\\title{GCP Matching Accuracy Evaluation Report}\")\n",
    "latex_content.append(\"\\\\author{Automated Analysis}\")\n",
    "latex_content.append(\"\\\\date{\\\\today}\")\n",
    "latex_content.append(\"\\\\maketitle\")\n",
    "latex_content.append(\"\\\\section{Executive Summary}\")\n",
    "latex_content.append(\"This report presents the accuracy evaluation of registered orthomosaics against the ground control basemap.\")\n",
    "latex_content.append(\"\\\\section{Accuracy Metrics}\")\n",
    "latex_content.append(\"\\\\begin{table}[H]\")\n",
    "latex_content.append(\"\\\\centering\")\n",
    "latex_content.append(\"\\\\begin{tabular}{lcccc}\")\n",
    "latex_content.append(\"\\\\toprule\")\n",
    "latex_content.append(\"Orthomosaic & Mean Error (m) & RMSE (m) & Max Error (m) & Points \\\\\\\\\")\n",
    "latex_content.append(\"\\\\midrule\")\n",
    "\n",
    "# Collect all accuracy metrics\n",
    "all_accuracy_metrics = {}\n",
    "\n",
    "for ortho_name in registered_paths.keys():\n",
    "    registered_path = registered_paths[ortho_name]\n",
    "    if not registered_path.exists():\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        accuracy_metrics = evaluate_accuracy(registered_path, basemap_path, gcps)\n",
    "        all_accuracy_metrics[ortho_name] = accuracy_metrics\n",
    "        \n",
    "        # Add to LaTeX table\n",
    "        mean_err = accuracy_metrics.get('mean_error', 0.0)\n",
    "        rmse = accuracy_metrics.get('rmse', 0.0)\n",
    "        max_err = accuracy_metrics.get('max_error', 0.0)\n",
    "        num_pts = accuracy_metrics.get('num_points', 0)\n",
    "        \n",
    "        mean_str = f\"{mean_err:.3f}\" if isinstance(mean_err, (int, float)) else \"N/A\"\n",
    "        rmse_str = f\"{rmse:.3f}\" if isinstance(rmse, (int, float)) else \"N/A\"\n",
    "        max_str = f\"{max_err:.3f}\" if isinstance(max_err, (int, float)) else \"N/A\"\n",
    "        \n",
    "        ortho_display = ortho_name.replace('_', ' ').title()\n",
    "        latex_content.append(f\"{ortho_display} & {mean_str} & {rmse_str} & {max_str} & {num_pts} \\\\\\\\\")\n",
    "    except Exception as e:\n",
    "        print(f\"  \u26a0\ufe0f  Error evaluating {ortho_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "latex_content.append(\"\\\\bottomrule\")\n",
    "latex_content.append(\"\\\\end{tabular}\")\n",
    "latex_content.append(\"\\\\caption{Accuracy metrics for registered orthomosaics}\")\n",
    "latex_content.append(\"\\\\label{tab:accuracy}\")\n",
    "latex_content.append(\"\\\\end{table}\")\n",
    "\n",
    "# Add visualization section if figures exist\n",
    "latex_content.append(\"\\\\section{Visualizations}\")\n",
    "\n",
    "# Check for visualization files\n",
    "vis_dir = gcp_matching_dir / \"visualizations\"\n",
    "if vis_dir.exists():\n",
    "    vis_files = list(vis_dir.glob(\"*.png\")) + list(vis_dir.glob(\"*.jpg\"))\n",
    "    for vis_file in sorted(vis_files):\n",
    "        # Copy to report directory for LaTeX\n",
    "        report_vis_dir = latex_report_path.parent / \"figures\"\n",
    "        report_vis_dir.mkdir(exist_ok=True)\n",
    "        import shutil\n",
    "        dest_file = report_vis_dir / vis_file.name\n",
    "        if not dest_file.exists():\n",
    "            shutil.copy(vis_file, dest_file)\n",
    "        \n",
    "        # Add figure to LaTeX\n",
    "        fig_name = vis_file.stem.replace('_', ' ').title()\n",
    "        latex_content.append(f\"\\\\begin{{figure}}[H]\")\n",
    "        latex_content.append(f\"\\\\centering\")\n",
    "        latex_content.append(f\"\\\\includegraphics[width=0.8\\\\textwidth]{{figures/{vis_file.name}}}\")\n",
    "        latex_content.append(f\"\\\\caption{{{fig_name}}}\")\n",
    "        latex_content.append(f\"\\\\label{{fig:{vis_file.stem}}}\")\n",
    "        latex_content.append(f\"\\\\end{{figure}}\")\n",
    "\n",
    "latex_content.append(\"\\\\section{Conclusion}\")\n",
    "latex_content.append(\"The registered orthomosaics show improved alignment with the ground control basemap.\")\n",
    "latex_content.append(\"\\\\end{document}\")\n",
    "\n",
    "# Write LaTeX file\n",
    "with open(latex_report_path, 'w') as f:\n",
    "    f.write('\\n'.join(latex_content))\n",
    "\n",
    "print(f\"\\n\u2713 LaTeX report generated: {latex_report_path}\")\n",
    "print(f\"  To compile: pdflatex {latex_report_path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  \u274c Error evaluating accuracy: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}